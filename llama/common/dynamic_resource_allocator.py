"""
åŠ¨æ€èµ„æºåˆ†é…å™¨ - è·¨WorkeråŠ¨æ€è°ƒæ•´èµ„æºåˆ†é…

å½“åªæœ‰ä¸€ä¸ªWorkeræ´»è·ƒæ—¶ï¼Œè‡ªåŠ¨å°†å…¶ä»–ç©ºé—²Workerçš„èµ„æºåˆ†é…ç»™æ´»è·ƒWorkerï¼Œ
å®ç°èµ„æºçš„æœ€ä¼˜åˆ©ç”¨å’Œæ€§èƒ½æå‡ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
- ç›‘æ§æ‰€æœ‰Workerçš„æ´»åŠ¨çŠ¶æ€
- åŠ¨æ€è°ƒæ•´å¹¶å‘è¯·æ±‚æ•°ã€RPMé™åˆ¶ã€TPMé™åˆ¶
- æ”¯æŒå¤šç§å…±äº«çŠ¶æ€å­˜å‚¨æ–¹å¼ï¼ˆRedis/æ–‡ä»¶ï¼‰
- ä¸ç°æœ‰é¢‘ç‡æ§åˆ¶ç³»ç»Ÿæ— ç¼é›†æˆ
"""

import os
import time
import json
import threading
import logging
import fcntl
from typing import Dict, Optional, List, Any
from dataclasses import dataclass, asdict
from pathlib import Path
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


@dataclass
class WorkerState:
    """WorkerçŠ¶æ€ä¿¡æ¯"""
    worker_id: str
    is_active: bool
    last_active_time: float
    current_load: float  # 0.0-1.0
    active_tasks: int
    allocated_resources: Dict[str, int]


@dataclass
class ResourceAllocation:
    """èµ„æºåˆ†é…é…ç½®"""
    max_concurrent_requests: int
    rpm_limit: int
    tpm_limit: int
    num_workers: int
    
    def to_dict(self) -> Dict[str, int]:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict[str, int]) -> 'ResourceAllocation':
        return cls(**data)


class WorkerActivityMonitor:
    """Workeræ´»åŠ¨ç›‘æ§å™¨ - è·Ÿè¸ªæ‰€æœ‰Workerçš„æ´»åŠ¨çŠ¶æ€"""
    
    def __init__(
        self,
        worker_id: str,
        total_workers: int = 4,
        activity_timeout: float = 30.0,
        monitoring_interval: float = 5.0,
        storage_backend: str = "file"
    ):
        """
        åˆå§‹åŒ–Workeræ´»åŠ¨ç›‘æ§å™¨
        
        Args:
            worker_id: å½“å‰Workerçš„å”¯ä¸€æ ‡è¯†
            total_workers: æ€»Workeræ•°é‡
            activity_timeout: æ´»åŠ¨è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œè¶…è¿‡æ­¤æ—¶é—´æœªæ´»åŠ¨åˆ™è§†ä¸ºç©ºé—²
            monitoring_interval: ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰
            storage_backend: å­˜å‚¨åç«¯ï¼ˆ"file" æˆ– "redis"ï¼‰
        """
        self.worker_id = worker_id
        self.total_workers = total_workers
        self.activity_timeout = activity_timeout
        self.monitoring_interval = monitoring_interval
        self.storage_backend = storage_backend
        
        # å…±äº«çŠ¶æ€å­˜å‚¨è·¯å¾„
        self.state_file = Path(os.getcwd()) / ".worker_activity_state.json"
        
        # æœ¬åœ°çŠ¶æ€
        self.local_state: WorkerState = WorkerState(
            worker_id=worker_id,
            is_active=False,
            last_active_time=time.time(),
            current_load=0.0,
            active_tasks=0,
            allocated_resources={}
        )
        
        # ç›‘æ§çº¿ç¨‹
        self.monitor_thread: Optional[threading.Thread] = None
        self.stop_monitoring = threading.Event()
        
        # é”
        self.state_lock = threading.Lock()
        
        logger.info(f"åˆå§‹åŒ–Workeræ´»åŠ¨ç›‘æ§å™¨: worker_id={worker_id}, total_workers={total_workers}")
    
    def _get_all_worker_states(self) -> Dict[str, WorkerState]:
        """è·å–æ‰€æœ‰Workerçš„çŠ¶æ€"""
        if self.storage_backend == "file":
            return self._read_from_file()
        elif self.storage_backend == "redis":
            return self._read_from_redis()
        else:
            logger.warning(f"æœªçŸ¥çš„å­˜å‚¨åç«¯: {self.storage_backend}ï¼Œä½¿ç”¨æœ¬åœ°çŠ¶æ€")
            return {self.worker_id: self.local_state}
    
    def _read_from_file(self) -> Dict[str, WorkerState]:
        """ä»æ–‡ä»¶è¯»å–æ‰€æœ‰WorkerçŠ¶æ€"""
        max_retries = 3
        retry_delay = 0.1
        
        for attempt in range(max_retries):
            try:
                if not self.state_file.exists():
                    return {}
                
                # ä½¿ç”¨æ–‡ä»¶é”ç¡®ä¿è¯»å–ä¸€è‡´æ€§
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    # å°è¯•è·å–å…±äº«é”ï¼ˆè¯»å–é”ï¼‰
                    try:
                        fcntl.flock(f.fileno(), fcntl.LOCK_SH)
                    except (AttributeError, OSError):
                        pass  # Windowsä¸æ”¯æŒfcntlï¼Œæˆ–é”è·å–å¤±è´¥
                    
                    try:
                        data = json.load(f)
                    finally:
                        try:
                            fcntl.flock(f.fileno(), fcntl.LOCK_UN)
                        except (AttributeError, OSError):
                            pass
                
                states = {}
                for worker_id, state_data in data.items():
                    states[worker_id] = WorkerState(**state_data)
                
                return states
                
            except json.JSONDecodeError as e:
                logger.warning(f"è§£æWorkerçŠ¶æ€æ–‡ä»¶å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰: {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay * (attempt + 1))
                else:
                    logger.error(f"è§£æWorkerçŠ¶æ€æ–‡ä»¶æœ€ç»ˆå¤±è´¥: {e}")
                    return {}
            except Exception as e:
                logger.warning(f"è¯»å–WorkerçŠ¶æ€æ–‡ä»¶å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰: {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay * (attempt + 1))
                else:
                    logger.error(f"è¯»å–WorkerçŠ¶æ€æ–‡ä»¶æœ€ç»ˆå¤±è´¥: {e}")
                    return {}
    
    def _write_to_file(self, states: Dict[str, WorkerState]):
        """å†™å…¥æ‰€æœ‰WorkerçŠ¶æ€åˆ°æ–‡ä»¶"""
        max_retries = 3
        retry_delay = 0.1
        
        for attempt in range(max_retries):
            try:
                data = {}
                for worker_id, state in states.items():
                    data[worker_id] = asdict(state)
                
                # ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
                self.state_file.parent.mkdir(parents=True, exist_ok=True)
                
                # ä½¿ç”¨å”¯ä¸€çš„ä¸´æ—¶æ–‡ä»¶åï¼ˆåŒ…å«è¿›ç¨‹IDå’ŒWorker IDï¼‰
                import uuid
                temp_file = self.state_file.with_suffix(f'.tmp.{os.getpid()}.{self.worker_id}.{uuid.uuid4().hex[:8]}')
                
                # å†™å…¥ä¸´æ—¶æ–‡ä»¶
                with open(temp_file, 'w', encoding='utf-8') as f:
                    # å°è¯•è·å–æ’ä»–é”ï¼ˆå†™å…¥é”ï¼‰
                    try:
                        fcntl.flock(f.fileno(), fcntl.LOCK_EX)
                    except (AttributeError, OSError):
                        pass  # Windowsä¸æ”¯æŒfcntlï¼Œæˆ–é”è·å–å¤±è´¥
                    
                    try:
                        json.dump(data, f, indent=2, ensure_ascii=False)
                        f.flush()
                        os.fsync(f.fileno())
                    finally:
                        try:
                            fcntl.flock(f.fileno(), fcntl.LOCK_UN)
                        except (AttributeError, OSError):
                            pass
                
                # åŸå­æ›¿æ¢
                temp_file.replace(self.state_file)
                
                return
                
            except Exception as e:
                logger.warning(f"å†™å…¥WorkerçŠ¶æ€æ–‡ä»¶å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰: {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay * (attempt + 1))
                else:
                    logger.error(f"å†™å…¥WorkerçŠ¶æ€æ–‡ä»¶æœ€ç»ˆå¤±è´¥: {e}")
    
    def _read_from_redis(self) -> Dict[str, WorkerState]:
        """ä»Redisè¯»å–æ‰€æœ‰WorkerçŠ¶æ€"""
        try:
            import redis
            redis_client = redis.Redis(
                host=os.getenv('REDIS_HOST', 'localhost'),
                port=int(os.getenv('REDIS_PORT', 6379)),
                db=int(os.getenv('REDIS_DB', 0)),
                decode_responses=True
            )
            
            key = "worker_activity_states"
            data = redis_client.hgetall(key)
            
            states = {}
            for worker_id, state_json in data.items():
                state_data = json.loads(state_json)
                states[worker_id] = WorkerState(**state_data)
            
            return states
        except ImportError:
            logger.warning("Redisæœªå®‰è£…ï¼Œå›é€€åˆ°æ–‡ä»¶å­˜å‚¨")
            self.storage_backend = "file"
            return self._read_from_file()
        except Exception as e:
            logger.error(f"ä»Redisè¯»å–WorkerçŠ¶æ€å¤±è´¥: {e}")
            return {}
    
    def _write_to_redis(self, states: Dict[str, WorkerState]):
        """å†™å…¥æ‰€æœ‰WorkerçŠ¶æ€åˆ°Redis"""
        try:
            import redis
            redis_client = redis.Redis(
                host=os.getenv('REDIS_HOST', 'localhost'),
                port=int(os.getenv('REDIS_PORT', 6379)),
                db=int(os.getenv('REDIS_DB', 0)),
                decode_responses=True
            )
            
            key = "worker_activity_states"
            pipe = redis_client.pipeline()
            
            for worker_id, state in states.items():
                state_json = json.dumps(asdict(state), ensure_ascii=False)
                pipe.hset(key, worker_id, state_json)
            
            pipe.expire(key, 3600)  # 1å°æ—¶è¿‡æœŸ
            pipe.execute()
            
        except Exception as e:
            logger.error(f"å†™å…¥Redis WorkerçŠ¶æ€å¤±è´¥: {e}")
    
    def update_local_state(
        self,
        is_active: bool,
        current_load: float = 0.0,
        active_tasks: int = 0
    ):
        """
        æ›´æ–°æœ¬åœ°WorkerçŠ¶æ€
        
        Args:
            is_active: æ˜¯å¦æ´»è·ƒ
            current_load: å½“å‰è´Ÿè½½ï¼ˆ0.0-1.0ï¼‰
            active_tasks: æ´»è·ƒä»»åŠ¡æ•°
        """
        with self.state_lock:
            self.local_state.is_active = is_active
            self.local_state.current_load = current_load
            self.local_state.active_tasks = active_tasks
            
            if is_active:
                self.local_state.last_active_time = time.time()
            
            logger.debug(
                f"æ›´æ–°æœ¬åœ°WorkerçŠ¶æ€: {self.worker_id}, "
                f"active={is_active}, load={current_load:.2f}, tasks={active_tasks}"
            )
    
    def sync_state(self):
        """åŒæ­¥æœ¬åœ°çŠ¶æ€åˆ°å…±äº«å­˜å‚¨"""
        with self.state_lock:
            all_states = self._get_all_worker_states()
            all_states[self.worker_id] = self.local_state
            
            # æ¸…ç†è¿‡æœŸçš„WorkerçŠ¶æ€
            current_time = time.time()
            expired_workers = []
            for worker_id, state in all_states.items():
                if current_time - state.last_active_time > self.activity_timeout * 2:
                    expired_workers.append(worker_id)
            
            for worker_id in expired_workers:
                del all_states[worker_id]
                logger.debug(f"æ¸…ç†è¿‡æœŸWorkerçŠ¶æ€: {worker_id}")
            
            # å†™å…¥å…±äº«å­˜å‚¨
            if self.storage_backend == "file":
                self._write_to_file(all_states)
            elif self.storage_backend == "redis":
                self._write_to_redis(all_states)
    
    def get_active_workers(self) -> List[str]:
        """è·å–å½“å‰æ´»è·ƒçš„Workeråˆ—è¡¨"""
        all_states = self._get_all_worker_states()
        current_time = time.time()
        
        active_workers = []
        for worker_id, state in all_states.items():
            if state.is_active and (current_time - state.last_active_time) < self.activity_timeout:
                active_workers.append(worker_id)
        
        return active_workers
    
    def get_active_worker_count(self) -> int:
        """è·å–æ´»è·ƒWorkeræ•°é‡"""
        return len(self.get_active_workers())
    
    def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§çº¿ç¨‹"""
        if self.monitor_thread is not None:
            return
        
        def monitor_loop():
            while not self.stop_monitoring.is_set():
                try:
                    # åŒæ­¥çŠ¶æ€
                    self.sync_state()
                    
                    # è®°å½•æ´»è·ƒWorkeræ•°é‡
                    active_count = self.get_active_worker_count()
                    logger.debug(f"å½“å‰æ´»è·ƒWorkeræ•°é‡: {active_count}/{self.total_workers}")
                    
                    # ç­‰å¾…ä¸‹ä¸€æ¬¡ç›‘æ§
                    self.stop_monitoring.wait(self.monitoring_interval)
                    
                except Exception as e:
                    logger.error(f"ç›‘æ§çº¿ç¨‹å‡ºé”™: {e}")
        
        self.monitor_thread = threading.Thread(
            target=monitor_loop,
            daemon=True,
            name=f"WorkerMonitor-{self.worker_id}"
        )
        self.monitor_thread.start()
        
        logger.info(f"Workerç›‘æ§çº¿ç¨‹å·²å¯åŠ¨: {self.worker_id}")
    
    def stop_monitoring_thread(self):
        """åœæ­¢ç›‘æ§çº¿ç¨‹"""
        if self.monitor_thread:
            self.stop_monitoring.set()
            self.monitor_thread.join(timeout=5.0)
            logger.info(f"Workerç›‘æ§çº¿ç¨‹å·²åœæ­¢: {self.worker_id}")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.stop_monitoring_thread()
        
        # ä»å…±äº«å­˜å‚¨ä¸­ç§»é™¤æœ¬Workerçš„çŠ¶æ€
        try:
            all_states = self._get_all_worker_states()
            if self.worker_id in all_states:
                del all_states[self.worker_id]
                
                if self.storage_backend == "file":
                    self._write_to_file(all_states)
                elif self.storage_backend == "redis":
                    self._write_to_redis(all_states)
                
                logger.info(f"å·²æ¸…ç†WorkerçŠ¶æ€: {self.worker_id}")
        except Exception as e:
            logger.error(f"æ¸…ç†WorkerçŠ¶æ€å¤±è´¥: {e}")


class DynamicResourceAllocator:
    """åŠ¨æ€èµ„æºåˆ†é…å™¨ - æ ¹æ®æ´»è·ƒWorkeræ•°é‡åŠ¨æ€è°ƒæ•´èµ„æºåˆ†é…"""
    
    def __init__(
        self,
        worker_id: str,
        total_workers: int = 4,
        base_allocation: Optional[ResourceAllocation] = None,
        monitor: Optional[WorkerActivityMonitor] = None,
        adjustment_interval: float = 10.0,
        enable_scaling: bool = True
    ):
        """
        åˆå§‹åŒ–åŠ¨æ€èµ„æºåˆ†é…å™¨
        
        Args:
            worker_id: å½“å‰Workerçš„å”¯ä¸€æ ‡è¯†
            total_workers: æ€»Workeræ•°é‡
            base_allocation: åŸºç¡€èµ„æºåˆ†é…ï¼ˆæ¯ä¸ªWorkerçš„é»˜è®¤èµ„æºï¼‰
            monitor: Workeræ´»åŠ¨ç›‘æ§å™¨
            adjustment_interval: èµ„æºè°ƒæ•´é—´éš”ï¼ˆç§’ï¼‰
            enable_scaling: æ˜¯å¦å¯ç”¨åŠ¨æ€ç¼©æ”¾
        """
        self.worker_id = worker_id
        self.total_workers = total_workers
        self.adjustment_interval = adjustment_interval
        self.enable_scaling = enable_scaling
        
        # åŸºç¡€èµ„æºåˆ†é…ï¼ˆæ¯ä¸ªWorkerçš„é»˜è®¤èµ„æºï¼‰
        if base_allocation is None:
            base_allocation = ResourceAllocation(
                max_concurrent_requests=10,  # æ¯ä¸ªworkerçš„å¹¶å‘æ•° (40/4=10)
                rpm_limit=200,  # æ¯ä¸ªworkerçš„RPMé™åˆ¶ (800/4=200)
                tpm_limit=10000,  # æ¯ä¸ªworkerçš„TPMé™åˆ¶ (40000/4=10000)
                num_workers=10  # æ¯ä¸ªworkerçš„å·¥ä½œçº¿ç¨‹æ•° (40/4=10)
            )
        
        self.base_allocation = base_allocation
        
        # æ€»èµ„æºï¼ˆæ‰€æœ‰Workerå…±äº«ï¼‰
        self.total_resources = ResourceAllocation(
            max_concurrent_requests=base_allocation.max_concurrent_requests * total_workers,
            rpm_limit=base_allocation.rpm_limit * total_workers,
            tpm_limit=base_allocation.tpm_limit * total_workers,
            num_workers=base_allocation.num_workers * total_workers
        )
        
        # å½“å‰åˆ†é…çš„èµ„æº
        self.current_allocation = ResourceAllocation(**asdict(base_allocation))
        
        # Workeræ´»åŠ¨ç›‘æ§å™¨
        if monitor is None:
            monitor = WorkerActivityMonitor(
                worker_id=worker_id,
                total_workers=total_workers
            )
        self.monitor = monitor
        
        # è°ƒæ•´çº¿ç¨‹
        self.adjustment_thread: Optional[threading.Thread] = None
        self.stop_adjustment = threading.Event()
        
        # èµ„æºåˆ†é…å›è°ƒå‡½æ•°
        self.allocation_callback: Optional[callable] = None
        
        logger.info(
            f"åˆå§‹åŒ–åŠ¨æ€èµ„æºåˆ†é…å™¨: worker_id={worker_id}, "
            f"base_allocation={base_allocation.to_dict()}, "
            f"total_resources={self.total_resources.to_dict()}"
        )
    
    def set_allocation_callback(self, callback: callable):
        """
        è®¾ç½®èµ„æºåˆ†é…å›è°ƒå‡½æ•°
        
        Args:
            callback: å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ResourceAllocationå‚æ•°
        """
        self.allocation_callback = callback
        logger.info("èµ„æºåˆ†é…å›è°ƒå‡½æ•°å·²è®¾ç½®")
    
    def calculate_optimal_allocation(self, active_workers: List[str]) -> ResourceAllocation:
        """
        è®¡ç®—æœ€ä¼˜èµ„æºåˆ†é…
        
        Args:
            active_workers: æ´»è·ƒWorkeråˆ—è¡¨
            
        Returns:
            æœ€ä¼˜èµ„æºåˆ†é…
        """
        active_count = len(active_workers)
        
        if active_count == 0:
            # æ²¡æœ‰æ´»è·ƒWorkerï¼Œä½¿ç”¨åŸºç¡€åˆ†é…
            return ResourceAllocation(**asdict(self.base_allocation))
        
        if active_count == 1:
            # åªæœ‰ä¸€ä¸ªæ´»è·ƒWorkerï¼Œåˆ†é…æ‰€æœ‰èµ„æº
            logger.info(f"ğŸš€ æ¿€æ´»åŠ¨æ€ç¼©æ”¾: åªæœ‰1ä¸ªæ´»è·ƒWorkerï¼Œåˆ†é…å…¨éƒ¨èµ„æº")
            return ResourceAllocation(**asdict(self.total_resources))
        
        # å¤šä¸ªæ´»è·ƒWorkerï¼Œå¹³å‡åˆ†é…èµ„æº
        avg_concurrent = self.total_resources.max_concurrent_requests // active_count
        avg_rpm = self.total_resources.rpm_limit // active_count
        avg_tpm = self.total_resources.tpm_limit // active_count
        avg_workers = self.total_resources.num_workers // active_count
        
        allocation = ResourceAllocation(
            max_concurrent_requests=avg_concurrent,
            rpm_limit=avg_rpm,
            tpm_limit=avg_tpm,
            num_workers=avg_workers
        )
        
        logger.info(
            f"ğŸ“Š å¹³å‡åˆ†é…èµ„æº: {active_count}ä¸ªæ´»è·ƒWorker, "
            f"concurrent={avg_concurrent}, rpm={avg_rpm}, tpm={avg_tpm}, workers={avg_workers}"
        )
        
        return allocation
    
    def adjust_resources(self):
        """è°ƒæ•´èµ„æºåˆ†é…"""
        if not self.enable_scaling:
            return
        
        # è·å–æ´»è·ƒWorkeråˆ—è¡¨
        active_workers = self.monitor.get_active_workers()
        
        # è®¡ç®—æœ€ä¼˜åˆ†é…
        optimal_allocation = self.calculate_optimal_allocation(active_workers)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´
        if optimal_allocation.to_dict() != self.current_allocation.to_dict():
            old_allocation = self.current_allocation.to_dict()
            new_allocation = optimal_allocation.to_dict()
            
            self.current_allocation = optimal_allocation
            
            logger.info(
                f"ğŸ”„ èµ„æºåˆ†é…è°ƒæ•´: {self.worker_id}\n"
                f"  æ—§é…ç½®: {old_allocation}\n"
                f"  æ–°é…ç½®: {new_allocation}\n"
                f"  æ´»è·ƒWorker: {len(active_workers)}/{self.total_workers}"
            )
            
            # è°ƒç”¨å›è°ƒå‡½æ•°
            if self.allocation_callback:
                try:
                    self.allocation_callback(self.current_allocation)
                except Exception as e:
                    logger.error(f"èµ„æºåˆ†é…å›è°ƒå‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
    
    def start_adjustment(self):
        """å¯åŠ¨èµ„æºè°ƒæ•´çº¿ç¨‹"""
        if self.adjustment_thread is not None:
            return
        
        def adjustment_loop():
            while not self.stop_adjustment.is_set():
                try:
                    # è°ƒæ•´èµ„æº
                    self.adjust_resources()
                    
                    # ç­‰å¾…ä¸‹ä¸€æ¬¡è°ƒæ•´
                    self.stop_adjustment.wait(self.adjustment_interval)
                    
                except Exception as e:
                    logger.error(f"èµ„æºè°ƒæ•´çº¿ç¨‹å‡ºé”™: {e}")
        
        self.adjustment_thread = threading.Thread(
            target=adjustment_loop,
            daemon=True,
            name=f"ResourceAdjuster-{self.worker_id}"
        )
        self.adjustment_thread.start()
        
        logger.info(f"èµ„æºè°ƒæ•´çº¿ç¨‹å·²å¯åŠ¨: {self.worker_id}")
    
    def stop_adjustment_thread(self):
        """åœæ­¢èµ„æºè°ƒæ•´çº¿ç¨‹"""
        if self.adjustment_thread:
            self.stop_adjustment.set()
            self.adjustment_thread.join(timeout=5.0)
            logger.info(f"èµ„æºè°ƒæ•´çº¿ç¨‹å·²åœæ­¢: {self.worker_id}")
    
    def get_current_allocation(self) -> ResourceAllocation:
        """è·å–å½“å‰èµ„æºåˆ†é…"""
        return self.current_allocation
    
    def get_scaling_status(self) -> Dict[str, Any]:
        """è·å–ç¼©æ”¾çŠ¶æ€"""
        active_workers = self.monitor.get_active_workers()
        active_count = len(active_workers)
        
        return {
            'worker_id': self.worker_id,
            'total_workers': self.total_workers,
            'active_workers': active_count,
            'active_worker_ids': active_workers,
            'is_scaling_enabled': self.enable_scaling,
            'base_allocation': self.base_allocation.to_dict(),
            'current_allocation': self.current_allocation.to_dict(),
            'total_resources': self.total_resources.to_dict(),
            'utilization_ratio': {
                'concurrent': self.current_allocation.max_concurrent_requests / self.base_allocation.max_concurrent_requests,
                'rpm': self.current_allocation.rpm_limit / self.base_allocation.rpm_limit,
                'tpm': self.current_allocation.tpm_limit / self.base_allocation.tpm_limit,
                'workers': self.current_allocation.num_workers / self.base_allocation.num_workers
            }
        }
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.stop_adjustment_thread()
        self.monitor.cleanup()
        logger.info(f"åŠ¨æ€èµ„æºåˆ†é…å™¨å·²æ¸…ç†: {self.worker_id}")


class DynamicScalingManager:
    """åŠ¨æ€ç¼©æ”¾ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†Workerç›‘æ§å’Œèµ„æºåˆ†é…"""
    
    def __init__(
        self,
        worker_id: str,
        total_workers: int = 4,
        base_allocation: Optional[ResourceAllocation] = None,
        enable_scaling: bool = True
    ):
        """
        åˆå§‹åŒ–åŠ¨æ€ç¼©æ”¾ç®¡ç†å™¨
        
        Args:
            worker_id: å½“å‰Workerçš„å”¯ä¸€æ ‡è¯†
            total_workers: æ€»Workeræ•°é‡
            base_allocation: åŸºç¡€èµ„æºåˆ†é…
            enable_scaling: æ˜¯å¦å¯ç”¨åŠ¨æ€ç¼©æ”¾
        """
        self.worker_id = worker_id
        
        # åˆ›å»ºWorkeræ´»åŠ¨ç›‘æ§å™¨
        self.monitor = WorkerActivityMonitor(
            worker_id=worker_id,
            total_workers=total_workers
        )
        
        # åˆ›å»ºåŠ¨æ€èµ„æºåˆ†é…å™¨
        self.allocator = DynamicResourceAllocator(
            worker_id=worker_id,
            total_workers=total_workers,
            base_allocation=base_allocation,
            monitor=self.monitor,
            enable_scaling=enable_scaling
        )
        
        logger.info(f"åŠ¨æ€ç¼©æ”¾ç®¡ç†å™¨å·²åˆå§‹åŒ–: {worker_id}")
    
    def start(self):
        """å¯åŠ¨åŠ¨æ€ç¼©æ”¾"""
        self.monitor.start_monitoring()
        self.allocator.start_adjustment()
        logger.info(f"åŠ¨æ€ç¼©æ”¾å·²å¯åŠ¨: {self.worker_id}")
    
    def stop(self):
        """åœæ­¢åŠ¨æ€ç¼©æ”¾"""
        self.allocator.cleanup()
        self.monitor.cleanup()
        logger.info(f"åŠ¨æ€ç¼©æ”¾å·²åœæ­¢: {self.worker_id}")
    
    def update_activity(
        self,
        is_active: bool,
        current_load: float = 0.0,
        active_tasks: int = 0
    ):
        """
        æ›´æ–°Workeræ´»åŠ¨çŠ¶æ€
        
        Args:
            is_active: æ˜¯å¦æ´»è·ƒ
            current_load: å½“å‰è´Ÿè½½ï¼ˆ0.0-1.0ï¼‰
            active_tasks: æ´»è·ƒä»»åŠ¡æ•°
        """
        self.monitor.update_local_state(is_active, current_load, active_tasks)
    
    def set_allocation_callback(self, callback: callable):
        """è®¾ç½®èµ„æºåˆ†é…å›è°ƒå‡½æ•°"""
        self.allocator.set_allocation_callback(callback)
    
    def get_current_allocation(self) -> ResourceAllocation:
        """è·å–å½“å‰èµ„æºåˆ†é…"""
        return self.allocator.get_current_allocation()
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–çŠ¶æ€"""
        return self.allocator.get_scaling_status()
    
    def __enter__(self):
        """æ”¯æŒä¸Šä¸‹æ–‡ç®¡ç†å™¨åè®®"""
        self.start()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡ºæ—¶åœæ­¢"""
        self.stop()
