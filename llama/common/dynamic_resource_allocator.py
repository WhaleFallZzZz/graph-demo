"""
åŠ¨æ€èµ„æºåˆ†é…å™¨ - è·¨WorkeråŠ¨æ€è°ƒæ•´èµ„æºåˆ†é…

å½“åªæœ‰ä¸€ä¸ªWorkeræ´»è·ƒæ—¶ï¼Œè‡ªåŠ¨å°†å…¶ä»–ç©ºé—²Workerçš„èµ„æºåˆ†é…ç»™æ´»è·ƒWorkerï¼Œ
å®ç°èµ„æºçš„æœ€ä¼˜åˆ©ç”¨å’Œæ€§èƒ½æå‡ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
- ç›‘æ§æ‰€æœ‰Workerçš„æ´»åŠ¨çŠ¶æ€
- åŠ¨æ€è°ƒæ•´å¹¶å‘è¯·æ±‚æ•°ã€RPMé™åˆ¶ã€TPMé™åˆ¶
- æ”¯æŒå¤šç§å…±äº«çŠ¶æ€å­˜å‚¨æ–¹å¼ï¼ˆRedis/æ–‡ä»¶ï¼‰
- ä¸ç°æœ‰é¢‘ç‡æ§åˆ¶ç³»ç»Ÿæ— ç¼é›†æˆ

ä½¿ç”¨ç¤ºä¾‹ï¼š
    from llama.common.dynamic_resource_allocator import DynamicScalingManager, ResourceAllocation
    
    # åˆ›å»ºåŸºç¡€èµ„æºåˆ†é…é…ç½®
    base_allocation = ResourceAllocation(
        max_concurrent_requests=5,
        rpm_limit=60,
        tpm_limit=100000,
        num_workers=3
    )
    
    # åˆ›å»ºåŠ¨æ€ç¼©æ”¾ç®¡ç†å™¨
    scaling_manager = DynamicScalingManager(
        worker_id='worker_1',
        total_workers=4,
        base_allocation=base_allocation,
        enable_scaling=True
    )
    
    # è®¾ç½®èµ„æºåˆ†é…å›è°ƒå‡½æ•°
    def apply_allocation(allocation):
        print(f"åº”ç”¨èµ„æºåˆ†é…: {allocation.to_dict()}")
    
    scaling_manager.set_allocation_callback(apply_allocation)
    
    # å¯åŠ¨åŠ¨æ€ç¼©æ”¾
    scaling_manager.start()
    
    # æ›´æ–°Workeræ´»åŠ¨çŠ¶æ€
    scaling_manager.update_activity(is_active=True, current_load=0.5, active_tasks=2)
    
    # è·å–å½“å‰èµ„æºåˆ†é…
    current_allocation = scaling_manager.get_current_allocation()
    
    # åœæ­¢åŠ¨æ€ç¼©æ”¾
    scaling_manager.stop()
"""

import os
import time
import json
import threading
import logging
import fcntl
import uuid
from typing import Dict, Optional, List, Any, Callable
from dataclasses import dataclass, asdict
from pathlib import Path

logger = logging.getLogger(__name__)


@dataclass
class WorkerState:
    """
    WorkerçŠ¶æ€ä¿¡æ¯
    
    å­˜å‚¨å•ä¸ªWorkerçš„è¿è¡Œæ—¶çŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ´»åŠ¨çŠ¶æ€ã€è´Ÿè½½ã€ä»»åŠ¡æ•°ç­‰ã€‚
    
    Attributes:
        worker_id: Workerçš„å”¯ä¸€æ ‡è¯†ç¬¦
        is_active: Workeræ˜¯å¦å¤„äºæ´»è·ƒçŠ¶æ€
        last_active_time: æœ€åä¸€æ¬¡æ´»åŠ¨çš„æ—¶é—´æˆ³ï¼ˆUnixæ—¶é—´æˆ³ï¼‰
        current_load: å½“å‰è´Ÿè½½ï¼ˆ0.0-1.0ï¼‰ï¼Œè¡¨ç¤ºWorkerçš„ç¹å¿™ç¨‹åº¦
        active_tasks: å½“å‰æ´»è·ƒçš„ä»»åŠ¡æ•°é‡
        allocated_resources: å·²åˆ†é…çš„èµ„æºå­—å…¸
    """
    worker_id: str
    is_active: bool
    last_active_time: float
    current_load: float
    active_tasks: int
    allocated_resources: Dict[str, int]


@dataclass
class ResourceAllocation:
    """
    èµ„æºåˆ†é…é…ç½®
    
    å®šä¹‰ç³»ç»Ÿå¯ç”¨çš„èµ„æºé™åˆ¶ï¼ŒåŒ…æ‹¬å¹¶å‘è¯·æ±‚æ•°ã€RPMï¼ˆæ¯åˆ†é’Ÿè¯·æ±‚æ•°ï¼‰ã€TPMï¼ˆæ¯åˆ†é’ŸTokenæ•°ï¼‰ç­‰ã€‚
    
    Attributes:
        max_concurrent_requests: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
        rpm_limit: RPMï¼ˆæ¯åˆ†é’Ÿè¯·æ±‚æ•°ï¼‰é™åˆ¶
        tpm_limit: TPMï¼ˆæ¯åˆ†é’ŸTokenæ•°ï¼‰é™åˆ¶
        num_workers: å·¥ä½œçº¿ç¨‹æ•°é‡
    """
    max_concurrent_requests: int
    rpm_limit: int
    tpm_limit: int
    num_workers: int
    
    def to_dict(self) -> Dict[str, int]:
        """
        è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        
        Returns:
            åŒ…å«æ‰€æœ‰èµ„æºåˆ†é…é…ç½®çš„å­—å…¸
        """
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict[str, int]) -> 'ResourceAllocation':
        """
        ä»å­—å…¸åˆ›å»ºResourceAllocationå®ä¾‹
        
        Args:
            data: åŒ…å«èµ„æºåˆ†é…é…ç½®çš„å­—å…¸
            
        Returns:
            ResourceAllocationå®ä¾‹
        """
        return cls(**data)


class WorkerActivityMonitor:
    """
    Workeræ´»åŠ¨ç›‘æ§å™¨ - è·Ÿè¸ªæ‰€æœ‰Workerçš„æ´»åŠ¨çŠ¶æ€
    
    è´Ÿè´£ç›‘æ§ç³»ç»Ÿä¸­æ‰€æœ‰Workerçš„æ´»åŠ¨çŠ¶æ€ï¼Œé€šè¿‡å…±äº«å­˜å‚¨ï¼ˆæ–‡ä»¶æˆ–Redisï¼‰åŒæ­¥çŠ¶æ€ä¿¡æ¯ã€‚
    å®šæœŸæ¸…ç†è¿‡æœŸçš„WorkerçŠ¶æ€ï¼Œå¹¶æä¾›æ´»è·ƒWorkeråˆ—è¡¨æŸ¥è¯¢åŠŸèƒ½ã€‚
    
    ä¸»è¦åŠŸèƒ½ï¼š
    - è·Ÿè¸ªæœ¬åœ°Workerçš„æ´»åŠ¨çŠ¶æ€
    - ä¸å…¶ä»–WorkeråŒæ­¥çŠ¶æ€ä¿¡æ¯
    - è‡ªåŠ¨æ¸…ç†è¿‡æœŸçš„WorkerçŠ¶æ€
    - æä¾›æ´»è·ƒWorkeråˆ—è¡¨æŸ¥è¯¢
    """
    
    def __init__(
        self,
        worker_id: str,
        total_workers: int = 4,
        activity_timeout: float = 30.0,
        monitoring_interval: float = 5.0,
        storage_backend: str = "file"
    ):
        """
        åˆå§‹åŒ–Workeræ´»åŠ¨ç›‘æ§å™¨
        
        Args:
            worker_id: å½“å‰Workerçš„å”¯ä¸€æ ‡è¯†
            total_workers: æ€»Workeræ•°é‡
            activity_timeout: æ´»åŠ¨è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œè¶…è¿‡æ­¤æ—¶é—´æœªæ´»åŠ¨åˆ™è§†ä¸ºç©ºé—²
            monitoring_interval: ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰ï¼Œå¤šä¹…åŒæ­¥ä¸€æ¬¡çŠ¶æ€
            storage_backend: å­˜å‚¨åç«¯ï¼ˆ"file" æˆ– "redis"ï¼‰
        """
        self.worker_id = worker_id
        self.total_workers = total_workers
        self.activity_timeout = activity_timeout
        self.monitoring_interval = monitoring_interval
        self.storage_backend = storage_backend
        
        # å…±äº«çŠ¶æ€å­˜å‚¨è·¯å¾„
        self.state_file = Path(os.getcwd()) / ".worker_activity_state.json"
        
        # æœ¬åœ°çŠ¶æ€
        self.local_state: WorkerState = WorkerState(
            worker_id=worker_id,
            is_active=False,
            last_active_time=time.time(),
            current_load=0.0,
            active_tasks=0,
            allocated_resources={}
        )
        
        # ç›‘æ§çº¿ç¨‹
        self.monitor_thread: Optional[threading.Thread] = None
        self.stop_monitoring = threading.Event()
        
        # çŠ¶æ€é”ï¼Œä¿æŠ¤æœ¬åœ°çŠ¶æ€çš„å¹¶å‘è®¿é—®
        self.state_lock = threading.Lock()
        
        logger.info(f"åˆå§‹åŒ–Workeræ´»åŠ¨ç›‘æ§å™¨: worker_id={worker_id}, total_workers={total_workers}")
    
    def _get_all_worker_states(self) -> Dict[str, WorkerState]:
        """
        è·å–æ‰€æœ‰Workerçš„çŠ¶æ€
        
        ä»å…±äº«å­˜å‚¨ä¸­è¯»å–æ‰€æœ‰Workerçš„çŠ¶æ€ä¿¡æ¯ã€‚
        
        Returns:
            Worker IDåˆ°WorkerStateçš„æ˜ å°„å­—å…¸
        """
        if self.storage_backend == "file":
            return self._read_from_file()
        elif self.storage_backend == "redis":
            return self._read_from_redis()
        else:
            logger.warning(f"æœªçŸ¥çš„å­˜å‚¨åç«¯: {self.storage_backend}ï¼Œä½¿ç”¨æœ¬åœ°çŠ¶æ€")
            return {self.worker_id: self.local_state}
    
    def _read_from_file(self) -> Dict[str, WorkerState]:
        """
        ä»æ–‡ä»¶è¯»å–æ‰€æœ‰WorkerçŠ¶æ€
        
        ä½¿ç”¨æ–‡ä»¶é”ç¡®ä¿è¯»å–ä¸€è‡´æ€§ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶ã€‚
        
        Returns:
            Worker IDåˆ°WorkerStateçš„æ˜ å°„å­—å…¸
        """
        max_retries = 3
        retry_delay = 0.1
        
        for attempt in range(max_retries):
            try:
                if not self.state_file.exists():
                    return {}
                
                # ä½¿ç”¨æ–‡ä»¶é”ç¡®ä¿è¯»å–ä¸€è‡´æ€§
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    # å°è¯•è·å–å…±äº«é”ï¼ˆè¯»å–é”ï¼‰
                    try:
                        fcntl.flock(f.fileno(), fcntl.LOCK_SH)
                    except (AttributeError, OSError):
                        pass
                    
                    try:
                        data = json.load(f)
                    finally:
                        try:
                            fcntl.flock(f.fileno(), fcntl.LOCK_UN)
                        except (AttributeError, OSError):
                            pass
                
                states = {}
                for worker_id, state_data in data.items():
                    states[worker_id] = WorkerState(**state_data)
                
                return states
                
            except json.JSONDecodeError as e:
                logger.warning(f"è§£æWorkerçŠ¶æ€æ–‡ä»¶å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰: {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay * (attempt + 1))
                else:
                    logger.error(f"è§£æWorkerçŠ¶æ€æ–‡ä»¶æœ€ç»ˆå¤±è´¥: {e}")
                    return {}
            except Exception as e:
                logger.warning(f"è¯»å–WorkerçŠ¶æ€æ–‡ä»¶å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰: {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay * (attempt + 1))
                else:
                    logger.error(f"è¯»å–WorkerçŠ¶æ€æ–‡ä»¶æœ€ç»ˆå¤±è´¥: {e}")
                    return {}
    
    def _write_to_file(self, states: Dict[str, WorkerState]):
        """
        å†™å…¥æ‰€æœ‰WorkerçŠ¶æ€åˆ°æ–‡ä»¶
        
        ä½¿ç”¨åŸå­å†™å…¥æ“ä½œï¼ˆå…ˆå†™ä¸´æ—¶æ–‡ä»¶ï¼Œå†æ›¿æ¢åŸæ–‡ä»¶ï¼‰ç¡®ä¿æ•°æ®ä¸€è‡´æ€§ã€‚
        ä½¿ç”¨æ–‡ä»¶é”ç¡®ä¿å†™å…¥çš„å¹¶å‘å®‰å…¨ã€‚
        
        Args:
            states: Worker IDåˆ°WorkerStateçš„æ˜ å°„å­—å…¸
        """
        max_retries = 3
        retry_delay = 0.1
        
        for attempt in range(max_retries):
            try:
                data = {}
                for worker_id, state in states.items():
                    data[worker_id] = asdict(state)
                
                # ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
                self.state_file.parent.mkdir(parents=True, exist_ok=True)
                
                # ä½¿ç”¨å”¯ä¸€çš„ä¸´æ—¶æ–‡ä»¶åï¼ˆåŒ…å«è¿›ç¨‹IDå’ŒWorker IDï¼‰
                temp_file = self.state_file.with_suffix(
                    f'.tmp.{os.getpid()}.{self.worker_id}.{uuid.uuid4().hex[:8]}'
                )
                
                # å†™å…¥ä¸´æ—¶æ–‡ä»¶
                with open(temp_file, 'w', encoding='utf-8') as f:
                    # å°è¯•è·å–æ’ä»–é”ï¼ˆå†™å…¥é”ï¼‰
                    try:
                        fcntl.flock(f.fileno(), fcntl.LOCK_EX)
                    except (AttributeError, OSError):
                        pass
                    
                    try:
                        json.dump(data, f, indent=2, ensure_ascii=False)
                        f.flush()
                        os.fsync(f.fileno())
                    finally:
                        try:
                            fcntl.flock(f.fileno(), fcntl.LOCK_UN)
                        except (AttributeError, OSError):
                            pass
                
                # åŸå­æ›¿æ¢
                temp_file.replace(self.state_file)
                
                return
                
            except Exception as e:
                logger.warning(f"å†™å…¥WorkerçŠ¶æ€æ–‡ä»¶å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰: {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay * (attempt + 1))
                else:
                    logger.error(f"å†™å…¥WorkerçŠ¶æ€æ–‡ä»¶æœ€ç»ˆå¤±è´¥: {e}")
    
    def _read_from_redis(self) -> Dict[str, WorkerState]:
        """
        ä»Redisè¯»å–æ‰€æœ‰WorkerçŠ¶æ€
        
        ä»Rediså“ˆå¸Œè¡¨ä¸­è¯»å–æ‰€æœ‰Workerçš„çŠ¶æ€ä¿¡æ¯ã€‚
        
        Returns:
            Worker IDåˆ°WorkerStateçš„æ˜ å°„å­—å…¸
        """
        try:
            import redis
            redis_client = redis.Redis(
                host=os.getenv('REDIS_HOST', 'localhost'),
                port=int(os.getenv('REDIS_PORT', 6379)),
                db=int(os.getenv('REDIS_DB', 0)),
                decode_responses=True
            )
            
            key = "worker_activity_states"
            data = redis_client.hgetall(key)
            
            states = {}
            for worker_id, state_json in data.items():
                state_data = json.loads(state_json)
                states[worker_id] = WorkerState(**state_data)
            
            return states
        except ImportError:
            logger.warning("Redisæœªå®‰è£…ï¼Œå›é€€åˆ°æ–‡ä»¶å­˜å‚¨")
            self.storage_backend = "file"
            return self._read_from_file()
        except Exception as e:
            logger.error(f"ä»Redisè¯»å–WorkerçŠ¶æ€å¤±è´¥: {e}")
            return {}
    
    def _write_to_redis(self, states: Dict[str, WorkerState]):
        """
        å†™å…¥æ‰€æœ‰WorkerçŠ¶æ€åˆ°Redis
        
        å°†æ‰€æœ‰Workerçš„çŠ¶æ€ä¿¡æ¯å†™å…¥Rediså“ˆå¸Œè¡¨ï¼Œå¹¶è®¾ç½®è¿‡æœŸæ—¶é—´ã€‚
        
        Args:
            states: Worker IDåˆ°WorkerStateçš„æ˜ å°„å­—å…¸
        """
        try:
            import redis
            redis_client = redis.Redis(
                host=os.getenv('REDIS_HOST', 'localhost'),
                port=int(os.getenv('REDIS_PORT', 6379)),
                db=int(os.getenv('REDIS_DB', 0)),
                decode_responses=True
            )
            
            key = "worker_activity_states"
            pipe = redis_client.pipeline()
            
            for worker_id, state in states.items():
                state_json = json.dumps(asdict(state), ensure_ascii=False)
                pipe.hset(key, worker_id, state_json)
            
            pipe.expire(key, 3600)
            pipe.execute()
            
        except Exception as e:
            logger.error(f"å†™å…¥Redis WorkerçŠ¶æ€å¤±è´¥: {e}")
    
    def update_local_state(
        self,
        is_active: bool,
        current_load: float = 0.0,
        active_tasks: int = 0
    ):
        """
        æ›´æ–°æœ¬åœ°WorkerçŠ¶æ€
        
        æ›´æ–°å½“å‰Workerçš„çŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ´»åŠ¨çŠ¶æ€ã€è´Ÿè½½å’Œä»»åŠ¡æ•°ã€‚
        å¦‚æœWorkerå¤„äºæ´»è·ƒçŠ¶æ€ï¼Œä¼šè‡ªåŠ¨æ›´æ–°æœ€åæ´»åŠ¨æ—¶é—´ã€‚
        
        Args:
            is_active: æ˜¯å¦æ´»è·ƒ
            current_load: å½“å‰è´Ÿè½½ï¼ˆ0.0-1.0ï¼‰
            active_tasks: æ´»è·ƒä»»åŠ¡æ•°
        """
        with self.state_lock:
            self.local_state.is_active = is_active
            self.local_state.current_load = current_load
            self.local_state.active_tasks = active_tasks
            
            if is_active:
                self.local_state.last_active_time = time.time()
            
            logger.debug(
                f"æ›´æ–°æœ¬åœ°WorkerçŠ¶æ€: {self.worker_id}, "
                f"active={is_active}, load={current_load:.2f}, tasks={active_tasks}"
            )
    
    def sync_state(self):
        """
        åŒæ­¥æœ¬åœ°çŠ¶æ€åˆ°å…±äº«å­˜å‚¨
        
        å°†æœ¬åœ°Workerçš„çŠ¶æ€åŒæ­¥åˆ°å…±äº«å­˜å‚¨ï¼ˆæ–‡ä»¶æˆ–Redisï¼‰ï¼Œ
        åŒæ—¶æ¸…ç†è¿‡æœŸçš„WorkerçŠ¶æ€ã€‚
        """
        with self.state_lock:
            all_states = self._get_all_worker_states()
            all_states[self.worker_id] = self.local_state
            
            # æ¸…ç†è¿‡æœŸçš„WorkerçŠ¶æ€
            current_time = time.time()
            expired_workers = []
            for worker_id, state in all_states.items():
                if current_time - state.last_active_time > self.activity_timeout * 2:
                    expired_workers.append(worker_id)
            
            for worker_id in expired_workers:
                del all_states[worker_id]
                logger.debug(f"æ¸…ç†è¿‡æœŸWorkerçŠ¶æ€: {worker_id}")
            
            # å†™å…¥å…±äº«å­˜å‚¨
            if self.storage_backend == "file":
                self._write_to_file(all_states)
            elif self.storage_backend == "redis":
                self._write_to_redis(all_states)
    
    def get_active_workers(self) -> List[str]:
        """
        è·å–å½“å‰æ´»è·ƒçš„Workeråˆ—è¡¨
        
        Returns:
            æ´»è·ƒWorker IDåˆ—è¡¨
        """
        all_states = self._get_all_worker_states()
        current_time = time.time()
        
        active_workers = []
        for worker_id, state in all_states.items():
            if state.is_active and (current_time - state.last_active_time) < self.activity_timeout:
                active_workers.append(worker_id)
        
        return active_workers
    
    def get_active_worker_count(self) -> int:
        """
        è·å–æ´»è·ƒWorkeræ•°é‡
        
        Returns:
            æ´»è·ƒWorkeræ•°é‡
        """
        return len(self.get_active_workers())
    
    def start_monitoring(self):
        """
        å¯åŠ¨ç›‘æ§çº¿ç¨‹
        
        å¯åŠ¨åå°ç›‘æ§çº¿ç¨‹ï¼Œå®šæœŸåŒæ­¥çŠ¶æ€å¹¶è®°å½•æ´»è·ƒWorkeræ•°é‡ã€‚
        """
        if self.monitor_thread is not None:
            return
        
        def monitor_loop():
            while not self.stop_monitoring.is_set():
                try:
                    # åŒæ­¥çŠ¶æ€
                    self.sync_state()
                    
                    # è®°å½•æ´»è·ƒWorkeræ•°é‡
                    active_count = self.get_active_worker_count()
                    logger.debug(f"å½“å‰æ´»è·ƒWorkeræ•°é‡: {active_count}/{self.total_workers}")
                    
                    # ç­‰å¾…ä¸‹ä¸€æ¬¡ç›‘æ§
                    self.stop_monitoring.wait(self.monitoring_interval)
                    
                except Exception as e:
                    logger.error(f"ç›‘æ§çº¿ç¨‹å‡ºé”™: {e}")
        
        self.monitor_thread = threading.Thread(
            target=monitor_loop,
            daemon=True,
            name=f"WorkerMonitor-{self.worker_id}"
        )
        self.monitor_thread.start()
        
        logger.info(f"Workerç›‘æ§çº¿ç¨‹å·²å¯åŠ¨: {self.worker_id}")
    
    def stop_monitoring_thread(self):
        """
        åœæ­¢ç›‘æ§çº¿ç¨‹
        
        åœæ­¢åå°ç›‘æ§çº¿ç¨‹ï¼Œç­‰å¾…çº¿ç¨‹é€€å‡ºã€‚
        """
        if self.monitor_thread:
            self.stop_monitoring.set()
            self.monitor_thread.join(timeout=5.0)
            logger.info(f"Workerç›‘æ§çº¿ç¨‹å·²åœæ­¢: {self.worker_id}")
    
    def cleanup(self):
        """
        æ¸…ç†èµ„æº
        
        åœæ­¢ç›‘æ§çº¿ç¨‹ï¼Œå¹¶ä»å…±äº«å­˜å‚¨ä¸­ç§»é™¤æœ¬Workerçš„çŠ¶æ€ã€‚
        """
        self.stop_monitoring_thread()
        
        # ä»å…±äº«å­˜å‚¨ä¸­ç§»é™¤æœ¬Workerçš„çŠ¶æ€
        try:
            all_states = self._get_all_worker_states()
            if self.worker_id in all_states:
                del all_states[self.worker_id]
                
                if self.storage_backend == "file":
                    self._write_to_file(all_states)
                elif self.storage_backend == "redis":
                    self._write_to_redis(all_states)
                
                logger.info(f"å·²æ¸…ç†WorkerçŠ¶æ€: {self.worker_id}")
        except Exception as e:
            logger.error(f"æ¸…ç†WorkerçŠ¶æ€å¤±è´¥: {e}")


class DynamicResourceAllocator:
    """
    åŠ¨æ€èµ„æºåˆ†é…å™¨ - æ ¹æ®æ´»è·ƒWorkeræ•°é‡åŠ¨æ€è°ƒæ•´èµ„æºåˆ†é…
    
    æ ¹æ®å½“å‰æ´»è·ƒçš„Workeræ•°é‡ï¼ŒåŠ¨æ€è°ƒæ•´æ¯ä¸ªWorkerå¯ç”¨çš„èµ„æºé™åˆ¶ã€‚
    å½“åªæœ‰ä¸€ä¸ªWorkeræ´»è·ƒæ—¶ï¼Œå°†æ‰€æœ‰èµ„æºåˆ†é…ç»™è¯¥Workerï¼Œå®ç°æ€§èƒ½æœ€å¤§åŒ–ã€‚
    å½“å¤šä¸ªWorkeræ´»è·ƒæ—¶ï¼Œå¹³å‡åˆ†é…èµ„æºã€‚
    
    ä¸»è¦åŠŸèƒ½ï¼š
    - ç›‘æ§æ´»è·ƒWorkeræ•°é‡
    - åŠ¨æ€è®¡ç®—æœ€ä¼˜èµ„æºåˆ†é…
    - é€šè¿‡å›è°ƒå‡½æ•°åº”ç”¨èµ„æºåˆ†é…
    - æä¾›èµ„æºåˆ†é…çŠ¶æ€æŸ¥è¯¢
    """
    
    def __init__(
        self,
        worker_id: str,
        total_workers: int = 4,
        base_allocation: Optional[ResourceAllocation] = None,
        monitor: Optional[WorkerActivityMonitor] = None,
        adjustment_interval: float = 10.0,
        enable_scaling: bool = True
    ):
        """
        åˆå§‹åŒ–åŠ¨æ€èµ„æºåˆ†é…å™¨
        
        Args:
            worker_id: å½“å‰Workerçš„å”¯ä¸€æ ‡è¯†
            total_workers: æ€»Workeræ•°é‡
            base_allocation: åŸºç¡€èµ„æºåˆ†é…ï¼ˆæ¯ä¸ªWorkerçš„é»˜è®¤èµ„æºï¼‰
            monitor: Workeræ´»åŠ¨ç›‘æ§å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨åˆ›å»º
            adjustment_interval: èµ„æºè°ƒæ•´é—´éš”ï¼ˆç§’ï¼‰
            enable_scaling: æ˜¯å¦å¯ç”¨åŠ¨æ€ç¼©æ”¾
        """
        self.worker_id = worker_id
        self.total_workers = total_workers
        self.adjustment_interval = adjustment_interval
        self.enable_scaling = enable_scaling
        
        # åŸºç¡€èµ„æºåˆ†é…ï¼ˆæ¯ä¸ªWorkerçš„é»˜è®¤èµ„æºï¼‰
        if base_allocation is None:
            base_allocation = ResourceAllocation(
                max_concurrent_requests=10,
                rpm_limit=200,
                tpm_limit=10000,
                num_workers=10
            )
        
        self.base_allocation = base_allocation
        
        # æ€»èµ„æºï¼ˆæ‰€æœ‰Workerå…±äº«ï¼‰
        self.total_resources = ResourceAllocation(
            max_concurrent_requests=base_allocation.max_concurrent_requests * total_workers,
            rpm_limit=base_allocation.rpm_limit * total_workers,
            tpm_limit=base_allocation.tpm_limit * total_workers,
            num_workers=base_allocation.num_workers * total_workers
        )
        
        # å½“å‰åˆ†é…çš„èµ„æº
        self.current_allocation = ResourceAllocation(**asdict(base_allocation))
        
        # Workeræ´»åŠ¨ç›‘æ§å™¨
        if monitor is None:
            monitor = WorkerActivityMonitor(
                worker_id=worker_id,
                total_workers=total_workers
            )
        self.monitor = monitor
        
        # è°ƒæ•´çº¿ç¨‹
        self.adjustment_thread: Optional[threading.Thread] = None
        self.stop_adjustment = threading.Event()
        
        # èµ„æºåˆ†é…å›è°ƒå‡½æ•°
        self.allocation_callback: Optional[Callable[[ResourceAllocation], None]] = None
        
        logger.info(
            f"åˆå§‹åŒ–åŠ¨æ€èµ„æºåˆ†é…å™¨: worker_id={worker_id}, "
            f"base_allocation={base_allocation.to_dict()}, "
            f"total_resources={self.total_resources.to_dict()}"
        )
    
    def set_allocation_callback(self, callback: Callable[[ResourceAllocation], None]):
        """
        è®¾ç½®èµ„æºåˆ†é…å›è°ƒå‡½æ•°
        
        å½“èµ„æºåˆ†é…å‘ç”Ÿå˜åŒ–æ—¶ï¼Œä¼šè°ƒç”¨æ­¤å›è°ƒå‡½æ•°ã€‚
        
        Args:
            callback: å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ResourceAllocationå‚æ•°
        """
        self.allocation_callback = callback
        logger.info("èµ„æºåˆ†é…å›è°ƒå‡½æ•°å·²è®¾ç½®")
    
    def calculate_optimal_allocation(self, active_workers: List[str]) -> ResourceAllocation:
        """
        è®¡ç®—æœ€ä¼˜èµ„æºåˆ†é…
        
        æ ¹æ®æ´»è·ƒWorkeræ•°é‡è®¡ç®—æœ€ä¼˜çš„èµ„æºåˆ†é…ç­–ç•¥ï¼š
        - 0ä¸ªæ´»è·ƒWorkerï¼šä½¿ç”¨åŸºç¡€åˆ†é…
        - 1ä¸ªæ´»è·ƒWorkerï¼šåˆ†é…æ‰€æœ‰èµ„æºï¼ˆæ€§èƒ½æœ€å¤§åŒ–ï¼‰
        - å¤šä¸ªæ´»è·ƒWorkerï¼šå¹³å‡åˆ†é…èµ„æº
        
        Args:
            active_workers: æ´»è·ƒWorkeråˆ—è¡¨
            
        Returns:
            æœ€ä¼˜èµ„æºåˆ†é…
        """
        active_count = len(active_workers)
        
        if active_count == 0:
            return ResourceAllocation(**asdict(self.base_allocation))
        
        if active_count == 1:
            logger.info(f"ğŸš€ æ¿€æ´»åŠ¨æ€ç¼©æ”¾: åªæœ‰1ä¸ªæ´»è·ƒWorkerï¼Œåˆ†é…å…¨éƒ¨èµ„æº")
            return ResourceAllocation(**asdict(self.total_resources))
        
        # å¤šä¸ªæ´»è·ƒWorkerï¼Œå¹³å‡åˆ†é…èµ„æº
        avg_concurrent = self.total_resources.max_concurrent_requests // active_count
        avg_rpm = self.total_resources.rpm_limit // active_count
        avg_tpm = self.total_resources.tpm_limit // active_count
        avg_workers = self.total_resources.num_workers // active_count
        
        allocation = ResourceAllocation(
            max_concurrent_requests=avg_concurrent,
            rpm_limit=avg_rpm,
            tpm_limit=avg_tpm,
            num_workers=avg_workers
        )
        
        logger.info(
            f"ğŸ“Š å¹³å‡åˆ†é…èµ„æº: {active_count}ä¸ªæ´»è·ƒWorker, "
            f"concurrent={avg_concurrent}, rpm={avg_rpm}, tpm={avg_tpm}, workers={avg_workers}"
        )
        
        return allocation
    
    def adjust_resources(self):
        """
        è°ƒæ•´èµ„æºåˆ†é…
        
        æ ¹æ®å½“å‰æ´»è·ƒWorkeræ•°é‡ï¼Œè®¡ç®—æœ€ä¼˜èµ„æºåˆ†é…å¹¶åº”ç”¨ã€‚
        å¦‚æœèµ„æºåˆ†é…å‘ç”Ÿå˜åŒ–ï¼Œä¼šè°ƒç”¨å›è°ƒå‡½æ•°é€šçŸ¥åº”ç”¨å±‚ã€‚
        """
        if not self.enable_scaling:
            return
        
        # è·å–æ´»è·ƒWorkeråˆ—è¡¨
        active_workers = self.monitor.get_active_workers()
        
        # è®¡ç®—æœ€ä¼˜åˆ†é…
        optimal_allocation = self.calculate_optimal_allocation(active_workers)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´
        if optimal_allocation.to_dict() != self.current_allocation.to_dict():
            old_allocation = self.current_allocation.to_dict()
            new_allocation = optimal_allocation.to_dict()
            
            self.current_allocation = optimal_allocation
            
            logger.info(
                f"ğŸ”„ èµ„æºåˆ†é…è°ƒæ•´: {self.worker_id}\n"
                f"  æ—§é…ç½®: {old_allocation}\n"
                f"  æ–°é…ç½®: {new_allocation}\n"
                f"  æ´»è·ƒWorker: {len(active_workers)}/{self.total_workers}"
            )
            
            # è°ƒç”¨å›è°ƒå‡½æ•°
            if self.allocation_callback:
                try:
                    self.allocation_callback(self.current_allocation)
                except Exception as e:
                    logger.error(f"èµ„æºåˆ†é…å›è°ƒå‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
    
    def start_adjustment(self):
        """
        å¯åŠ¨èµ„æºè°ƒæ•´çº¿ç¨‹
        
        å¯åŠ¨åå°è°ƒæ•´çº¿ç¨‹ï¼Œå®šæœŸæ£€æŸ¥å¹¶è°ƒæ•´èµ„æºåˆ†é…ã€‚
        """
        if self.adjustment_thread is not None:
            return
        
        def adjustment_loop():
            while not self.stop_adjustment.is_set():
                try:
                    # è°ƒæ•´èµ„æº
                    self.adjust_resources()
                    
                    # ç­‰å¾…ä¸‹ä¸€æ¬¡è°ƒæ•´
                    self.stop_adjustment.wait(self.adjustment_interval)
                    
                except Exception as e:
                    logger.error(f"èµ„æºè°ƒæ•´çº¿ç¨‹å‡ºé”™: {e}")
        
        self.adjustment_thread = threading.Thread(
            target=adjustment_loop,
            daemon=True,
            name=f"ResourceAdjuster-{self.worker_id}"
        )
        self.adjustment_thread.start()
        
        logger.info(f"èµ„æºè°ƒæ•´çº¿ç¨‹å·²å¯åŠ¨: {self.worker_id}")
    
    def stop_adjustment_thread(self):
        """
        åœæ­¢èµ„æºè°ƒæ•´çº¿ç¨‹
        
        åœæ­¢åå°è°ƒæ•´çº¿ç¨‹ï¼Œç­‰å¾…çº¿ç¨‹é€€å‡ºã€‚
        """
        if self.adjustment_thread:
            self.stop_adjustment.set()
            self.adjustment_thread.join(timeout=5.0)
            logger.info(f"èµ„æºè°ƒæ•´çº¿ç¨‹å·²åœæ­¢: {self.worker_id}")
    
    def get_current_allocation(self) -> ResourceAllocation:
        """
        è·å–å½“å‰èµ„æºåˆ†é…
        
        Returns:
            å½“å‰èµ„æºåˆ†é…é…ç½®
        """
        return self.current_allocation
    
    def get_scaling_status(self) -> Dict[str, Any]:
        """
        è·å–ç¼©æ”¾çŠ¶æ€
        
        Returns:
            åŒ…å«ç¼©æ”¾çŠ¶æ€ä¿¡æ¯çš„å­—å…¸ï¼ŒåŒ…æ‹¬ï¼š
            - worker_id: Worker ID
            - total_workers: æ€»Workeræ•°é‡
            - active_workers: æ´»è·ƒWorkeræ•°é‡
            - active_worker_ids: æ´»è·ƒWorker IDåˆ—è¡¨
            - is_scaling_enabled: æ˜¯å¦å¯ç”¨ç¼©æ”¾
            - base_allocation: åŸºç¡€èµ„æºåˆ†é…
            - current_allocation: å½“å‰èµ„æºåˆ†é…
            - total_resources: æ€»èµ„æº
            - utilization_ratio: èµ„æºåˆ©ç”¨ç‡
        """
        active_workers = self.monitor.get_active_workers()
        active_count = len(active_workers)
        
        return {
            'worker_id': self.worker_id,
            'total_workers': self.total_workers,
            'active_workers': active_count,
            'active_worker_ids': active_workers,
            'is_scaling_enabled': self.enable_scaling,
            'base_allocation': self.base_allocation.to_dict(),
            'current_allocation': self.current_allocation.to_dict(),
            'total_resources': self.total_resources.to_dict(),
            'utilization_ratio': {
                'concurrent': self.current_allocation.max_concurrent_requests / self.base_allocation.max_concurrent_requests,
                'rpm': self.current_allocation.rpm_limit / self.base_allocation.rpm_limit,
                'tpm': self.current_allocation.tpm_limit / self.base_allocation.tpm_limit,
                'workers': self.current_allocation.num_workers / self.base_allocation.num_workers
            }
        }
    
    def cleanup(self):
        """
        æ¸…ç†èµ„æº
        
        åœæ­¢è°ƒæ•´çº¿ç¨‹å’Œç›‘æ§çº¿ç¨‹ï¼Œé‡Šæ”¾æ‰€æœ‰èµ„æºã€‚
        """
        self.stop_adjustment_thread()
        self.monitor.cleanup()
        logger.info(f"åŠ¨æ€èµ„æºåˆ†é…å™¨å·²æ¸…ç†: {self.worker_id}")


class DynamicScalingManager:
    """
    åŠ¨æ€ç¼©æ”¾ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†Workerç›‘æ§å’Œèµ„æºåˆ†é…
    
    æä¾›ç»Ÿä¸€çš„æ¥å£æ¥ç®¡ç†åŠ¨æ€èµ„æºåˆ†é…åŠŸèƒ½ï¼Œç®€åŒ–ä½¿ç”¨ã€‚
    å†…éƒ¨é›†æˆäº†WorkerActivityMonitorå’ŒDynamicResourceAllocatorã€‚
    
    ä¸»è¦åŠŸèƒ½ï¼š
    - ç»Ÿä¸€ç®¡ç†Workerç›‘æ§å’Œèµ„æºåˆ†é…
    - æä¾›ç®€æ´çš„APIæ¥å£
    - æ”¯æŒä¸Šä¸‹æ–‡ç®¡ç†å™¨åè®®
    - æä¾›çŠ¶æ€æŸ¥è¯¢åŠŸèƒ½
    """
    
    def __init__(
        self,
        worker_id: str,
        total_workers: int = 4,
        base_allocation: Optional[ResourceAllocation] = None,
        enable_scaling: bool = True
    ):
        """
        åˆå§‹åŒ–åŠ¨æ€ç¼©æ”¾ç®¡ç†å™¨
        
        Args:
            worker_id: å½“å‰Workerçš„å”¯ä¸€æ ‡è¯†
            total_workers: æ€»Workeræ•°é‡
            base_allocation: åŸºç¡€èµ„æºåˆ†é…
            enable_scaling: æ˜¯å¦å¯ç”¨åŠ¨æ€ç¼©æ”¾
        """
        self.worker_id = worker_id
        
        # åˆ›å»ºWorkeræ´»åŠ¨ç›‘æ§å™¨
        self.monitor = WorkerActivityMonitor(
            worker_id=worker_id,
            total_workers=total_workers
        )
        
        # åˆ›å»ºåŠ¨æ€èµ„æºåˆ†é…å™¨
        self.allocator = DynamicResourceAllocator(
            worker_id=worker_id,
            total_workers=total_workers,
            base_allocation=base_allocation,
            monitor=self.monitor,
            enable_scaling=enable_scaling
        )
        
        logger.info(f"åŠ¨æ€ç¼©æ”¾ç®¡ç†å™¨å·²åˆå§‹åŒ–: {worker_id}")
    
    def start(self):
        """
        å¯åŠ¨åŠ¨æ€ç¼©æ”¾
        
        å¯åŠ¨Workerç›‘æ§å’Œèµ„æºè°ƒæ•´åŠŸèƒ½ã€‚
        """
        self.monitor.start_monitoring()
        self.allocator.start_adjustment()
        logger.info(f"åŠ¨æ€ç¼©æ”¾å·²å¯åŠ¨: {self.worker_id}")
    
    def stop(self):
        """
        åœæ­¢åŠ¨æ€ç¼©æ”¾
        
        åœæ­¢Workerç›‘æ§å’Œèµ„æºè°ƒæ•´åŠŸèƒ½ï¼Œé‡Šæ”¾æ‰€æœ‰èµ„æºã€‚
        """
        self.allocator.cleanup()
        self.monitor.cleanup()
        logger.info(f"åŠ¨æ€ç¼©æ”¾å·²åœæ­¢: {self.worker_id}")
    
    def update_activity(
        self,
        is_active: bool,
        current_load: float = 0.0,
        active_tasks: int = 0
    ):
        """
        æ›´æ–°Workeræ´»åŠ¨çŠ¶æ€
        
        Args:
            is_active: æ˜¯å¦æ´»è·ƒ
            current_load: å½“å‰è´Ÿè½½ï¼ˆ0.0-1.0ï¼‰
            active_tasks: æ´»è·ƒä»»åŠ¡æ•°
        """
        self.monitor.update_local_state(is_active, current_load, active_tasks)
    
    def set_allocation_callback(self, callback: Callable[[ResourceAllocation], None]):
        """
        è®¾ç½®èµ„æºåˆ†é…å›è°ƒå‡½æ•°
        
        Args:
            callback: å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ResourceAllocationå‚æ•°
        """
        self.allocator.set_allocation_callback(callback)
    
    def get_current_allocation(self) -> ResourceAllocation:
        """
        è·å–å½“å‰èµ„æºåˆ†é…
        
        Returns:
            å½“å‰èµ„æºåˆ†é…é…ç½®
        """
        return self.allocator.get_current_allocation()
    
    def get_status(self) -> Dict[str, Any]:
        """
        è·å–çŠ¶æ€
        
        Returns:
            åŒ…å«ç¼©æ”¾çŠ¶æ€ä¿¡æ¯çš„å­—å…¸
        """
        return self.allocator.get_scaling_status()
    
    def __enter__(self):
        """
        æ”¯æŒä¸Šä¸‹æ–‡ç®¡ç†å™¨åè®®
        
        è¿›å…¥ä¸Šä¸‹æ–‡æ—¶è‡ªåŠ¨å¯åŠ¨åŠ¨æ€ç¼©æ”¾ã€‚
        
        Returns:
            DynamicScalingManagerå®ä¾‹
        """
        self.start()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """
        ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡ºæ—¶åœæ­¢
        
        é€€å‡ºä¸Šä¸‹æ–‡æ—¶è‡ªåŠ¨åœæ­¢åŠ¨æ€ç¼©æ”¾ã€‚
        """
        self.stop()
