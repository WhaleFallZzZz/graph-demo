"""
åŠ¨æ€å¹¶å‘ç®¡ç†å™¨ - æ™ºèƒ½è°ƒæ•´å·¥ä½œçº¿ç¨‹æ•°
æ ¹æ®ç³»ç»Ÿè´Ÿè½½ã€é˜Ÿåˆ—çŠ¶æ€å’Œå¤„ç†é€Ÿåº¦åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°
"""

import os
import time
import threading
import logging
from typing import Optional, Dict, Any
from collections import deque
import psutil

logger = logging.getLogger(__name__)


class DynamicConcurrencyManager:
    """åŠ¨æ€å¹¶å‘ç®¡ç†å™¨ - æ ¹æ®è´Ÿè½½è‡ªåŠ¨è°ƒæ•´å·¥ä½œçº¿ç¨‹æ•°"""
    
    def __init__(
        self,
        min_workers: int = 2,
        max_workers: int = 16,
        workers_per_cpu_core: float = 1.5,
        scale_up_threshold: float = 0.8,
        scale_down_threshold: float = 0.3,
        monitoring_interval: int = 10,
        enable_monitoring: bool = True
    ):
        """
        åˆå§‹åŒ–åŠ¨æ€å¹¶å‘ç®¡ç†å™¨
        
        Args:
            min_workers: æœ€å°å·¥ä½œçº¿ç¨‹æ•°
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            workers_per_cpu_core: æ¯ä¸ªCPUæ ¸å¿ƒåˆ†é…çš„å·¥ä½œçº¿ç¨‹æ•°(å¯ä»¥æ˜¯æµ®ç‚¹æ•°)
            scale_up_threshold: è´Ÿè½½è¶…è¿‡æ­¤é˜ˆå€¼æ—¶æ‰©å®¹
            scale_down_threshold: è´Ÿè½½ä½äºæ­¤é˜ˆå€¼æ—¶ç¼©å®¹
            monitoring_interval: ç›‘æ§é—´éš”(ç§’)
            enable_monitoring: æ˜¯å¦å¯ç”¨æ€§èƒ½ç›‘æ§
        """
        self.min_workers = min_workers
        self.max_workers = max_workers
        self.workers_per_cpu_core = workers_per_cpu_core
        self.scale_up_threshold = scale_up_threshold
        self.scale_down_threshold = scale_down_threshold
        self.monitoring_interval = monitoring_interval
        self.enable_monitoring = enable_monitoring
        
        # åˆå§‹å·¥ä½œçº¿ç¨‹æ•°
        cpu_count = os.cpu_count() or 4
        self.current_workers = min(
            max_workers,
            max(min_workers, int(cpu_count * workers_per_cpu_core))
        )
        
        # æ€§èƒ½æŒ‡æ ‡
        self.metrics = {
            "queue_size": 0,
            "processing_rate": 0.0,  # æ¯ç§’å¤„ç†çš„ä»»åŠ¡æ•°
            "cpu_usage": 0.0,
            "memory_usage_mb": 0.0,
            "average_task_time": 0.0,
            "total_tasks_processed": 0,
            "last_adjustment_time": time.time()
        }
        
        # ä»»åŠ¡å¤„ç†æ—¶é—´è®°å½•(æ»‘åŠ¨çª—å£,æœ€è¿‘100ä¸ªä»»åŠ¡)
        self.task_times = deque(maxlen=100)
        
        # ç›‘æ§çº¿ç¨‹
        self.monitoring_thread: Optional[threading.Thread] = None
        self.stop_monitoring = threading.Event()
        
        if enable_monitoring:
            self.start_monitoring()
        
        logger.info(f"åˆå§‹åŒ–å¹¶å‘ç®¡ç†å™¨: current_workers={self.current_workers}, min={min_workers}, max={max_workers}")
    
    def calculate_optimal_workers(self, queue_size: int, avg_task_time: float) -> int:
        """
        è®¡ç®—æœ€ä¼˜å·¥ä½œçº¿ç¨‹æ•°
        
        Args:
            queue_size: å½“å‰é˜Ÿåˆ—å¤§å°
            avg_task_time: å¹³å‡ä»»åŠ¡å¤„ç†æ—¶é—´(ç§’)
        
        Returns:
            å»ºè®®çš„å·¥ä½œçº¿ç¨‹æ•°
        """
        # åŸºäºCPUæ ¸å¿ƒæ•°çš„åŸºå‡†
        cpu_count = os.cpu_count() or 4
        base_workers = int(cpu_count * self.workers_per_cpu_core)
        
        # åŸºäºé˜Ÿåˆ—ç§¯å‹æƒ…å†µè°ƒæ•´
        if queue_size > 0 and avg_task_time > 0:
            # ä¼°ç®—éœ€è¦å¤šå°‘çº¿ç¨‹æ‰èƒ½åœ¨åˆç†æ—¶é—´å†…æ¸…ç©ºé˜Ÿåˆ—
            # å‡è®¾ç›®æ ‡æ˜¯åœ¨60ç§’å†…å¤„ç†å®Œé˜Ÿåˆ—
            target_clearance_time = 60.0
            needed_workers = int((queue_size * avg_task_time) / target_clearance_time) + 1
            
            # ç»“åˆåŸºå‡†å’Œéœ€æ±‚
            optimal = max(base_workers, needed_workers)
        else:
            optimal = base_workers
        
        # åº”ç”¨é™åˆ¶
        optimal = max(self.min_workers, min(self.max_workers, optimal))
        
        # åŸºäºCPUå’Œå†…å­˜ä½¿ç”¨ç‡çš„é™åˆ¶
        cpu_usage = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        
        # å¦‚æœCPUæˆ–å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜,é™åˆ¶çº¿ç¨‹æ•°
        if cpu_usage > 85.0:
            optimal = max(self.min_workers, optimal - 2)
            logger.warning(f"CPUä½¿ç”¨ç‡è¿‡é«˜({cpu_usage:.1f}%),é™ä½å¹¶å‘æ•°åˆ° {optimal}")
        
        if memory_info.percent > 85.0:
            optimal = max(self.min_workers, optimal - 2)
            logger.warning(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜({memory_info.percent:.1f}%),é™ä½å¹¶å‘æ•°åˆ° {optimal}")
        
        return optimal
    
    def should_scale_up(self, queue_size: int, current_load: float) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰©å®¹"""
        if self.current_workers >= self.max_workers:
            return False
        
        # æ¡ä»¶1: é˜Ÿåˆ—ç§¯å‹ä¸¥é‡
        if queue_size > 50:
            return True
        
        # æ¡ä»¶2: è´Ÿè½½è¿‡é«˜
        if current_load > self.scale_up_threshold:
            return True
        
        return False
    
    def should_scale_down(self, queue_size: int, current_load: float) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç¼©å®¹"""
        if self.current_workers <= self.min_workers:
            return False
        
        # æ¡ä»¶1: é˜Ÿåˆ—å‡ ä¹ä¸ºç©º
        if queue_size < 10:
            # æ¡ä»¶2: è´Ÿè½½å¾ˆä½
            if current_load < self.scale_down_threshold:
                return True
        
        return False
    
    def adjust_workers(self, queue_size: int) -> int:
        """
        æ ¹æ®å½“å‰çŠ¶æ€è°ƒæ•´å·¥ä½œçº¿ç¨‹æ•°
        
        Args:
            queue_size: å½“å‰é˜Ÿåˆ—å¤§å°
        
        Returns:
            è°ƒæ•´åçš„å·¥ä½œçº¿ç¨‹æ•°
        """
        # è®¡ç®—å¹³å‡ä»»åŠ¡å¤„ç†æ—¶é—´
        avg_task_time = sum(self.task_times) / len(self.task_times) if self.task_times else 1.0
        
        # è®¡ç®—å½“å‰è´Ÿè½½(åŸºäºé˜Ÿåˆ—å¤§å°å’Œå¤„ç†é€Ÿåº¦)
        processing_capacity = self.current_workers / max(avg_task_time, 0.1)
        current_load = queue_size / max(processing_capacity, 1.0)
        
        # æ›´æ–°æŒ‡æ ‡
        self.metrics["queue_size"] = queue_size
        self.metrics["average_task_time"] = avg_task_time
        self.metrics["processing_rate"] = processing_capacity
        
        # è®¡ç®—æœ€ä¼˜å·¥ä½œçº¿ç¨‹æ•°
        optimal_workers = self.calculate_optimal_workers(queue_size, avg_task_time)
        
        # å†³ç­–é€»è¾‘
        new_workers = self.current_workers
        
        if self.should_scale_up(queue_size, current_load):
            # æ‰©å®¹:æ¯æ¬¡å¢åŠ 25%æˆ–è‡³å°‘2ä¸ª
            increment = max(2, int(self.current_workers * 0.25))
            new_workers = min(self.max_workers, self.current_workers + increment)
            logger.info(f"ğŸ”¼ æ‰©å®¹: {self.current_workers} -> {new_workers} (é˜Ÿåˆ—: {queue_size}, è´Ÿè½½: {current_load:.2f})")
        
        elif self.should_scale_down(queue_size, current_load):
            # ç¼©å®¹:æ¯æ¬¡å‡å°‘25%æˆ–è‡³å°‘1ä¸ª
            decrement = max(1, int(self.current_workers * 0.25))
            new_workers = max(self.min_workers, self.current_workers - decrement)
            logger.info(f"ğŸ”½ ç¼©å®¹: {self.current_workers} -> {new_workers} (é˜Ÿåˆ—: {queue_size}, è´Ÿè½½: {current_load:.2f})")
        
        # æ›´æ–°å½“å‰å·¥ä½œçº¿ç¨‹æ•°
        if new_workers != self.current_workers:
            self.current_workers = new_workers
            self.metrics["last_adjustment_time"] = time.time()
        
        return self.current_workers
    
    def record_task_completion(self, task_duration: float):
        """
        è®°å½•ä»»åŠ¡å®Œæˆæƒ…å†µ
        
        Args:
            task_duration: ä»»åŠ¡å¤„ç†æ—¶é—´(ç§’)
        """
        self.task_times.append(task_duration)
        self.metrics["total_tasks_processed"] += 1
    
    def get_current_workers(self) -> int:
        """è·å–å½“å‰å»ºè®®çš„å·¥ä½œçº¿ç¨‹æ•°"""
        return self.current_workers
    
    def start_monitoring(self):
        """å¯åŠ¨æ€§èƒ½ç›‘æ§çº¿ç¨‹"""
        if self.monitoring_thread is not None:
            return
        
        def monitor_loop():
            while not self.stop_monitoring.is_set():
                try:
                    # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                    self.metrics["cpu_usage"] = psutil.cpu_percent(interval=1.0)
                    
                    memory_info = psutil.virtual_memory()
                    self.metrics["memory_usage_mb"] = memory_info.used / (1024 * 1024)
                    
                    # è®°å½•æ—¥å¿—
                    if self.metrics["total_tasks_processed"] > 0:
                        logger.debug(
                            f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡: "
                            f"Workers={self.current_workers}, "
                            f"Queue={self.metrics['queue_size']}, "
                            f"CPU={self.metrics['cpu_usage']:.1f}%, "
                            f"Mem={self.metrics['memory_usage_mb']:.1f}MB, "
                            f"AvgTaskTime={self.metrics['average_task_time']:.2f}s, "
                            f"TotalTasks={self.metrics['total_tasks_processed']}"
                        )
                    
                    time.sleep(self.monitoring_interval)
                    
                except Exception as e:
                    logger.error(f"ç›‘æ§çº¿ç¨‹å‡ºé”™: {e}")
        
        self.monitoring_thread = threading.Thread(target=monitor_loop, daemon=True, name="ConcurrencyMonitor")
        self.monitoring_thread.start()
        logger.info("æ€§èƒ½ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")
    
    def stop_monitoring_thread(self):
        """åœæ­¢ç›‘æ§çº¿ç¨‹"""
        if self.monitoring_thread:
            self.stop_monitoring.set()
            self.monitoring_thread.join(timeout=5.0)
            logger.info("æ€§èƒ½ç›‘æ§çº¿ç¨‹å·²åœæ­¢")
    
    def get_metrics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        return self.metrics.copy()
    
    def __del__(self):
        """ææ„å‡½æ•°,ç¡®ä¿ç›‘æ§çº¿ç¨‹è¢«åœæ­¢"""
        self.stop_monitoring_thread()
