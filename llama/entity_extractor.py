"""
å¢å¼ºçš„å®ä½“ç±»å‹æå–å™¨ - å®Œå…¨ä¾èµ–LLMè¯­ä¹‰åˆ†æï¼Œæ— ä»»ä½•é™åˆ¶
"""

import logging
from typing import List, Tuple, Dict, Any, Optional
import json
import queue
import threading
import resource
import time
import sys
import asyncio
from concurrent.futures import ThreadPoolExecutor

# Import EntityNode and Relation from llama_index.core
from llama_index.core.graph_stores.types import EntityNode, Relation
from llama_index.core.schema import BaseNode
from llama_index.core.indices.property_graph import DynamicLLMPathExtractor

# å¯¼å…¥ common æ¨¡å—çš„å·¥å…·
from llama.common import (
    safe_json_parse,
    parse_llm_output,
    clean_text,
    sanitize_for_neo4j,
    DynamicThreadPool,
    TaskManager,
    DateTimeUtils,
    retry_on_failure_with_strategy,
    retry_on_failure
)

try:
    from enhanced_entity_extractor import StandardTermMapper
except ImportError:
    # Fallback if file not found or circular import
    logger.warning("StandardTermMapper not found in enhanced_entity_extractor.py")
    class StandardTermMapper:
        @classmethod
        def process_triplets(cls, triplets):
            return triplets

logger = logging.getLogger(__name__)

class EnhancedEntityExtractor:
    """å¢å¼ºçš„å®ä½“æå–å™¨ - å®Œå…¨ä¿¡ä»»LLMè¯­ä¹‰åˆ†æ"""
    
    @classmethod
    def extract_enhanced_triplets(cls, llm_output: str) -> List[Dict[str, Any]]:
        """æå–å¢å¼ºçš„ä¸‰å…ƒç»„ï¼Œå®Œå…¨ä¿¡ä»»LLMçš„è¯­ä¹‰åˆ†æç»“æœ"""
        enhanced_triplets = []
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—ä»¥æŸ¥çœ‹LLMåŸå§‹è¾“å‡º
        logger.info(f"LLMåŸå§‹è¾“å‡º (é•¿åº¦: {len(llm_output)}): {llm_output[:500]}...")
        
        # ä½¿ç”¨ common æ¨¡å—ä¸­çš„ parse_llm_output
        parsed_dicts = parse_llm_output(llm_output)
        
        if parsed_dicts:
            for item in parsed_dicts:
                head = item.get("head", "").strip()
                head_type = item.get("head_type", "").strip()
                relation = item.get("relation", "").strip()
                tail = item.get("tail", "").strip()
                tail_type = item.get("tail_type", "").strip()
                
                # åªæœ‰å½“head, relation, tailéƒ½å­˜åœ¨ä¸”ä¸å…¨æ˜¯æ ‡ç‚¹ç¬¦å·æ—¶æ‰æ·»åŠ 
                if head and relation and tail:
                    # é¿å…å°¾éƒ¨æ˜¯é€—å·ç­‰æ ‡ç‚¹ç¬¦å·çš„æ— æ•ˆæå–
                    if tail in {",", ".", "ã€‚", "ï¼Œ", "ã€"}:
                         logger.warning(f"æ£€æµ‹åˆ°æ— æ•ˆçš„å°¾éƒ¨å®ä½“(æ ‡ç‚¹ç¬¦å·): '{tail}'ï¼Œè·³è¿‡è¯¥ä¸‰å…ƒç»„")
                         continue

                    enhanced_triplets.append({
                        "head": head,
                        "head_type": head_type or "æ¦‚å¿µ",
                        "relation": relation,
                        "tail": tail,
                        "tail_type": tail_type or "æ¦‚å¿µ"
                    })
                    
                    logger.debug(f"æå–LLMè¯­ä¹‰ä¸‰å…ƒç»„: {head}({head_type}) - {relation} - {tail}({tail_type})")
        
        # åº”ç”¨æœ¯è¯­æ˜ å°„æ ‡å‡†åŒ– å…ˆæ³¨é‡Š
        enhanced_triplets = StandardTermMapper.process_triplets(enhanced_triplets)
        
        if not enhanced_triplets:
            logger.warning("æœªèƒ½ä»LLMè¾“å‡ºä¸­æå–åˆ°ä»»ä½•æœ‰æ•ˆçš„ä¸‰å…ƒç»„")
             
        return enhanced_triplets
    
    @classmethod
    def validate_llm_entity_types(cls, enhanced_triplets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """éªŒè¯LLMè¿”å›çš„å®ä½“ç±»å‹ - å®Œå…¨ä¿¡ä»»LLMï¼Œä¸å†è¿›è¡Œä»»ä½•é™åˆ¶"""
        # å®Œå…¨ä¿¡ä»»LLMçš„è¯­ä¹‰åˆ†æï¼Œä¸å†éªŒè¯ç±»å‹æ˜¯å¦åœ¨é¢„å®šä¹‰åˆ—è¡¨ä¸­
        # åªè¿›è¡ŒåŸºæœ¬çš„æ ¼å¼æ¸…ç†
        validated_triplets = []
        for triplet in enhanced_triplets:
            # åªè¿›è¡ŒåŸºæœ¬çš„éç©ºæ£€æŸ¥ï¼Œå®Œå…¨ä¿¡ä»»LLMçš„è¯­ä¹‰åˆ¤æ–­
            head_type = triplet.get("head_type", "æ¦‚å¿µ")
            tail_type = triplet.get("tail_type", "æ¦‚å¿µ")
            
            # åªæ¸…ç†ç©ºç™½å­—ç¬¦ï¼Œä¸å†è¿›è¡Œä»»ä½•ç±»å‹é™åˆ¶
            triplet["head_type"] = head_type.strip() if head_type else "æ¦‚å¿µ"
            triplet["tail_type"] = tail_type.strip() if tail_type else "æ¦‚å¿µ"
            
            validated_triplets.append(triplet)
        
        return validated_triplets

# ä¿®æ”¹ parse_llm_output_to_enhanced_triplets å‡½æ•°ä»¥è¿”å› EntityNode, Relation å¯¹è±¡
def parse_llm_output_to_enhanced_triplets(llm_output: str) -> List[Tuple[EntityNode, Relation, EntityNode]]:
    """å¢å¼ºçš„è§£æå‡½æ•°ï¼Œå®Œå…¨ä¿¡ä»»LLMçš„è¯­ä¹‰åˆ†æç»“æœï¼Œå¹¶æ¸…ç†ç‰¹æ®Šå­—ç¬¦"""
    from neo4j_text_sanitizer import Neo4jTextSanitizer
    
    enhanced_triplets_dicts = EnhancedEntityExtractor.extract_enhanced_triplets(llm_output)
    
    # éªŒè¯LLMè¿”å›çš„å®ä½“ç±»å‹ - å®Œå…¨ä¿¡ä»»æ¨¡å¼
    validated_triplets = EnhancedEntityExtractor.validate_llm_entity_types(enhanced_triplets_dicts)
    
    result_triplets = []
    for triplet_dict in validated_triplets:
        head_name = triplet_dict.get("head", "")
        head_type = triplet_dict.get("head_type", "æ¦‚å¿µ")
        relation_type = triplet_dict.get("relation", "å…³è”")
        tail_name = triplet_dict.get("tail", "")
        tail_type = triplet_dict.get("tail_type", "æ¦‚å¿µ")
        
        if head_name and relation_type and tail_name:
            # ä½¿ç”¨ common æ¨¡å—ä¸­çš„ clean_text è¿›è¡ŒåŸºæœ¬æ¸…ç†
            head_name = clean_text(head_name, remove_special=False)
            tail_name = clean_text(tail_name, remove_special=False)
            relation_type = clean_text(relation_type, remove_special=False)

            # ---------------------------------------------------------
            # å¼ºåˆ¶æ˜ å°„å±‚ (Standardization Error Fix) - ç”¨æˆ·è¯·æ±‚çš„å¼ºæ ¡éªŒé’©å­
            # ---------------------------------------------------------
            try:
                # å†æ¬¡å°è¯•æ ‡å‡†åŒ–ï¼Œç¡®ä¿åœ¨åˆ›å»ºèŠ‚ç‚¹å‰å¼ºåˆ¶åº”ç”¨æ ‡å‡†æœ¯è¯­
                std_head = StandardTermMapper.standardize(head_name)
                if std_head in StandardTermMapper.STANDARD_ENTITIES:
                    if head_name != std_head:
                        logger.info(f"ğŸ”§ å¼ºåˆ¶çº å (Head): {head_name} -> {std_head}")
                    head_name = std_head
                
                std_tail = StandardTermMapper.standardize(tail_name)
                if std_tail in StandardTermMapper.STANDARD_ENTITIES:
                    if tail_name != std_tail:
                        logger.info(f"ğŸ”§ å¼ºåˆ¶çº å (Tail): {tail_name} -> {std_tail}")
                    tail_name = std_tail
            except Exception as e:
                logger.warning(f"StandardTermMapper å¼ºæ ¡éªŒå¤±è´¥: {e}")
            # ---------------------------------------------------------
            
            # éªŒè¯ï¼šè·³è¿‡çº¯æ ‡ç‚¹æˆ–ç©ºçš„å®ä½“/å…³ç³»
            invalid_symbols = {",", ".", "ã€‚", "ï¼Œ", "ã€", " ", "\\", "/", ";", ":", "?", "!", "'", "\"", "(", ")", "[", "]", "{", "}", "-", "_", "+", "=", "*", "&", "^", "%", "$", "#", "@", "~", "`", "<", ">", "|"}
            
            def is_invalid(text):
                if not text: return True
                if text in invalid_symbols: return True
                return all(char in invalid_symbols for char in text)

            if is_invalid(head_name) or is_invalid(tail_name) or is_invalid(relation_type):
                logger.warning(f"è·³è¿‡æ— æ•ˆå®ä½“/å…³ç³»: '{head_name}' - '{relation_type}' - '{tail_name}'")
                continue
            
            # ä½¿ç”¨ common æ¨¡å—ä¸­çš„ sanitize_for_neo4j è¿›è¡Œ Neo4j å®‰å…¨æ¸…ç†
            # è®°å½•æ¸…ç†å‰çš„å€¼(ç”¨äºæ—¥å¿—å¯¹æ¯”)
            original_head = head_name
            original_tail = tail_name
            original_relation = relation_type
            original_head_type = head_type
            original_tail_type = tail_type
            
            # æ¸…ç†èŠ‚ç‚¹åç§°
            head_name = Neo4jTextSanitizer.sanitize_node_name(head_name)
            tail_name = Neo4jTextSanitizer.sanitize_node_name(tail_name)
            
            # æ¸…ç†å…³ç³»æ ‡ç­¾ï¼ˆåŒ…å«å…³ç³»è§„èŒƒåŒ–ï¼‰
            original_relation_length = len(relation_type)
            relation_type = Neo4jTextSanitizer.sanitize_relation_label(relation_type, max_length=10)
            
            # æ¸…ç†å®ä½“ç±»å‹(Label)
            head_type = Neo4jTextSanitizer.sanitize_entity_type(head_type)
            tail_type = Neo4jTextSanitizer.sanitize_entity_type(tail_type)
            
            # å¦‚æœæ¸…ç†åå‘ç”Ÿäº†å˜åŒ–ï¼Œè®°å½•æ—¥å¿—
            relation_changed = original_relation != relation_type
            relation_simplified = original_relation_length > 10 and len(relation_type) <= 10
            
            if (original_head != head_name or original_tail != tail_name or 
                relation_changed or original_head_type != head_type or 
                original_tail_type != tail_type):
                if relation_simplified:
                    logger.info(
                        f"ğŸ”§ å…³ç³»ç®€åŒ–: "
                        f"[{original_relation}] ({original_relation_length}å­—) -> [{relation_type}] ({len(relation_type)}å­—) | "
                        f"ä¸‰å…ƒç»„: {head_name} - {tail_name}"
                    )
                else:
                    logger.debug(
                        f"ğŸ§¹ å­—ç¬¦æ¸…ç†: "
                        f"[{original_head}({original_head_type})] -> [{head_name}({head_type})], "
                        f"[{original_relation}] -> [{relation_type}], "
                        f"[{original_tail}({original_tail_type})] -> [{tail_name}({tail_type})]"
                    )
            
            # å†æ¬¡éªŒè¯æ¸…ç†åä¸ä¸ºç©º
            if not head_name or not tail_name or not relation_type:
                logger.error(f"æ¸…ç†åå®ä½“/å…³ç³»ä¸ºç©ºï¼Œè·³è¿‡: {head_name} - {relation_type} - {tail_name}")
                continue
            # ===== æ¸…ç†ç»“æŸ =====

            logger.info(f"åˆ›å»ºè¯­ä¹‰ä¸‰å…ƒç»„: {head_name}({head_type}) - {relation_type} - {tail_name}({tail_type})")
                
            head_node = EntityNode(name=head_name, label=head_type)
            tail_node = EntityNode(name=tail_name, label=tail_type)
            
            relation = Relation(
                source_id=head_node.id,
                target_id=tail_node.id,
                label=relation_type
            )
            result_triplets.append((head_node, relation, tail_node))
        else:
            logger.warning(f"è·³è¿‡æ— æ•ˆä¸‰å…ƒç»„: {triplet_dict}")
            
    return result_triplets

# ä¿æŒåŸæœ‰çš„å‡½æ•°åå…¼å®¹æ€§
parse_dynamic_triplets = parse_llm_output_to_enhanced_triplets

class MultiStageLLMExtractor(DynamicLLMPathExtractor):
    """
    å¤šé˜¶æ®µLLMæå–å™¨ï¼š
    1. å¹¶è¡Œå®ä½“è¯†åˆ«
    2. ç”Ÿäº§è€…-æ¶ˆè´¹è€…å…³ç³»æå–
    """
    def __init__(
        self,
        llm: Any,
        entity_prompt: str,
        relation_prompt: str,
        num_workers: int = 4,
        max_triplets_per_chunk: int = 15,
        graph_store: Optional[Any] = None,
        lightweight_llm: Optional[Any] = None,
        **kwargs: Any,
    ) -> None:
        super().__init__(
            llm=llm,
            extract_prompt=entity_prompt, # å ä½ç¬¦
            parse_fn=None, # æˆ‘ä»¬å®ç°è‡ªå®šä¹‰é€»è¾‘
            num_workers=num_workers,
            max_triplets_per_chunk=max_triplets_per_chunk,
            **kwargs,
        )
        # ç»•è¿‡ Pydantic éªŒè¯ä»¥æ”¯æŒè‡ªå®šä¹‰å­—æ®µ
        object.__setattr__(self, "entity_prompt", entity_prompt)
        object.__setattr__(self, "relation_prompt", relation_prompt)
        object.__setattr__(self, "real_num_workers", num_workers)
        object.__setattr__(self, "graph_store", graph_store)
        object.__setattr__(self, "lightweight_llm", lightweight_llm or llm)
        
        # å†…å­˜ç›‘æ§é…ç½®
        object.__setattr__(self, "memory_threshold_mb", 100)
        object.__setattr__(self, "peak_memory_usage", 0)
        
        # æ–‡ä»¶å†™å…¥é”ï¼Œç”¨äºä¿å­˜JSONè¾“å‡º
        object.__setattr__(self, "_file_lock", threading.Lock())
        
        # å¼‚æ­¥æ–‡ä»¶å†™å…¥æ‰§è¡Œå™¨
        object.__setattr__(self, "_write_executor", ThreadPoolExecutor(max_workers=2, thread_name_prefix="async_writer"))
        
        # Neo4jæ‰¹é‡å†™å…¥ç¼“å†²åŒº
        object.__setattr__(self, "_node_buffer", {})
        object.__setattr__(self, "_relation_buffer", [])
        object.__setattr__(self, "_buffer_lock", threading.Lock())
        object.__setattr__(self, "_batch_write_threshold", 50)  # æ¯50ä¸ªä¸‰å…ƒç»„æ‰¹é‡å†™å…¥ä¸€æ¬¡

    @retry_on_failure_with_strategy(max_retries=3)
    def _call_llm_api(self, prompt: str, llm_instance: Any = None) -> str:
        """è°ƒç”¨LLM APIå¹¶è¿”å›ç»“æœ"""
        target_llm = llm_instance or self.llm
        response = target_llm.complete(prompt)
        return response.text

    @retry_on_failure(max_retries=3, delay=0.1)
    def _write_to_file(self, output_path: str, header: str, content: str) -> None:
        """å†™å…¥æ–‡ä»¶ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        with self._file_lock:
            with open(output_path, "a", encoding="utf-8") as f:
                f.write(header)
                f.write(content)
                f.write("\n\n")

    def _write_to_file_async(self, output_path: str, header: str, content: str) -> None:
        """å¼‚æ­¥å†™å…¥æ–‡ä»¶ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        def write_task():
            try:
                with self._file_lock:
                    with open(output_path, "a", encoding="utf-8") as f:
                        f.write(header)
                        f.write(content)
                        f.write("\n\n")
                logger.debug(f"âœ… å¼‚æ­¥å†™å…¥å®Œæˆ: {output_path}")
            except Exception as e:
                logger.error(f"âŒ å¼‚æ­¥å†™å…¥å¤±è´¥: {e}")
                raise
        
        # æäº¤åˆ°çº¿ç¨‹æ± å¼‚æ­¥æ‰§è¡Œ
        self._write_executor.submit(write_task)

    def _add_to_batch_buffer(self, nodes: List[EntityNode], relations: List[Relation]) -> bool:
        """æ·»åŠ èŠ‚ç‚¹å…³ç³»åˆ°æ‰¹é‡ç¼“å†²åŒºï¼Œè¿”å›æ˜¯å¦è¾¾åˆ°æ‰¹é‡å†™å…¥é˜ˆå€¼"""
        with self._buffer_lock:
            # æ·»åŠ èŠ‚ç‚¹åˆ°ç¼“å†²åŒºï¼ˆå»é‡ï¼‰
            for node in nodes:
                self._node_buffer[node.id] = node
            
            # æ·»åŠ å…³ç³»åˆ°ç¼“å†²åŒº
            self._relation_buffer.extend(relations)
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ‰¹é‡å†™å…¥é˜ˆå€¼
            return len(self._relation_buffer) >= self._batch_write_threshold

    def _flush_batch_buffer(self) -> None:
        """å°†ç¼“å†²åŒºçš„æ•°æ®æ‰¹é‡å†™å…¥Neo4j"""
        with self._buffer_lock:
            if not self._node_buffer and not self._relation_buffer:
                return
            
            try:
                start_write = time.time()
                
                # æ‰¹é‡å†™å…¥èŠ‚ç‚¹
                if self._node_buffer:
                    self.graph_store.upsert_nodes(list(self._node_buffer.values()))
                    logger.debug(f"âœ… æ‰¹é‡å†™å…¥ {len(self._node_buffer)} ä¸ªèŠ‚ç‚¹åˆ° Neo4j")
                
                # æ‰¹é‡å†™å…¥å…³ç³»
                if self._relation_buffer:
                    self.graph_store.upsert_relations(self._relation_buffer)
                    logger.debug(f"âœ… æ‰¹é‡å†™å…¥ {len(self._relation_buffer)} ä¸ªå…³ç³»åˆ° Neo4j")
                
                write_time = time.time() - start_write
                logger.info(f"âœ… æ‰¹é‡å†™å…¥å®Œæˆ: {len(self._node_buffer)} ä¸ªèŠ‚ç‚¹, {len(self._relation_buffer)} ä¸ªå…³ç³», è€—æ—¶ {write_time:.2f}ç§’")
                
                # æ¸…ç©ºç¼“å†²åŒº
                self._node_buffer.clear()
                self._relation_buffer.clear()
                
            except Exception as e:
                logger.error(f"âŒ æ‰¹é‡å†™å…¥ Neo4j å¤±è´¥: {e}")
                # æ¸…ç©ºç¼“å†²åŒºä»¥é¿å…é‡å¤å†™å…¥
                self._node_buffer.clear()
                self._relation_buffer.clear()
                raise

    def _safe_llm_call(self, prompt: str, max_retries: int = 3, llm_instance: Any = None) -> str:
        """ä½¿ç”¨å¢å¼ºçš„é‡è¯•æœºåˆ¶å’Œç¼“å­˜è°ƒç”¨LLM"""
        from llm_cache_manager import get_global_cache
        
        target_llm = llm_instance or self.llm
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cache = get_global_cache()
        cached_result = cache.get(prompt, model_params={
            "temperature": 0.0,
            "model": getattr(target_llm, "model", "unknown")
        })
        
        if cached_result:
            logger.debug("ä½¿ç”¨ç¼“å­˜çš„LLMå“åº”")
            return cached_result
        
        # è°ƒç”¨ LLM APIï¼ˆå¸¦é‡è¯•ï¼‰
        result = self._call_llm_api(prompt, llm_instance)
        
        # ç¼“å­˜æˆåŠŸçš„ç»“æœ
        cache.put(prompt, result, model_params={
            "temperature": 0.0,
            "model": getattr(target_llm, "model", "unknown")
        })
        
        return result

    def _save_json_output(self, node: BaseNode, triplets: List[Tuple]) -> None:
        """
        å®‰å…¨åœ°å°†LLMè¾“å‡ºä¿å­˜åˆ°JSONæ–‡ä»¶ï¼ŒåŒ…å«å…ƒæ•°æ®ã€‚
        æ ¼å¼ï¼šåœ¨ "llm_outputs/{date}/" ç›®å½•ä¸‹ä¿å­˜ä¸º "original_filename-json.txt"
        """
        import os
        
        try:
            # 1. å‡†å¤‡æ•°æ®
            file_name = node.metadata.get('file_name', 'unknown_file')
            safe_filename = os.path.basename(file_name)
            
            # å¦‚æœå¯èƒ½ï¼Œç§»é™¤æ‰©å±•åä»¥è·å¾—æ›´æ¸…æ™°çš„å‘½å
            if '.' in safe_filename:
                base_name = safe_filename.rsplit('.', 1)[0]
            else:
                base_name = safe_filename
                
            json_data = {
                "node_id": node.node_id,
                "file_name": file_name,
                "timestamp": DateTimeUtils.format_iso_datetime(DateTimeUtils.now()),
                "triplets": [
                    {
                        "head": t[0].name,
                        "head_type": t[0].label,
                        "relation": t[1].label,
                        "tail": t[2].name,
                        "tail_type": t[2].label
                    }
                    for t in triplets
                ]
            }
            
            # 2. å‡†å¤‡ç›®å½•
            today_str = DateTimeUtils.today_str()
            storage_dir = os.path.join(os.getcwd(), "llm_outputs", today_str)
            
            # ä½¿ç”¨é”åˆ›å»ºç›®å½•ä»¥é¿å…ç«æ€æ¡ä»¶
            with self._file_lock:
                if not os.path.exists(storage_dir):
                    os.makedirs(storage_dir, exist_ok=True)
            
            # 3. å‡†å¤‡æ–‡ä»¶å
            output_filename = f"{base_name}-json.txt"
            output_path = os.path.join(storage_dir, output_filename)
            
            # 4. æ ¼å¼åŒ–å†…å®¹
            current_time_str = DateTimeUtils.now_str()
            header = f"/* å¤„ç†æ—¶é—´: {current_time_str} */\n"
            content = json.dumps(json_data, ensure_ascii=False, indent=2)
            
            # 5. å†™å…¥æ–‡ä»¶ï¼ˆå¼‚æ­¥ï¼Œå¸¦é‡è¯•ï¼‰
            try:
                self._write_to_file_async(output_path, header, content)
                logger.info(f"âœ… JSONè¾“å‡ºå·²ä¿å­˜ï¼ˆå¼‚æ­¥ï¼‰åˆ°: {output_path}")
            except Exception as write_err:
                logger.error(f"æ— æ³•å°†JSONè¾“å‡ºå†™å…¥æ–‡ä»¶: {write_err}")
                raise write_err
            
            # ç›‘æ§æ—¥å¿—
            process_time = time.time() - start_time
            logger.info(f"æ€§èƒ½: èŠ‚ç‚¹ {node.node_id[:8]} å¤„ç†è€—æ—¶ {process_time:.4f}ç§’ã€‚æå–äº† {len(triplets)} ä¸ªä¸‰å…ƒç»„ã€‚")
                        
        except Exception as e:
            logger.error(f"æ— æ³•ä¸ºèŠ‚ç‚¹ {node.node_id} ä¿å­˜JSONè¾“å‡º: {e}")

    def extract(self, nodes: List[BaseNode]) -> List[Dict[str, Any]]:
        results = [{} for _ in range(len(nodes))]
        # é™åˆ¶é˜Ÿåˆ—å¤§å°ä»¥æ§åˆ¶å†…å­˜ç¼“å†²åŒºï¼ˆçº¦100ä¸ªæ–‡æœ¬å—ï¼‰
        relation_queue = queue.Queue(maxsize=100)
        
        # å†…å­˜ç›‘æ§è¾…åŠ©å‡½æ•°
        def check_memory():
            try:
                # è·å–å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰
                rss = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
                if sys.platform == 'darwin':
                    usage_mb = rss / (1024 * 1024)
                else:
                    usage_mb = rss / 1024
                
                if usage_mb > self.peak_memory_usage:
                    object.__setattr__(self, "peak_memory_usage", usage_mb)
                    
                if usage_mb > self.memory_threshold_mb:
                    logger.warning(f"âš ï¸ å†…å­˜ä½¿ç”¨é‡ {usage_mb:.2f}MB è¶…è¿‡é˜ˆå€¼ {self.memory_threshold_mb}MB")
            except Exception:
                pass

        # é˜¶æ®µ1ï¼šæ‰¹é‡å®ä½“æå–ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        def batch_entity_producer():
            """æ‰¹é‡å®ä½“æå– - ä¼˜åŒ–ç‰ˆæœ¬"""
            try:
                # æ‰¹é‡æ”¶é›†æ‰€æœ‰æ–‡æœ¬
                batch_size = 10
                for i in range(0, len(nodes), batch_size):
                    batch_nodes = nodes[i:i+batch_size]
                    batch_indices = list(range(i, min(i+batch_size, len(nodes))))
                    
                    # æ„å»ºæ‰¹é‡prompt
                    batch_prompt = self._build_batch_entity_prompt(batch_nodes)
                    
                    # ä½¿ç”¨è½»é‡çº§LLMè¿›è¡Œæ‰¹é‡å®ä½“è¯†åˆ«
                    output = self._safe_llm_call(batch_prompt, llm_instance=self.lightweight_llm)
                    
                    # è§£ææ‰¹é‡ç»“æœ
                    batch_entities = self._parse_batch_entities(output, batch_indices)
                    
                    # å°†ç»“æœæ”¾å…¥é˜Ÿåˆ—
                    for node_idx, node in zip(batch_indices, batch_nodes):
                        entities = batch_entities.get(node_idx, [])
                        relation_queue.put((node_idx, node, entities))
                        logger.debug(f"é˜¶æ®µ1ï¼ˆæ‰¹é‡å®ä½“ï¼‰å®ŒæˆèŠ‚ç‚¹ {node_idx}ï¼Œå‘ç° {len(entities)} ä¸ªå®ä½“")
                    
                    logger.info(f"âœ… æ‰¹æ¬¡ {i//batch_size + 1}: å¤„ç†äº† {len(batch_nodes)} ä¸ªèŠ‚ç‚¹")
                    
            except Exception as e:
                logger.error(f"é˜¶æ®µ1ï¼ˆæ‰¹é‡å®ä½“ï¼‰å¤±è´¥: {e}")
                # å›é€€åˆ°å•ç‹¬å¤„ç†
                for node_idx, node in enumerate(nodes):
                    try:
                        prompt = self.entity_prompt.format(text=node.text)
                        output = self._safe_llm_call(prompt, llm_instance=self.lightweight_llm)
                        entities = self._parse_entities(output)
                        relation_queue.put((node_idx, node, entities))
                    except Exception as err:
                        logger.error(f"èŠ‚ç‚¹ {node_idx} çš„å›é€€å®ä½“æå–å¤±è´¥: {err}")
                        relation_queue.put((node_idx, node, []))

        def _build_batch_entity_prompt(self, batch_nodes: List[BaseNode]) -> str:
            """æ„å»ºæ‰¹é‡å®ä½“æå–çš„prompt"""
            prompt_parts = ["è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“ï¼Œæ¯ä¸ªæ–‡æœ¬ç”¨ç¼–å·æ ‡è¯†ï¼š\n"]
            
            for idx, node in enumerate(batch_nodes):
                prompt_parts.append(f"[{idx}] {node.text}\n")
            
            prompt_parts.append("\nè¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n")
            prompt_parts.append("{\n")
            prompt_parts.append('  "results": [\n')
            prompt_parts.append('    {"index": 0, "entities": [{"name": "å®ä½“å", "type": "å®ä½“ç±»å‹"}]},\n')
            prompt_parts.append('    {"index": 1, "entities": [{"name": "å®ä½“å", "type": "å®ä½“ç±»å‹"}]}\n')
            prompt_parts.append('  ]\n')
            prompt_parts.append("}\n")
            
            return "".join(prompt_parts)

        def _parse_batch_entities(self, output: str, batch_indices: List[int]) -> Dict[int, List[Dict[str, str]]]:
            """è§£ææ‰¹é‡å®ä½“æå–ç»“æœ"""
            batch_entities = {}
            
            try:
                parsed = safe_json_parse(output)
                results = parsed.get("results", [])
                
                for result in results:
                    idx = result.get("index")
                    entities = result.get("entities", [])
                    if idx in batch_indices:
                        batch_entities[idx] = entities
                        
            except Exception as e:
                logger.error(f"è§£ææ‰¹é‡å®ä½“å¤±è´¥: {e}")
                # è¿”å›ç©ºå­—å…¸ï¼Œè§¦å‘å›é€€
                pass
            
            return batch_entities

        # é˜¶æ®µ2ï¼šæ‰¹é‡å…³ç³»æå–ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        def relation_consumer():
            """æ‰¹é‡å…³ç³»æå– - ä¼˜åŒ–ç‰ˆæœ¬"""
            batch_buffer = []
            batch_size = 5
            batch_timeout = 2.0  # ç§’
            
            while True:
                try:
                    # ä»é˜Ÿåˆ—è·å–æ•°æ®ï¼Œå¸¦è¶…æ—¶
                    item = relation_queue.get(timeout=batch_timeout)
                    
                    if item is None:
                        # å¤„ç†ç¼“å†²åŒºä¸­çš„å‰©ä½™æ•°æ®
                        if batch_buffer:
                            self._process_batch_relations(batch_buffer)
                            batch_buffer = []
                        break
                    
                    batch_buffer.append(item)
                    
                    # è¾¾åˆ°æ‰¹é‡å¤§å°æ—¶å¤„ç†
                    if len(batch_buffer) >= batch_size:
                        self._process_batch_relations(batch_buffer)
                        batch_buffer = []
                        
                except queue.Empty:
                    # è¶…æ—¶åå¤„ç†ç¼“å†²åŒºä¸­çš„æ•°æ®
                    if batch_buffer:
                        self._process_batch_relations(batch_buffer)
                        batch_buffer = []
                except Exception as e:
                    logger.error(f"å…³ç³»æ¶ˆè´¹è€…é”™è¯¯: {e}")
                finally:
                    if item is not None:
                        relation_queue.task_done()

        def _process_batch_relations(self, batch_items: List[Tuple]) -> None:
            """æ‰¹é‡å¤„ç†å…³ç³»æå–"""
            if not batch_items:
                return
            
            logger.info(f"ğŸ”„ æ­£åœ¨å¤„ç† {len(batch_items)} ä¸ªå…³ç³»æå–çš„æ‰¹æ¬¡")
            
            for node_idx, node, entities in batch_items:
                if not entities:
                    continue
                    
                try:
                    entities_str = json.dumps(entities, ensure_ascii=False)
                    prompt = self.relation_prompt.format(text=node.text, entities=entities_str)
                    
                    output = self._safe_llm_call(prompt)
                    
                    # ä½¿ç”¨ç°æœ‰çš„è§£æé€»è¾‘
                    triplets = parse_llm_output_to_enhanced_triplets(output)
                    
                    # ä½¿ç”¨æ–°çš„ç¨³å¥æ–¹æ³•ä¿å­˜JSONè¾“å‡º
                    self._save_json_output(node, triplets)

                    # å¦‚æœ graph_store å¯ç”¨ï¼Œç›´æ¥å†™å…¥ï¼ˆä½¿ç”¨æ‰¹é‡ç¼“å†²åŒºä¼˜åŒ–ï¼‰
                    if self.graph_store and triplets:
                        # æå–èŠ‚ç‚¹å’Œå…³ç³»
                        head_nodes = [t[0] for t in triplets]
                        tail_nodes = [t[2] for t in triplets]
                        relations = [t[1] for t in triplets]
                        
                        # æ·»åŠ åˆ°æ‰¹é‡ç¼“å†²åŒº
                        should_flush = self._add_to_batch_buffer(head_nodes + tail_nodes, relations)
                        
                        # å¦‚æœè¾¾åˆ°é˜ˆå€¼ï¼Œåˆ·æ–°ç¼“å†²åŒº
                        if should_flush:
                            self._flush_batch_buffer()
                        
                        # æ›´æ–°ç»“æœ
                        results[node_idx] = {
                            "kg_triplets": [], 
                            "saved_to_neo4j": True, 
                            "count": len(triplets)
                        }
                    else:
                        # å›é€€åˆ°å†…å­˜å­˜å‚¨
                        results[node_idx] = {"kg_triplets": triplets}
                    
                    logger.debug(f"é˜¶æ®µ2ï¼ˆå…³ç³»ï¼‰å®ŒæˆèŠ‚ç‚¹ {node_idx}ï¼Œå‘ç° {len(triplets)} ä¸ªä¸‰å…ƒç»„")
                    
                    # å®šæœŸæ£€æŸ¥å†…å­˜
                    check_memory()
                    
                except Exception as e:
                    logger.error(f"èŠ‚ç‚¹ {node_idx} çš„é˜¶æ®µ2ï¼ˆå…³ç³»ï¼‰å¤±è´¥: {e}")

        # å¯åŠ¨æ¶ˆè´¹è€…
        consumer_threads = []
        num_consumers = max(1, self.real_num_workers // 2)
        for _ in range(num_consumers):
            t = threading.Thread(target=relation_consumer)
            t.start()
            consumer_threads.append(t)
            
        # å¯åŠ¨æ‰¹é‡ç”Ÿäº§è€…ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        logger.info("å¯åŠ¨æ‰¹é‡å®ä½“æå–ï¼ˆé˜¶æ®µ1ï¼‰...")
        batch_entity_producer()
        logger.info("æ‰¹é‡å®ä½“æå–ï¼ˆé˜¶æ®µ1ï¼‰å®Œæˆã€‚ç­‰å¾…å…³ç³»æå–ï¼ˆé˜¶æ®µ2ï¼‰...")
        
        # åœæ­¢æ¶ˆè´¹è€…
        for _ in range(num_consumers):
            relation_queue.put(None)
        
        for t in consumer_threads:
            t.join()
        
        # å°†å‰©ä½™çš„æ‰¹é‡ç¼“å†²åŒºåˆ·æ–°åˆ°Neo4j
        if self.graph_store:
            logger.info("å°†å‰©ä½™çš„æ‰¹é‡ç¼“å†²åŒºåˆ·æ–°åˆ°Neo4j...")
            self._flush_batch_buffer()
            
        return results

    def _parse_entities(self, output: str) -> List[Dict[str, str]]:
        try:
            return safe_json_parse(output)
        except:
            # å›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼
            import re
            matches = re.findall(r'\{\s*"name"\s*:\s*"(.*?)",\s*"type"\s*:\s*"(.*?)"\s*\}', output)
            return [{"name": m[0], "type": m[1]} for m in matches]
