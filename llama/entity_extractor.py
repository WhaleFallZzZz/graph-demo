"""
å¢å¼ºçš„å®ä½“ç±»å‹æå–å™¨ - å®Œå…¨ä¾èµ–LLMè¯­ä¹‰åˆ†æï¼Œæ— ä»»ä½•é™åˆ¶
"""

import logging
from typing import List, Tuple, Dict, Any, Optional
import json
import queue
import threading
import resource
import time
import sys
import asyncio
import re
from concurrent.futures import ThreadPoolExecutor

# Import EntityNode and Relation from llama_index.core
from llama_index.core.graph_stores.types import EntityNode, Relation
from llama_index.core.schema import BaseNode
from llama_index.core.indices.property_graph import DynamicLLMPathExtractor

# å¯¼å…¥ common æ¨¡å—çš„å·¥å…·
from llama.common import (
    safe_json_parse,
    parse_llm_output,
    clean_text,
    sanitize_for_neo4j,
    DynamicThreadPool,
    retry_on_failure
)
from llama.config import RERANK_CONFIG, EXTRACTOR_CONFIG
from llama.custom_siliconflow_rerank import CustomSiliconFlowRerank
from llama_index.core.schema import TextNode, NodeWithScore, QueryBundle

logger = logging.getLogger(__name__)

ENTITY_TYPE_SCHEMA: Dict[str, str] = {}

# æ³¨é‡Š StandardTermMapper (æ ‡å‡†è¯æ˜ å°„) ç›¸å…³ä»£ç 
# try:
#     from enhanced_entity_extractor import StandardTermMapper
# except ImportError:
#     # Fallback if file not found or circular import
#     logger.warning("StandardTermMapper not found in enhanced_entity_extractor.py")
#     class StandardTermMapper:
#         @classmethod
#         def process_triplets(cls, triplets):
#             return triplets

class EnhancedEntityExtractor:
    """å¢å¼ºçš„å®ä½“æå–å™¨ - å®Œå…¨ä¿¡ä»»LLMè¯­ä¹‰åˆ†æ"""
    
    @classmethod
    def extract_enhanced_triplets(cls, llm_output: str) -> List[Dict[str, Any]]:
        enhanced_triplets = []
        
       # --- æ–°å¢ï¼šæ¸…æ´— R1 æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ ---
        # 1. å»é™¤ <think> æ ‡ç­¾åŠå†…å®¹
        llm_output = re.sub(r'<think>.*?</think>', '', llm_output, flags=re.DOTALL)
        
        # 2. å»é™¤ Markdown ä»£ç å—æ ‡è®° (```json ... ```)
        llm_output = re.sub(r'^```json\s*', '', llm_output, flags=re.MULTILINE)
        llm_output = re.sub(r'^```\s*', '', llm_output, flags=re.MULTILINE)
        
        # 3. æ¸…ç†é¦–å°¾ç©ºç™½
        llm_output = llm_output.strip()
        # -----------------------------------
        try:
            start = llm_output.find('[')
            end = llm_output.rfind(']')
            if start != -1 and end != -1:
                json_str = llm_output[start : end + 1]
                # å°è¯•ç›´æ¥è§£æ
                return json.loads(json_str)
        except Exception:
            pass
        
        logger.info(f"æ¸…æ´—åçš„ LLM è¾“å‡º: {llm_output[:200]}...") # è°ƒè¯•æ—¥å¿—
        
        # ä½¿ç”¨ common æ¨¡å—ä¸­çš„ parse_llm_output
        parsed_dicts = parse_llm_output(llm_output)
        
        if parsed_dicts:
            for item in parsed_dicts:
                head = (item.get("head") or "").strip()
                head_type = (item.get("head_type") or "").strip()
                relation = (item.get("relation") or "").strip()
                tail = (item.get("tail") or "").strip()
                tail_type = (item.get("tail_type") or "").strip()
                
                # åªæœ‰å½“head, relation, tailéƒ½å­˜åœ¨ä¸”ä¸å…¨æ˜¯æ ‡ç‚¹ç¬¦å·æ—¶æ‰æ·»åŠ 
                if head and relation and tail:
                    # é¿å…å°¾éƒ¨æ˜¯é€—å·ç­‰æ ‡ç‚¹ç¬¦å·çš„æ— æ•ˆæå–
                    if tail in {",", ".", "ã€‚", "ï¼Œ", "ã€"}:
                         logger.warning(f"æ£€æµ‹åˆ°æ— æ•ˆçš„å°¾éƒ¨å®ä½“(æ ‡ç‚¹ç¬¦å·): '{tail}'ï¼Œè·³è¿‡è¯¥ä¸‰å…ƒç»„")
                         continue
                    
                    # ç¡®ä¿å®ä½“ç±»å‹å’Œå…³ç³»ä¸ä¸ºç©ºæˆ– None
                    head_type = head_type.strip() if head_type else "æ¦‚å¿µ"
                    tail_type = tail_type.strip() if tail_type else "æ¦‚å¿µ"
                    relation = relation.strip() if relation else None
                    
                    # å¦‚æœå…³ç³»ä¸ºç©ºï¼Œè·³è¿‡è¯¥ä¸‰å…ƒç»„
                    if not relation:
                        logger.warning(f"å…³ç³»ç±»å‹ä¸ºç©ºï¼Œè·³è¿‡ä¸‰å…ƒç»„: {head} - {relation} - {tail}")
                        continue

                    enhanced_triplets.append({
                        "head": head,
                        "head_type": head_type,
                        "relation": relation,
                        "tail": tail,
                        "tail_type": tail_type
                    })
                    
                    logger.debug(f"æå–LLMè¯­ä¹‰ä¸‰å…ƒç»„: {head}({head_type}) - {relation} - {tail}({tail_type})")
        
        # åº”ç”¨æœ¯è¯­æ˜ å°„æ ‡å‡†åŒ– å…ˆæ³¨é‡Š
        # enhanced_triplets = StandardTermMapper.process_triplets(enhanced_triplets)
        
        if not enhanced_triplets:
            logger.warning("æœªèƒ½ä»LLMè¾“å‡ºä¸­æå–åˆ°ä»»ä½•æœ‰æ•ˆçš„ä¸‰å…ƒç»„")
             
        return enhanced_triplets
    
    @classmethod
    def validate_llm_entity_types(cls, enhanced_triplets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """éªŒè¯LLMè¿”å›çš„å®ä½“ç±»å‹ - å®Œå…¨ä¿¡ä»»LLMï¼Œä¸å†è¿›è¡Œä»»ä½•é™åˆ¶"""
        # å®Œå…¨ä¿¡ä»»LLMçš„è¯­ä¹‰åˆ†æï¼Œä¸å†éªŒè¯ç±»å‹æ˜¯å¦åœ¨é¢„å®šä¹‰åˆ—è¡¨ä¸­
        # åªè¿›è¡ŒåŸºæœ¬çš„æ ¼å¼æ¸…ç†
        validated_triplets = []
        for triplet in enhanced_triplets:
            # åªè¿›è¡ŒåŸºæœ¬çš„éç©ºæ£€æŸ¥ï¼Œå®Œå…¨ä¿¡ä»»LLMçš„è¯­ä¹‰åˆ¤æ–­
            head_type = triplet.get("head_type", "æ¦‚å¿µ")
            tail_type = triplet.get("tail_type", "æ¦‚å¿µ")
            
            # åªæ¸…ç†ç©ºç™½å­—ç¬¦ï¼Œä¸å†è¿›è¡Œä»»ä½•ç±»å‹é™åˆ¶
            triplet["head_type"] = head_type.strip() if head_type else "æ¦‚å¿µ"
            triplet["tail_type"] = tail_type.strip() if tail_type else "æ¦‚å¿µ"
            
            validated_triplets.append(triplet)
        
        return validated_triplets


def validate_and_map_entity_type(entity_type: str) -> Optional[str]:
    """
    éªŒè¯å’Œæ˜ å°„å®ä½“ç±»å‹åˆ°æ ‡å‡†æœ¬ä½“
    
    Args:
        entity_type: LLM è¿”å›çš„å®ä½“ç±»å‹
    
    Returns:
        æ ‡å‡†åŒ–åçš„å®ä½“ç±»å‹ï¼Œå¦‚æœä¸åœ¨å…è®¸åˆ—è¡¨ä¸­è¿”å› None æˆ– "Concept"
    
    è§„åˆ™ï¼š
    - å¦‚æœåœ¨ ENTITY_TYPE_SCHEMA ä¸­ï¼Œè¿”å›æ˜ å°„åçš„æ ‡å‡†ç±»å‹
    - å¦‚æœä¸åœ¨ï¼Œæ ¹æ®é…ç½®å†³å®šï¼šæ˜ å°„ä¸º "Concept" æˆ–è¿”å› Noneï¼ˆä¸¢å¼ƒï¼‰
    """
    if not entity_type:
        return None
    
    # æ¸…ç†ç±»å‹å­—ç¬¦ä¸²
    entity_type = entity_type.strip()
    
    # ç›´æ¥åŒ¹é…
    if entity_type in ENTITY_TYPE_SCHEMA:
        return ENTITY_TYPE_SCHEMA[entity_type]

    # æ¨¡ç³ŠåŒ¹é…ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«æ ‡å‡†ç±»å‹å…³é”®è¯
    standard_types = [
        "ç–¾ç—…", "éƒ¨ä½", "æ²»ç–—", "ç—‡çŠ¶","ä½“å¾", "æ£€æŸ¥",  # åŸæœ‰ä¸´åºŠç±»
        "æµè¡Œç—…å­¦æ–¹æ³•", "ç»Ÿè®¡æŒ‡æ ‡", "ç ”ç©¶ç±»å‹",    # æ–°å¢ç§‘ç ”ç±»
        "å«ç”Ÿç»æµå­¦", "å…¬å…±å«ç”Ÿç­–ç•¥", "è¡ŒåŠ¨è®¡åˆ’",  # æ–°å¢å…¬å«ç±»
        "é£é™©å› ç´ ", "è®¾å¤‡","å¼‚å¸¸","éƒ¨ä½","ç”Ÿç†","è¯Šç–—","å¹²é¢„","é£é™©","å› ç´ "                       # æ–°å¢å…¶ä»–ç±»
    ]
    for std_type in standard_types:
        if std_type in entity_type:
            return std_type
    
    # ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­ï¼Œæ ¹æ®é…ç½®å†³å®š
    if EXTRACTOR_CONFIG.get("strict_entity_type_schema", True):
        action = EXTRACTOR_CONFIG.get("invalid_entity_type_action", "map_to_concept")
        if action == "discard":
            return None  # è¿”å› None è¡¨ç¤ºåº”è¯¥ä¸¢å¼ƒ
        else:  # map_to_concept
            # return "Concept"  # æ˜ å°„ä¸º Concept
            return None
    
    # å¦‚æœæ²¡æœ‰å¯ç”¨ä¸¥æ ¼æ¨¡å¼ï¼Œè¿”å›åŸå€¼ï¼ˆå‘åå…¼å®¹ï¼‰
    return entity_type


# ä¿®æ”¹ parse_llm_output_to_enhanced_triplets å‡½æ•°ä»¥è¿”å› EntityNode, Relation å¯¹è±¡
def parse_llm_output_to_enhanced_triplets(llm_output: str) -> List[Tuple[EntityNode, Relation, EntityNode]]:
    """å¢å¼ºçš„è§£æå‡½æ•°ï¼Œå®Œå…¨ä¿¡ä»»LLMçš„è¯­ä¹‰åˆ†æç»“æœï¼Œå¹¶æ¸…ç†ç‰¹æ®Šå­—ç¬¦"""
    from llama.neo4j_text_sanitizer import Neo4jTextSanitizer
    
    enhanced_triplets_dicts = EnhancedEntityExtractor.extract_enhanced_triplets(llm_output)
    
    # éªŒè¯LLMè¿”å›çš„å®ä½“ç±»å‹ - å®Œå…¨ä¿¡ä»»æ¨¡å¼
    validated_triplets = EnhancedEntityExtractor.validate_llm_entity_types(enhanced_triplets_dicts)
    
    result_triplets = []
    for triplet_dict in validated_triplets:
        head_name = triplet_dict.get("head", "")
        head_type = triplet_dict.get("head_type", "æ¦‚å¿µ")
        relation_type = triplet_dict.get("relation", "å…³è”")
        tail_name = triplet_dict.get("tail", "")
        tail_type = triplet_dict.get("tail_type", "æ¦‚å¿µ")
        
        if head_name and relation_type and tail_name:
            # ä½¿ç”¨ common æ¨¡å—ä¸­çš„ clean_text è¿›è¡ŒåŸºæœ¬æ¸…ç†
            head_name = clean_text(head_name, remove_special=False)
            tail_name = clean_text(tail_name, remove_special=False)
            relation_type = clean_text(relation_type, remove_special=False)

            # ---------------------------------------------------------
            # å¼ºåˆ¶æ˜ å°„å±‚ (Standardization Error Fix) - ç”¨æˆ·è¯·æ±‚çš„å¼ºæ ¡éªŒé’©å­
            # å·²æ³¨é‡Šï¼šç§»é™¤ StandardTermMapper (æ ‡å‡†è¯æ˜ å°„)
            # ---------------------------------------------------------
            # try:
            #     # å†æ¬¡å°è¯•æ ‡å‡†åŒ–ï¼Œç¡®ä¿åœ¨åˆ›å»ºèŠ‚ç‚¹å‰å¼ºåˆ¶åº”ç”¨æ ‡å‡†æœ¯è¯­
            #     std_head = StandardTermMapper.standardize(head_name)
            #     if std_head in StandardTermMapper.STANDARD_ENTITIES:
            #         if head_name != std_head:
            #             logger.info(f"ğŸ”§ å¼ºåˆ¶çº å (Head): {head_name} -> {std_head}")
            #         head_name = std_head
            #     
            #     std_tail = StandardTermMapper.standardize(tail_name)
            #     if std_tail in StandardTermMapper.STANDARD_ENTITIES:
            #         if tail_name != std_tail:
            #             logger.info(f"ğŸ”§ å¼ºåˆ¶çº å (Tail): {tail_name} -> {std_tail}")
            #         tail_name = std_tail
            # except Exception as e:
            #     logger.warning(f"StandardTermMapper å¼ºæ ¡éªŒå¤±è´¥: {e}")
            # ---------------------------------------------------------
            
            # éªŒè¯ï¼šè·³è¿‡çº¯æ ‡ç‚¹æˆ–ç©ºçš„å®ä½“/å…³ç³»
            invalid_symbols = {",", ".", "ã€‚", "ï¼Œ", "ã€", " ", "\\", "/", ";", ":", "?", "!", "'", "\"", "(", ")", "[", "]", "{", "}", "-", "_", "+", "=", "*", "&", "^", "%", "$", "#", "@", "~", "`", "<", ">", "|"}
            
            def is_invalid(text):
                if not text: return True
                if text in invalid_symbols: return True
                return all(char in invalid_symbols for char in text)

            if is_invalid(head_name) or is_invalid(tail_name) or is_invalid(relation_type):
                logger.warning(f"è·³è¿‡æ— æ•ˆå®ä½“/å…³ç³»: '{head_name}' - '{relation_type}' - '{tail_name}'")
                continue
            
            # ä½¿ç”¨ common æ¨¡å—ä¸­çš„ sanitize_for_neo4j è¿›è¡Œ Neo4j å®‰å…¨æ¸…ç†
            # è®°å½•æ¸…ç†å‰çš„å€¼(ç”¨äºæ—¥å¿—å¯¹æ¯”)
            original_head = head_name
            original_tail = tail_name
            original_relation = relation_type
            original_head_type = head_type
            original_tail_type = tail_type
            
            # æ¸…ç†èŠ‚ç‚¹åç§°
            head_name = Neo4jTextSanitizer.sanitize_node_name(head_name)
            tail_name = Neo4jTextSanitizer.sanitize_node_name(tail_name)
            
            # æ¸…ç†å…³ç³»æ ‡ç­¾ï¼ˆåŒ…å«å…³ç³»è§„èŒƒåŒ–ï¼‰
            original_relation_length = len(relation_type)
            relation_type = Neo4jTextSanitizer.sanitize_relation_label(relation_type, max_length=10)
            
            # ===== å®ä½“ç±»å‹æœ¬ä½“çº¦æŸéªŒè¯ =====
            # æ¸…ç†å®ä½“ç±»å‹(Label) - ç¡®ä¿æœ‰é»˜è®¤å€¼
            head_type = head_type or "æ¦‚å¿µ"
            tail_type = tail_type or "æ¦‚å¿µ"
            head_type = Neo4jTextSanitizer.sanitize_entity_type(head_type)
            tail_type = Neo4jTextSanitizer.sanitize_entity_type(tail_type)
            
            # åº”ç”¨æœ¬ä½“çº¦æŸï¼šéªŒè¯å’Œæ˜ å°„å®ä½“ç±»å‹
            original_head_type = head_type
            original_tail_type = tail_type
            head_type = validate_and_map_entity_type(head_type)
            tail_type = validate_and_map_entity_type(tail_type)
            
            # å¦‚æœç±»å‹è¢«ä¸¢å¼ƒï¼ˆè¿”å› Noneï¼‰ï¼Œè·³è¿‡è¯¥ä¸‰å…ƒç»„
            if head_type is None or tail_type is None:
                discarded_types = []
                if head_type is None:
                    discarded_types.append(f"head: {original_head_type}")
                if tail_type is None:
                    discarded_types.append(f"tail: {original_tail_type}")
                logger.warning(
                    f"å®ä½“ç±»å‹ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­ï¼Œè·³è¿‡ä¸‰å…ƒç»„: "
                    f"{head_name}({original_head_type}) - {relation_type} - {tail_name}({original_tail_type}) | "
                    f"ä¸¢å¼ƒåŸå› : {', '.join(discarded_types)}"
                )
                continue
            
            # å¦‚æœç±»å‹è¢«æ˜ å°„ä¸º Conceptï¼Œè®°å½•æ—¥å¿—
            if original_head_type != head_type and head_type == "Concept":
                logger.info(f"å®ä½“ç±»å‹æ˜ å°„: {head_name} '{original_head_type}' -> 'Concept'")
            if original_tail_type != tail_type and tail_type == "Concept":
                logger.info(f"å®ä½“ç±»å‹æ˜ å°„: {tail_name} '{original_tail_type}' -> 'Concept'")
            
            # ç¡®ä¿æ¸…ç†åä¸ä¸º None æˆ–ç©ºå­—ç¬¦ä¸²ï¼ˆå¤‡ç”¨æ£€æŸ¥ï¼‰
            if not head_type or head_type == "None":
                head_type = "Concept"
                logger.warning(f"å®ä½“ç±»å‹ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼ 'Concept': {head_name}")
            if not tail_type or tail_type == "None":
                tail_type = "Concept"
                logger.warning(f"å®ä½“ç±»å‹ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼ 'Concept': {tail_name}")
            # ===== å®ä½“ç±»å‹éªŒè¯ç»“æŸ =====
            
            # éªŒè¯å…³ç³»ç±»å‹ä¸ä¸ºç©º
            if not relation_type or relation_type == "None":
                logger.warning(f"å…³ç³»ç±»å‹ä¸ºç©ºï¼Œè·³è¿‡ä¸‰å…ƒç»„: {head_name} - {relation_type} - {tail_name}")
                continue
            
            # å¦‚æœæ¸…ç†åå‘ç”Ÿäº†å˜åŒ–ï¼Œè®°å½•æ—¥å¿—
            relation_changed = original_relation != relation_type
            relation_simplified = original_relation_length > 10 and len(relation_type) <= 10
            
            if (original_head != head_name or original_tail != tail_name or 
                relation_changed or original_head_type != head_type or 
                original_tail_type != tail_type):
                if relation_simplified:
                    logger.info(
                        f"ğŸ”§ å…³ç³»ç®€åŒ–: "
                        f"[{original_relation}] ({original_relation_length}å­—) -> [{relation_type}] ({len(relation_type)}å­—) | "
                        f"ä¸‰å…ƒç»„: {head_name} - {tail_name}"
                    )
                else:
                    logger.debug(
                        f"ğŸ§¹ å­—ç¬¦æ¸…ç†: "
                        f"[{original_head}({original_head_type})] -> [{head_name}({head_type})], "
                        f"[{original_relation}] -> [{relation_type}], "
                        f"[{original_tail}({original_tail_type})] -> [{tail_name}({tail_type})]"
                    )
            
            # å†æ¬¡éªŒè¯æ¸…ç†åä¸ä¸ºç©º
            if not head_name or not tail_name or not relation_type:
                logger.error(f"æ¸…ç†åå®ä½“/å…³ç³»ä¸ºç©ºï¼Œè·³è¿‡: {head_name} - {relation_type} - {tail_name}")
                continue
            # ===== æ¸…ç†ç»“æŸ =====

            logger.info(f"åˆ›å»ºè¯­ä¹‰ä¸‰å…ƒç»„: {head_name}({head_type}) - {relation_type} - {tail_name}({tail_type})")
                
            head_node = EntityNode(name=head_name, label=head_type)
            tail_node = EntityNode(name=tail_name, label=tail_type)
            
            relation = Relation(
                source_id=head_node.id,
                target_id=tail_node.id,
                label=relation_type
            )
            result_triplets.append((head_node, relation, tail_node))
        else:
            logger.warning(f"è·³è¿‡æ— æ•ˆä¸‰å…ƒç»„: {triplet_dict}")
            
    return result_triplets


def parse_dynamic_triplets(llm_output: str) -> List[Tuple[EntityNode, Relation, EntityNode]]:
    return parse_llm_output_to_enhanced_triplets(llm_output)
