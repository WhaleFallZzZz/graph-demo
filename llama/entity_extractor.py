"""
å¢å¼ºçš„å®ä½“ç±»å‹æå–å™¨ - å®Œå…¨ä¾èµ–LLMè¯­ä¹‰åˆ†æï¼Œæ— ä»»ä½•é™åˆ¶
"""

import logging
import os
from typing import List, Tuple, Dict, Any, Optional
import json
import re
import queue
import threading
import resource
import time
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed

# Import EntityNode and Relation from llama_index.core
from llama_index.core.graph_stores.types import EntityNode, Relation
from llama_index.core.schema import BaseNode
from llama_index.core.indices.property_graph import DynamicLLMPathExtractor

try:
    from enhanced_entity_extractor import StandardTermMapper
except ImportError:
    # Fallback if file not found or circular import
    logger.warning("StandardTermMapper not found in enhanced_entity_extractor.py")
    class StandardTermMapper:
        @classmethod
        def process_triplets(cls, triplets):
            return triplets

logger = logging.getLogger(__name__)

# Try to import safe_json_parse from utils (handling different import paths)
try:
    from llama.utils import safe_json_parse
except ImportError:
    try:
        from utils import safe_json_parse
    except ImportError:
        def safe_json_parse(json_str: str) -> List[Dict[str, Any]]:
            try:
                # Basic JSON extraction and parsing
                start = json_str.find('[')
                end = json_str.rfind(']')
                if start != -1 and end != -1:
                    return json.loads(json_str[start:end+1])
                return json.loads(json_str)
            except:
                return []

def parse_llm_output_with_types(llm_output: str) -> List[Dict[str, Any]]:
    """Parse LLM output using JSON parsing first, falling back to regex"""
    # 1. Try JSON parsing
    parsed = safe_json_parse(llm_output)
    if parsed and isinstance(parsed, list):
        return parsed
    
    # 2. Fallback to regex (improved to handle different orders)
    import re
    results = []
    # Pattern to match individual objects
    object_pattern = r'\{[^{}]+\}'
    objects = re.findall(object_pattern, llm_output)
    
    for obj_str in objects:
        try:
            # Try to parse each object as JSON
            obj = json.loads(obj_str)
            if isinstance(obj, dict):
                results.append(obj)
                continue
        except:
            pass
            
        # Regex extraction for fields if object JSON parsing fails
        head = re.search(r'"head"\s*:\s*"(.*?)"', obj_str)
        head_type = re.search(r'"head_type"\s*:\s*"(.*?)"', obj_str)
        relation = re.search(r'"relation"\s*:\s*"(.*?)"', obj_str)
        tail = re.search(r'"tail"\s*:\s*"(.*?)"', obj_str)
        tail_type = re.search(r'"tail_type"\s*:\s*"(.*?)"', obj_str)
        
        if head and relation and tail:
            results.append({
                "head": head.group(1),
                "head_type": head_type.group(1) if head_type else "æ¦‚å¿µ",
                "relation": relation.group(1),
                "tail": tail.group(1),
                "tail_type": tail_type.group(1) if tail_type else "æ¦‚å¿µ"
            })
            
    return results

class EnhancedEntityExtractor:
    """å¢å¼ºçš„å®ä½“æå–å™¨ - å®Œå…¨ä¿¡ä»»LLMè¯­ä¹‰åˆ†æ"""
    
    @classmethod
    def extract_enhanced_triplets(cls, llm_output: str) -> List[Dict[str, Any]]:
        """æå–å¢å¼ºçš„ä¸‰å…ƒç»„ï¼Œå®Œå…¨ä¿¡ä»»LLMçš„è¯­ä¹‰åˆ†æç»“æœ"""
        enhanced_triplets = []
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—ä»¥æŸ¥çœ‹LLMåŸå§‹è¾“å‡º
        logger.info(f"LLMåŸå§‹è¾“å‡º (é•¿åº¦: {len(llm_output)}): {llm_output[:500]}...")
        
        # ä½¿ç”¨ enhanced_utils ä¸­çš„ parse_llm_output_with_types 
        # è¿™ä¸ªå‡½æ•°å·²ç»é›†æˆäº† safe_json_parse å’Œå¸¦ç±»å‹çš„æ­£åˆ™å›é€€
        parsed_dicts = parse_llm_output_with_types(llm_output)
        
        if parsed_dicts:
            for item in parsed_dicts:
                head = item.get("head", "").strip()
                head_type = item.get("head_type", "").strip()
                relation = item.get("relation", "").strip()
                tail = item.get("tail", "").strip()
                tail_type = item.get("tail_type", "").strip()
                
                # åªæœ‰å½“head, relation, tailéƒ½å­˜åœ¨ä¸”ä¸å…¨æ˜¯æ ‡ç‚¹ç¬¦å·æ—¶æ‰æ·»åŠ 
                if head and relation and tail:
                    # é¿å…å°¾éƒ¨æ˜¯é€—å·ç­‰æ ‡ç‚¹ç¬¦å·çš„æ— æ•ˆæå–
                    if tail in {",", ".", "ã€‚", "ï¼Œ", "ã€"}:
                         logger.warning(f"æ£€æµ‹åˆ°æ— æ•ˆçš„å°¾éƒ¨å®ä½“(æ ‡ç‚¹ç¬¦å·): '{tail}'ï¼Œè·³è¿‡è¯¥ä¸‰å…ƒç»„")
                         continue

                    enhanced_triplets.append({
                        "head": head,
                        "head_type": head_type or "æ¦‚å¿µ",
                        "relation": relation,
                        "tail": tail,
                        "tail_type": tail_type or "æ¦‚å¿µ"
                    })
                    
                    logger.debug(f"æå–LLMè¯­ä¹‰ä¸‰å…ƒç»„: {head}({head_type}) - {relation} - {tail}({tail_type})")
        
        # åº”ç”¨æœ¯è¯­æ˜ å°„æ ‡å‡†åŒ–
        enhanced_triplets = StandardTermMapper.process_triplets(enhanced_triplets)
        
        if not enhanced_triplets:
            logger.warning("æœªèƒ½ä»LLMè¾“å‡ºä¸­æå–åˆ°ä»»ä½•æœ‰æ•ˆçš„ä¸‰å…ƒç»„")
             
        return enhanced_triplets
    
    @classmethod
    def validate_llm_entity_types(cls, enhanced_triplets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """éªŒè¯LLMè¿”å›çš„å®ä½“ç±»å‹ - å®Œå…¨ä¿¡ä»»LLMï¼Œä¸å†è¿›è¡Œä»»ä½•é™åˆ¶"""
        # å®Œå…¨ä¿¡ä»»LLMçš„è¯­ä¹‰åˆ†æï¼Œä¸å†éªŒè¯ç±»å‹æ˜¯å¦åœ¨é¢„å®šä¹‰åˆ—è¡¨ä¸­
        # åªè¿›è¡ŒåŸºæœ¬çš„æ ¼å¼æ¸…ç†
        validated_triplets = []
        for triplet in enhanced_triplets:
            # åªè¿›è¡ŒåŸºæœ¬çš„éç©ºæ£€æŸ¥ï¼Œå®Œå…¨ä¿¡ä»»LLMçš„è¯­ä¹‰åˆ¤æ–­
            head_type = triplet.get("head_type", "æ¦‚å¿µ")
            tail_type = triplet.get("tail_type", "æ¦‚å¿µ")
            
            # åªæ¸…ç†ç©ºç™½å­—ç¬¦ï¼Œä¸å†è¿›è¡Œä»»ä½•ç±»å‹é™åˆ¶
            triplet["head_type"] = head_type.strip() if head_type else "æ¦‚å¿µ"
            triplet["tail_type"] = tail_type.strip() if tail_type else "æ¦‚å¿µ"
            
            validated_triplets.append(triplet)
        
        return validated_triplets

# ä¿®æ”¹ parse_llm_output_to_enhanced_triplets å‡½æ•°ä»¥è¿”å› EntityNode, Relation å¯¹è±¡
def parse_llm_output_to_enhanced_triplets(llm_output: str) -> List[Tuple[EntityNode, Relation, EntityNode]]:
    """å¢å¼ºçš„è§£æå‡½æ•°ï¼Œå®Œå…¨ä¿¡ä»»LLMçš„è¯­ä¹‰åˆ†æç»“æœï¼Œå¹¶æ¸…ç†ç‰¹æ®Šå­—ç¬¦"""
    from neo4j_text_sanitizer import Neo4jTextSanitizer
    
    enhanced_triplets_dicts = EnhancedEntityExtractor.extract_enhanced_triplets(llm_output)
    
    # éªŒè¯LLMè¿”å›çš„å®ä½“ç±»å‹ - å®Œå…¨ä¿¡ä»»æ¨¡å¼
    validated_triplets = EnhancedEntityExtractor.validate_llm_entity_types(enhanced_triplets_dicts)
    
    result_triplets = []
    for triplet_dict in validated_triplets:
        head_name = triplet_dict.get("head", "")
        head_type = triplet_dict.get("head_type", "æ¦‚å¿µ")
        relation_type = triplet_dict.get("relation", "å…³è”")
        tail_name = triplet_dict.get("tail", "")
        tail_type = triplet_dict.get("tail_type", "æ¦‚å¿µ")
        
        if head_name and relation_type and tail_name:
            # æ¸…ç†åç§°(åŸºæœ¬æ¸…ç†)
            head_name = str(head_name).strip()
            tail_name = str(tail_name).strip()
            relation_type = str(relation_type).strip()
            
            # éªŒè¯ï¼šè·³è¿‡çº¯æ ‡ç‚¹æˆ–ç©ºçš„å®ä½“/å…³ç³»
            invalid_symbols = {",", ".", "ã€‚", "ï¼Œ", "ã€", " ", "\\", "/", ";", ":", "?", "!", "'", "\"", "(", ")", "[", "]", "{", "}", "-", "_", "+", "=", "*", "&", "^", "%", "$", "#", "@", "~", "`", "<", ">", "|"}
            
            def is_invalid(text):
                if not text: return True
                if text in invalid_symbols: return True
                return all(char in invalid_symbols for char in text)

            if is_invalid(head_name) or is_invalid(tail_name) or is_invalid(relation_type):
                logger.warning(f"è·³è¿‡æ— æ•ˆå®ä½“/å…³ç³»: '{head_name}' - '{relation_type}' - '{tail_name}'")
                continue
            
            # ===== æ–°å¢ï¼šNeo4jç‰¹æ®Šå­—ç¬¦æ¸…ç† =====
            # è®°å½•æ¸…ç†å‰çš„å€¼(ç”¨äºæ—¥å¿—å¯¹æ¯”)
            original_head = head_name
            original_tail = tail_name
            original_relation = relation_type
            original_head_type = head_type
            original_tail_type = tail_type
            
            # æ¸…ç†èŠ‚ç‚¹åç§°
            head_name = Neo4jTextSanitizer.sanitize_node_name(head_name)
            tail_name = Neo4jTextSanitizer.sanitize_node_name(tail_name)
            
            # æ¸…ç†å…³ç³»æ ‡ç­¾
            relation_type = Neo4jTextSanitizer.sanitize_relation_label(relation_type)
            
            # æ¸…ç†å®ä½“ç±»å‹(Label)
            head_type = Neo4jTextSanitizer.sanitize_entity_type(head_type)
            tail_type = Neo4jTextSanitizer.sanitize_entity_type(tail_type)
            
            # å¦‚æœæ¸…ç†åå‘ç”Ÿäº†å˜åŒ–ï¼Œè®°å½•æ—¥å¿—
            if (original_head != head_name or original_tail != tail_name or 
                original_relation != relation_type or original_head_type != head_type or 
                original_tail_type != tail_type):
                logger.info(
                    f"ğŸ§¹ å­—ç¬¦æ¸…ç†: "
                    f"[{original_head}({original_head_type})] -> [{head_name}({head_type})], "
                    f"[{original_relation}] -> [{relation_type}], "
                    f"[{original_tail}({original_tail_type})] -> [{tail_name}({tail_type})]"
                )
            
            # å†æ¬¡éªŒè¯æ¸…ç†åä¸ä¸ºç©º
            if not head_name or not tail_name or not relation_type:
                logger.error(f"æ¸…ç†åå®ä½“/å…³ç³»ä¸ºç©ºï¼Œè·³è¿‡: {head_name} - {relation_type} - {tail_name}")
                continue
            # ===== æ¸…ç†ç»“æŸ =====

            logger.info(f"åˆ›å»ºè¯­ä¹‰ä¸‰å…ƒç»„: {head_name}({head_type}) - {relation_type} - {tail_name}({tail_type})")
                
            head_node = EntityNode(name=head_name, label=head_type)
            tail_node = EntityNode(name=tail_name, label=tail_type)
            
            relation = Relation(
                source_id=head_node.id,
                target_id=tail_node.id,
                label=relation_type
            )
            result_triplets.append((head_node, relation, tail_node))
        else:
            logger.warning(f"è·³è¿‡æ— æ•ˆä¸‰å…ƒç»„: {triplet_dict}")
            
    return result_triplets

# ä¿æŒåŸæœ‰çš„å‡½æ•°åå…¼å®¹æ€§
parse_dynamic_triplets = parse_llm_output_to_enhanced_triplets

class MultiStageLLMExtractor(DynamicLLMPathExtractor):
    """
    Multi-stage LLM Extractor:
    1. Parallel Entity Recognition
    2. Producer-Consumer Relation Extraction
    """
    def __init__(
        self,
        llm: Any,
        entity_prompt: str,
        relation_prompt: str,
        num_workers: int = 4,
        max_triplets_per_chunk: int = 15,
        graph_store: Optional[Any] = None,
        lightweight_llm: Optional[Any] = None,
        **kwargs: Any,
    ) -> None:
        super().__init__(
            llm=llm,
            extract_prompt=entity_prompt, # Placeholder
            parse_fn=None, # We implement custom logic
            num_workers=num_workers,
            max_triplets_per_chunk=max_triplets_per_chunk,
            **kwargs,
        )
        # Bypass Pydantic validation for custom fields
        object.__setattr__(self, "entity_prompt", entity_prompt)
        object.__setattr__(self, "relation_prompt", relation_prompt)
        object.__setattr__(self, "real_num_workers", num_workers)
        object.__setattr__(self, "graph_store", graph_store)
        object.__setattr__(self, "lightweight_llm", lightweight_llm or llm)
        
        # Memory monitoring config
        object.__setattr__(self, "memory_threshold_mb", 100)
        object.__setattr__(self, "peak_memory_usage", 0)
        
        # File write lock for saving JSON output
        object.__setattr__(self, "_file_lock", threading.Lock())

    def _safe_llm_call(self, prompt: str, max_retries: int = 3, llm_instance: Any = None) -> str:
        """Call LLM with enhanced retry mechanism and caching"""
        import time
        from llm_cache_manager import get_global_cache
        
        target_llm = llm_instance or self.llm
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cache = get_global_cache()
        cached_result = cache.get(prompt, model_params={
            "temperature": 0.0,
            "model": getattr(target_llm, "model", "unknown")
        })
        
        if cached_result:
            logger.debug("ä½¿ç”¨ç¼“å­˜çš„LLMå“åº”")
            return cached_result
        
        last_error = None
        for attempt in range(max_retries):
            try:
                response = target_llm.complete(prompt)
                result = response.text
                
                # ç¼“å­˜æˆåŠŸçš„ç»“æœ
                cache.put(prompt, result, model_params={
                    "temperature": 0.0,
                    "model": getattr(target_llm, "model", "unknown")
                })
                
                return result
                
            except Exception as e:
                last_error = e
                error_type = type(e).__name__
                
                # æ ¹æ®é”™è¯¯ç±»å‹é‡‡ç”¨ä¸åŒçš„é‡è¯•ç­–ç•¥
                if "RateLimitError" in error_type or "429" in str(e):
                    # é€Ÿç‡é™åˆ¶é”™è¯¯,ä½¿ç”¨æŒ‡æ•°é€€é¿
                    wait_time = min(60, (2 ** attempt) * 5)
                    logger.warning(f"é€Ÿç‡é™åˆ¶é”™è¯¯,ç­‰å¾… {wait_time}ç§’åé‡è¯• (attempt {attempt+1}/{max_retries})")
                    time.sleep(wait_time)
                elif "Timeout" in error_type or "timeout" in str(e).lower():
                    # è¶…æ—¶é”™è¯¯,çŸ­æš‚ç­‰å¾…
                    wait_time = 5 * (attempt + 1)
                    logger.warning(f"è¶…æ—¶é”™è¯¯,ç­‰å¾… {wait_time}ç§’åé‡è¯• (attempt {attempt+1}/{max_retries})")
                    time.sleep(wait_time)
                elif "ConnectionError" in error_type or "NetworkError" in error_type:
                    # ç½‘ç»œé”™è¯¯,ç­‰å¾…è¾ƒé•¿æ—¶é—´
                    wait_time = 10 * (attempt + 1)
                    logger.warning(f"ç½‘ç»œé”™è¯¯,ç­‰å¾… {wait_time}ç§’åé‡è¯• (attempt {attempt+1}/{max_retries})")
                    time.sleep(wait_time)
                else:
                    # å…¶ä»–é”™è¯¯,æ ‡å‡†é€€é¿
                    wait_time = 2 * (attempt + 1)
                    logger.warning(f"LLMè°ƒç”¨å¤±è´¥: {error_type}, ç­‰å¾… {wait_time}ç§’åé‡è¯• (attempt {attempt+1}/{max_retries})")
                    time.sleep(wait_time)
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        logger.error(f"LLMè°ƒç”¨å¤±è´¥,å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {last_error}")
        raise last_error

    def _save_json_output(self, node: BaseNode, triplets: List[Tuple]) -> None:
        """
        Securely save LLM output to a JSON file with metadata.
        Format: "original_filename-json.txt" in "llm_outputs/{date}/"
        """
        import datetime
        import os
        
        try:
            # 1. Prepare data
            file_name = node.metadata.get('file_name', 'unknown_file')
            safe_filename = os.path.basename(file_name)
            
            # Remove extension for cleaner naming if possible
            if '.' in safe_filename:
                base_name = safe_filename.rsplit('.', 1)[0]
            else:
                base_name = safe_filename
                
            json_data = {
                "node_id": node.node_id,
                "file_name": file_name,
                "timestamp": datetime.datetime.now().isoformat(),
                "triplets": [
                    {
                        "head": t[0].name,
                        "head_type": t[0].label,
                        "relation": t[1].label,
                        "tail": t[2].name,
                        "tail_type": t[2].label
                    }
                    for t in triplets
                ]
            }
            
            # 2. Prepare directory
            today_str = datetime.datetime.now().strftime("%Y-%m-%d")
            storage_dir = os.path.join(os.getcwd(), "llm_outputs", today_str)
            
            # Use lock for directory creation to avoid race conditions
            with self._file_lock:
                if not os.path.exists(storage_dir):
                    os.makedirs(storage_dir, exist_ok=True)
            
            # 3. Prepare filename
            output_filename = f"{base_name}-json.txt"
            output_path = os.path.join(storage_dir, output_filename)
            
            # 4. Format content
            current_time_str = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            header = f"/* å¤„ç†æ—¶é—´: {current_time_str} */\n"
            content = json.dumps(json_data, ensure_ascii=False, indent=2)
            
            # 5. Write to file (with retry)
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    with self._file_lock:
                        with open(output_path, "a", encoding="utf-8") as f:
                            f.write(header)
                            f.write(content)
                            f.write("\n\n") # Separator
                    logger.info(f"âœ… JSON output saved to: {output_path}")
                    break # Success
                except Exception as write_err:
                    if attempt < max_retries - 1:
                        time.sleep(0.1)
                    else:
                        raise write_err
            
            # Monitoring Log
            process_time = time.time() - start_time
            logger.info(f"Performance: Node {node.node_id[:8]} processed in {process_time:.4f}s. Extracted {len(triplets)} triplets.")
                        
        except Exception as e:
            logger.error(f"Failed to save JSON output for node {node.node_id}: {e}")

    def extract(self, nodes: List[BaseNode]) -> List[Dict[str, Any]]:
        results = [{} for _ in range(len(nodes))]
        # Limit queue size for memory buffer control (approx 100 chunks)
        relation_queue = queue.Queue(maxsize=100)
        
        # Memory monitoring helper
        def check_memory():
            try:
                # Get memory usage in MB
                rss = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
                if sys.platform == 'darwin':
                    usage_mb = rss / (1024 * 1024)
                else:
                    usage_mb = rss / 1024
                
                if usage_mb > self.peak_memory_usage:
                    object.__setattr__(self, "peak_memory_usage", usage_mb)
                    
                if usage_mb > self.memory_threshold_mb:
                    logger.warning(f"âš ï¸ Memory usage {usage_mb:.2f}MB exceeded threshold {self.memory_threshold_mb}MB")
            except Exception:
                pass

        # Stage 1: Entity Extraction (Producers)
        def entity_producer(node_idx, node):
            try:
                prompt = self.entity_prompt.format(text=node.text)
                # Use lightweight LLM for initial entity recognition
                output = self._safe_llm_call(prompt, llm_instance=self.lightweight_llm)
                entities = self._parse_entities(output)
                relation_queue.put((node_idx, node, entities))
                logger.debug(f"Stage 1 (Entity) done for node {node_idx}, found {len(entities)} entities")
            except Exception as e:
                logger.error(f"Stage 1 (Entity) failed for node {node_idx}: {e}")
                relation_queue.put((node_idx, node, []))

        # Stage 2: Relation Extraction (Consumers)
        def relation_consumer():
            while True:
                item = relation_queue.get()
                if item is None:
                    break
                node_idx, node, entities = item
                
                if not entities:
                    relation_queue.task_done()
                    continue
                    
                try:
                    entities_str = json.dumps(entities, ensure_ascii=False)
                    prompt = self.relation_prompt.format(text=node.text, entities=entities_str)
                    
                    output = self._safe_llm_call(prompt)
                    
                    # Use existing parsing logic
                    triplets = parse_llm_output_to_enhanced_triplets(output)
                    
                    # Save JSON output using the new robust method
                    self._save_json_output(node, triplets)

                    # If graph_store is available, write directly
                    if self.graph_store and triplets:
                        start_write = time.time()
                        try:
                            # Extract nodes and relations
                            head_nodes = [t[0] for t in triplets]
                            tail_nodes = [t[2] for t in triplets]
                            relations = [t[1] for t in triplets]
                            
                            # Deduplicate nodes by ID to reduce DB load
                            unique_nodes = {}
                            for n in head_nodes + tail_nodes:
                                unique_nodes[n.id] = n
                            
                            # Upsert to Neo4j
                            self.graph_store.upsert_nodes(list(unique_nodes.values()))
                            self.graph_store.upsert_relations(relations)
                            
                            write_time = time.time() - start_write
                            logger.info(f"âœ… Directly stored {len(triplets)} triplets to Neo4j in {write_time:.2f}s")
                            
                            # Do NOT store in results to save memory
                            # Store empty dict or metadata if needed
                            # Return empty kg_triplets to satisfy PropertyGraphIndex contract
                            results[node_idx] = {
                                "kg_triplets": [], 
                                "saved_to_neo4j": True, 
                                "count": len(triplets)
                            }
                            
                        except Exception as db_err:
                            logger.error(f"âŒ Failed to write to Neo4j: {db_err}. Falling back to memory.")
                            results[node_idx] = {"kg_triplets": triplets}
                    else:
                        # Fallback to memory storage
                        results[node_idx] = {"kg_triplets": triplets}
                    
                    logger.debug(f"Stage 2 (Relation) done for node {node_idx}, found {len(triplets)} triplets")
                    
                    # Check memory periodically
                    check_memory()
                    
                except Exception as e:
                    logger.error(f"Stage 2 (Relation) failed for node {node_idx}: {e}")
                finally:
                    relation_queue.task_done()

        # Start Consumers
        consumer_threads = []
        num_consumers = max(1, self.real_num_workers // 2)
        for _ in range(num_consumers):
            t = threading.Thread(target=relation_consumer)
            t.start()
            consumer_threads.append(t)
            
        # Start Producers
        try:
            from tqdm import tqdm
        except ImportError:
            def tqdm(iterable, **kwargs):
                return iterable

        with ThreadPoolExecutor(max_workers=self.real_num_workers) as executor:
            futures = [executor.submit(entity_producer, i, node) for i, node in enumerate(nodes)]
            for f in tqdm(as_completed(futures), total=len(nodes), desc="Entity Extraction", unit="node"):
                pass
        
        logger.info("Entity extraction (Stage 1) completed. Waiting for relation extraction (Stage 2)...")
        
        # Stop consumers
        for _ in range(num_consumers):
            relation_queue.put(None)
        
        for t in consumer_threads:
            t.join()
            
        return results

    def _parse_entities(self, output: str) -> List[Dict[str, str]]:
        try:
            return safe_json_parse(output)
        except:
            # Fallback regex
            import re
            matches = re.findall(r'\{\s*"name"\s*:\s*"(.*?)",\s*"type"\s*:\s*"(.*?)"\s*\}', output)
            return [{"name": m[0], "type": m[1]} for m in matches]
