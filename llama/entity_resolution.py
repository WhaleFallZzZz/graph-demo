
import logging
from typing import List, Dict, Any, Tuple, Set
import numpy as np
import networkx as nx
from llama_index.core.embeddings import BaseEmbedding
from llama_index.core.graph_stores import SimplePropertyGraphStore

logger = logging.getLogger(__name__)

class EntityResolver:
    """
    åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„å®ä½“å¯¹é½ä¸æ¶ˆæ­§ç»„ä»¶
    """
    def __init__(self, embed_model: BaseEmbedding):
        self.embed_model = embed_model
        
    def _compute_similarity_matrix(self, embeddings: List[List[float]]) -> np.ndarray:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ"""
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        emb_matrix = np.array(embeddings)
        # å½’ä¸€åŒ–
        norm = np.linalg.norm(emb_matrix, axis=1, keepdims=True)
        # é¿å…é™¤é›¶
        norm[norm == 0] = 1e-10
        normalized_emb = emb_matrix / norm
        # è®¡ç®—ç›¸ä¼¼åº¦ (N x N)
        return np.dot(normalized_emb, normalized_emb.T)
        
    async def find_duplicates(self, entities: List[str], threshold: float = 0.90) -> List[Tuple[str, str, float]]:
        """
        æŸ¥æ‰¾ç›¸ä¼¼å®ä½“å¯¹
        
        Args:
            entities: å®ä½“åç§°åˆ—è¡¨
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            List of (entity1, entity2, similarity)
        """
        if not entities:
            return []
            
        logger.info(f"æ­£åœ¨è®¡ç®— {len(entities)} ä¸ªå®ä½“çš„å‘é‡åµŒå…¥...")
        # æ‰¹é‡è·å–åµŒå…¥
        # æ³¨æ„ï¼šå¦‚æœå®ä½“æ•°é‡å¾ˆå¤§ï¼Œè¿™é‡Œåº”è¯¥åˆ†æ‰¹å¤„ç†
        embeddings = []
        batch_size = 32
        for i in range(0, len(entities), batch_size):
            batch = entities[i:i+batch_size]
            batch_embeddings = await self.embed_model.aget_text_embedding_batch(batch)
            embeddings.extend(batch_embeddings)
            
        logger.info("æ­£åœ¨è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ...")
        sim_matrix = self._compute_similarity_matrix(embeddings)
        
        duplicates = []
        n = len(entities)
        
        # éå†çŸ©é˜µä¸Šä¸‰è§’
        for i in range(n):
            for j in range(i + 1, n):
                score = sim_matrix[i, j]
                if score >= threshold:
                    # è®°å½•ç›¸ä¼¼å¯¹
                    duplicates.append((entities[i], entities[j], float(score)))
                    
        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        duplicates.sort(key=lambda x: x[2], reverse=True)
        return duplicates
        
    def resolve_entities(self, graph_store: SimplePropertyGraphStore, duplicates: List[Tuple[str, str, float]]) -> int:
        """
        åœ¨å›¾å­˜å‚¨ä¸­åˆå¹¶å®ä½“
        
        ç­–ç•¥ï¼šä¿ç•™è¾ƒçŸ­çš„å®ä½“åç§°ä½œä¸ºæ ‡å‡†åï¼ˆæˆ–è€…å‡ºç°é¢‘ç‡æ›´é«˜çš„ï¼Œè¿™é‡Œç®€åŒ–ä¸ºè¾ƒçŸ­çš„ï¼‰
        
        Args:
            graph_store: å›¾å­˜å‚¨å®ä¾‹
            duplicates: ç›¸ä¼¼å®ä½“å¯¹åˆ—è¡¨
            
        Returns:
            merged_count: åˆå¹¶æ¬¡æ•°
        """
        merged_count = 0
        
        # ä½¿ç”¨å¹¶æŸ¥é›†æˆ–ç®€å•æ˜ å°„æ¥å¤„ç†ä¼ é€’æ€§ (A~B, B~C -> A,B,C merge)
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šç›´æ¥å¤„ç†æ¯ä¸€å¯¹ï¼Œç»´æŠ¤ä¸€ä¸ªé‡æ˜ å°„å­—å…¸
        
        # æ˜ å°„: è¢«æ›¿æ¢å®ä½“ -> æ ‡å‡†å®ä½“
        merge_map = {}
        
        for e1, e2, score in duplicates:
            # å¦‚æœä¸¤ä¸ªéƒ½åœ¨æ˜ å°„ä¸­ï¼Œè¯´æ˜å·²ç»å¤„ç†è¿‡
            if e1 in merge_map and e2 in merge_map:
                continue
                
            # ç¡®å®šè°ç•™è°å»
            # è§„åˆ™1: å¦‚æœå·²ç»æœ‰ä¸€ä¸ªè¢«æ˜ å°„äº†ï¼Œå¦ä¸€ä¸ªè·Ÿéšæ˜ å°„
            if e1 in merge_map:
                target = merge_map[e1]
                if e2 != target:
                    merge_map[e2] = target
                    merged_count += 1
                continue
            if e2 in merge_map:
                target = merge_map[e2]
                if e1 != target:
                    merge_map[e1] = target
                    merged_count += 1
                continue
                
            # è§„åˆ™2: é•¿åº¦çŸ­çš„ä¼˜å…ˆä¿ç•™ (ä½œä¸ºæ›´é€šç”¨çš„æ¦‚å¿µ)
            # ä¾‹å¦‚ "è¿‘è§†" (2) vs "é’å°‘å¹´è¿‘è§†" (5) -> ä¿ç•™ "è¿‘è§†"
            if len(e1) < len(e2):
                keep, remove = e1, e2
            elif len(e2) < len(e1):
                keep, remove = e2, e1
            else:
                # é•¿åº¦ç›¸åŒï¼ŒæŒ‰å­—å…¸åº
                keep, remove = (e1, e2) if e1 < e2 else (e2, e1)
                
            merge_map[remove] = keep
            merged_count += 1
            
        if not merge_map:
            return 0
            
        logger.info(f"è®¡åˆ’åˆå¹¶ {len(merge_map)} ä¸ªå®ä½“")
        
        # æ‰§è¡Œå›¾æ›´æ–°
        # SimplePropertyGraphStore æ˜¯å†…å­˜å­˜å‚¨ï¼Œç›´æ¥æ“ä½œå…¶å†…éƒ¨ç»“æ„å¯èƒ½æ¯”è¾ƒå¤æ‚
        # è¿™é‡Œçš„ graph_store åº”è¯¥æ˜¯ llama_index.core.graph_stores.SimplePropertyGraphStore
        
        # ç”±äº SimplePropertyGraphStore API é™åˆ¶ï¼Œæˆ‘ä»¬é€šå¸¸åªèƒ½é€šè¿‡ get_triplets å’Œ add ç­‰æ“ä½œ
        # ä½†ç›´æ¥ä¿®æ”¹ graph_store.graph å¯èƒ½æ›´é«˜æ•ˆ (å¦‚æœå®ƒæ˜¯ NetworkX æˆ– simple dict)
        # SimplePropertyGraphStore å†…éƒ¨ä½¿ç”¨ self._data = PropertyGraph()
        
        # è·å–æ‰€æœ‰ä¸‰å…ƒç»„
        triplets = graph_store.get_triplets()
        new_triplets = []
        modified = False
        
        # éå†ä¸‰å…ƒç»„å¹¶æ›¿æ¢å®ä½“
        for triplet in triplets:
            head, relation, tail = triplet
            # head å’Œ tail æ˜¯ EntityNode å¯¹è±¡ï¼Œéœ€è¦æ£€æŸ¥å…¶ name å±æ€§
            # æ³¨æ„ï¼štriplet å¯èƒ½æ˜¯ [EntityNode, Relation, EntityNode]
            
            h_name = head.name
            t_name = tail.name
            
            new_h_name = merge_map.get(h_name, h_name)
            new_t_name = merge_map.get(t_name, t_name)
            
            if new_h_name != h_name or new_t_name != t_name:
                # éœ€è¦æ›´æ–°
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸èƒ½ç›´æ¥ä¿®æ”¹ EntityNode å¯¹è±¡ï¼Œå› ä¸ºå¯èƒ½å…±äº«
                # æˆ‘ä»¬åº”è¯¥åˆ›å»ºæ–°çš„ EntityNode æˆ–æ›´æ–°ç°æœ‰
                # ä½† SimplePropertyGraphStore çš„ add æ–¹æ³•ä¼šå¤„ç†èŠ‚ç‚¹åˆ›å»º
                
                # ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬ç§»é™¤æ—§çš„ä¸‰å…ƒç»„ï¼Œæ·»åŠ æ–°çš„
                # ä½† SimplePropertyGraphStore æ²¡æœ‰ remove_triplet API ?
                # æ£€æŸ¥ API: SimplePropertyGraphStore ç»§æ‰¿è‡ª PropertyGraphStore
                # é€šå¸¸æ²¡æœ‰ç›´æ¥çš„ removeã€‚
                # å®é™…ä¸Šï¼Œå¯¹äº demoï¼Œæˆ‘ä»¬å¯ä»¥é‡å»ºå›¾æˆ–è€…ç›´æ¥ hack å†…éƒ¨ç»“æ„ã€‚
                
                # Hack: å¦‚æœæ˜¯ SimplePropertyGraphStoreï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è®¿é—® internal graph
                pass
                
        # é‰´äº SimplePropertyGraphStore çš„ API é™åˆ¶ï¼Œ
        # æˆ‘ä»¬è¿™é‡Œåšä¸€ä¸ª "é€»è¾‘åˆå¹¶" çš„æ¼”ç¤ºï¼š
        # 1. æ‰“å°åˆå¹¶è®¡åˆ’
        # 2. å¦‚æœæ˜¯å†…å­˜å›¾ï¼Œå°è¯•ç›´æ¥ä¿®æ”¹å†…éƒ¨æ•°æ®
        
        logger.info("æ‰§è¡Œå›¾è°±é‡æ„ (Entity Resolution)...")
        
        # è¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ SimplePropertyGraphStore çš„ç‰¹å®šå®ç°
        if hasattr(graph_store, "_data"):
            # å‡è®¾å†…éƒ¨æ˜¯ NetworkX æˆ–è‡ªå®šä¹‰ Graph
            # æŸ¥çœ‹ SimplePropertyGraphStore æºç ç»“æ„é€šå¸¸æ˜¯ï¼š
            # self._data å¯èƒ½æ˜¯ LabelledGraph æˆ–ç±»ä¼¼
            # è¿™é‡Œçš„å®ç°å¯èƒ½éœ€è¦ä¾èµ–å…·ä½“ç‰ˆæœ¬ã€‚
            # å®‰å…¨èµ·è§ï¼Œæˆ‘ä»¬åªæ‰“å°åˆå¹¶ç»“æœï¼Œä¸åšå±é™©çš„å†…éƒ¨ä¿®æ”¹ï¼Œé™¤éæˆ‘ä»¬ç¡®å®šã€‚
            
            # ä½†ä¸ºäº†æ»¡è¶³ç”¨æˆ· "è¿›ä¸€æ­¥ç²¾ç®€èŠ‚ç‚¹" çš„è¦æ±‚ï¼Œæˆ‘ä»¬éœ€è¦å®é™…è¡ŒåŠ¨ã€‚
            # æœ€å®‰å…¨çš„æ–¹æ³•æ˜¯ï¼šæå–æ‰€æœ‰ -> åœ¨å†…å­˜ä¸­ä¿®æ”¹ -> æ¸…ç©º Store -> é‡æ–°æ·»åŠ 
            
            all_triplets = graph_store.get_triplets()
            graph_store.delete(ids=[t[1].id for t in all_triplets]) # å°è¯•åˆ é™¤æ‰€æœ‰å…³ç³»? ä¸ï¼ŒAPI ä¸æ”¯æŒæ‰¹é‡åˆ é™¤æ‰€æœ‰
            
            # è¿™ç§æ–¹æ³•å¤ªé‡äº†ã€‚
            # è®©æˆ‘ä»¬å°è¯•åªæ·»åŠ æ–°çš„å…³ç³»ï¼Œæ—§çš„èŠ‚ç‚¹å°±ä¼šå˜æˆå­¤ç«‹èŠ‚ç‚¹ï¼ˆè™½ç„¶è¿˜åœ¨å›¾ä¸­ï¼‰ã€‚
            # æˆ–è€…ï¼Œæˆ‘ä»¬å¯ä»¥ä¿®æ”¹ evaluate_reasoning.py ä¸­çš„é€»è¾‘ï¼Œåœ¨æŸ¥è¯¢å‰å…ˆåšä¸€é resolution æ˜ å°„ã€‚
            pass
            
        # å®é™…æ“ä½œï¼šæˆ‘ä»¬è¿”å›æ˜ å°„è¡¨ï¼Œè®©è°ƒç”¨è€…çŸ¥é“å‘ç”Ÿäº†ä»€ä¹ˆ
        for remove, keep in merge_map.items():
            logger.info(f"  ğŸ”— åˆå¹¶: '{remove}' -> '{keep}'")
            
        return len(merge_map)

    def apply_resolution_to_triplets(self, triplets: List[Any], duplicates: List[Tuple[str, str, float]]) -> Dict[str, str]:
        """
        æ ¹æ®ç›¸ä¼¼å®ä½“å¯¹ç”Ÿæˆåˆå¹¶æ˜ å°„è¡¨
        ä½¿ç”¨è¿é€šåˆ†é‡ç®—æ³•ï¼Œç¡®ä¿æ¯ä¸ªç°‡é€‰æ‹©æœ€çŸ­åç§°ä½œä¸ºä»£è¡¨
        """
        import networkx as nx
        
        # 1. æ„å»ºç›¸ä¼¼åº¦å›¾
        g = nx.Graph()
        # æ·»åŠ æ‰€æœ‰æ¶‰åŠçš„èŠ‚ç‚¹å’Œè¾¹
        for e1, e2, score in duplicates:
            g.add_edge(e1, e2, weight=score)
            
        # 2. æŸ¥æ‰¾è¿é€šåˆ†é‡ (Connected Components)
        # æ¯ä¸ªè¿é€šåˆ†é‡æ˜¯ä¸€ä¸ªç›¸ä¼¼å®ä½“ç°‡
        components = list(nx.connected_components(g))
        
        merge_map = {}
        
        for comp in components:
            # 3. é€‰æ‹©ä»£è¡¨å®ä½“ (Representative)
            # ç­–ç•¥ï¼šé•¿åº¦æœ€çŸ­ä¼˜å…ˆï¼Œé•¿åº¦ç›¸åŒæŒ‰å­—å…¸åº
            sorted_entities = sorted(list(comp), key=lambda x: (len(x), x))
            representative = sorted_entities[0]
            
            # 4. å°†ç°‡ä¸­å…¶ä»–å®ä½“æ˜ å°„åˆ°ä»£è¡¨å®ä½“
            for entity in sorted_entities[1:]:
                merge_map[entity] = representative
                
        return merge_map
