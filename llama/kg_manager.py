#!/usr/bin/env python3
"""
çŸ¥è¯†å›¾è°±ç®¡ç†å™¨ - æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
ä½¿ç”¨å·¥å‚æ¨¡å¼é‡æ„ï¼Œè´Ÿè´£åè°ƒå„ä¸ªç»„ä»¶çš„å·¥ä½œ
"""

import sys
import os
import re
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple
import logging
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from llama_index.core.graph_stores.types import EntityNode, Relation

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from llama.config import setup_logging, DOCUMENT_CONFIG, API_CONFIG, EMBEDDING_CONFIG, NEO4J_CONFIG, OSS_CONFIG, RERANK_CONFIG, EXTRACTOR_CONFIG, ENTITY_DESCRIPTION_CONFIG, HYBRID_SEARCH_CONFIG
from llama.factories import LlamaModuleFactory, ModelFactory, GraphStoreFactory, ExtractorFactory, RerankerFactory
from llama.progress_sse import ProgressTracker, progress_callback
from llama.oss_uploader import COSUploader, OSSConfig
from llama.ocr_parser import DeepSeekOCRParser
# æ³¨é‡Š StandardTermMapper (æ ‡å‡†è¯æ˜ å°„) ç›¸å…³ä»£ç 
# from enhanced_entity_extractor import StandardTermMapper
from llama.graph_agent import GraphAgent
from llama.semantic_chunker import ImprovedSemanticChunker, ImprovedSemanticSplitter
import json
import collections

# å¯¼å…¥ common æ¨¡å—çš„å·¥å…·
from llama.common import (
    get_file_hash,
    DynamicThreadPool
)

class DocumentIndex:
    """æ–‡æ¡£å€’æ’ç´¢å¼• - ç”¨äºåŠ é€Ÿå…³é”®ä¿¡æ¯å®šä½"""
    def __init__(self):
        self.index = collections.defaultdict(list) # keyword -> list of (doc_id, chunk_index)
        
    def build_index(self, documents: List[Any], keywords: List[str]):
        """å»ºç«‹å…³é”®è¯åˆ°æ–‡æ¡£åˆ†å—çš„å€’æ’ç´¢å¼•"""
        logger.info(f"æ­£åœ¨ä¸º {len(documents)} ä¸ªæ–‡æ¡£åˆ†å—å»ºç«‹å€’æ’ç´¢å¼•...")
        start_time = time.time()
        
        for idx, doc in enumerate(documents):
            text = getattr(doc, "text", "")
            doc_id = getattr(doc, "id_", str(idx))
            
            # æ£€æŸ¥æ¯ä¸ªå…³é”®è¯
            for keyword in keywords:
                if keyword in text:
                    self.index[keyword].append((doc_id, idx))
                    
        elapsed = time.time() - start_time
        logger.info(f"å€’æ’ç´¢å¼•å»ºç«‹å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}sï¼ŒåŒ…å« {len(self.index)} ä¸ªå…³é”®è¯æ¡ç›®")
        return self.index

# è®¾ç½®æ—¥å¿—
logger = setup_logging()

class ProcessedFileManager:
    """å·²å¤„ç†æ–‡ä»¶ç®¡ç†å™¨ - æ”¯æŒå¢é‡æ›´æ–°"""
    def __init__(self, record_file: str = "processed_files.json"):
        self.record_file = Path(os.getcwd()) / record_file
        self.processed_files = self._load_records()
        self._dirty = False
        
    def _load_records(self) -> Dict[str, str]:
        if self.record_file.exists():
            try:
                with open(self.record_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"æ— æ³•è¯»å–å¤„ç†è®°å½•: {e}")
                return {}
        return {}
        
    def save_records(self):
        try:
            with open(self.record_file, 'w', encoding='utf-8') as f:
                json.dump(self.processed_files, f, ensure_ascii=False, indent=2)
            self._dirty = False
        except Exception as e:
            logger.error(f"ä¿å­˜å¤„ç†è®°å½•å¤±è´¥: {e}")
            
    def is_processed(self, file_path: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†ä¸”æœªä¿®æ”¹"""
        abs_path = str(Path(file_path).absolute())
        # ä½¿ç”¨ common æ¨¡å—çš„ get_file_hash
        current_hash = get_file_hash(abs_path)
        if not current_hash:
            return False
            
        stored_hash = self.processed_files.get(abs_path)
        return stored_hash == current_hash
        
    def mark_processed(self, file_path: str):
        """æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†"""
        abs_path = str(Path(file_path).absolute())
        # ä½¿ç”¨ common æ¨¡å—çš„ get_file_hash
        current_hash = get_file_hash(abs_path)
        if current_hash:
            self.processed_files[abs_path] = current_hash
            self._dirty = True

class KnowledgeGraphManager:
    """çŸ¥è¯†å›¾è°±ç®¡ç†å™¨ - æ ¸å¿ƒFacade"""
    
    def __init__(self):
        """åˆå§‹åŒ–çŸ¥è¯†å›¾è°±ç®¡ç†å™¨"""
        self.modules = None
        self.llm = None
        self.embed_model = None
        self.graph_store = None
        # ä½¿ç”¨ common æ¨¡å—çš„ DynamicThreadPool æ›¿ä»£ ThreadPoolExecutor
        self.thread_pool = DynamicThreadPool(
            min_workers=2,
            max_workers=DOCUMENT_CONFIG.get("num_workers", 4),
            idle_timeout=60.0
        )
        self._initialized = False
        self.processed_file_manager = ProcessedFileManager()
        self.metrics = {
            "processed_docs": 0,
            "total_docs": 0,
            "entities_count": 0,
            "relationships_count": 0
        }
        self.graph_agent = None  # æ™ºèƒ½å›¾è°±æŸ¥è¯¢ä»£ç†
        
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        try:
            if self._initialized:
                return True
                
            progress_callback("initialization", "æ­£åœ¨åˆå§‹åŒ–çŸ¥è¯†å›¾è°±ç®¡ç†å™¨...")
            
            # 1. åŠ è½½æ¨¡å—
            self.modules = LlamaModuleFactory.get_modules()
            if not self.modules:
                progress_callback("initialization", "æ¨¡å—å¯¼å…¥å¤±è´¥", 0)
                return False
                
            # 2. åˆ›å»ºLLM
            progress_callback("initialization", "æ­£åœ¨åˆå§‹åŒ–LLMæ¨¡å‹...", 20)
            self.llm = ModelFactory.create_llm()
            if not self.llm:
                progress_callback("initialization", "LLMåˆå§‹åŒ–å¤±è´¥", 0)
                return False
            
            # 3. åˆ›å»ºEmbedding
            progress_callback("initialization", "æ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...", 40)
            self.embed_model = ModelFactory.create_embedding_model()
            if not self.embed_model:
                progress_callback("initialization", "Embeddingåˆå§‹åŒ–å¤±è´¥", 0)
                return False
            
            # 4. åˆ›å»ºå›¾å­˜å‚¨
            progress_callback("initialization", "æ­£åœ¨åˆå§‹åŒ–å›¾æ•°æ®åº“...", 60)
            self.graph_store = GraphStoreFactory.create_graph_store()
            if not self.graph_store:
                progress_callback("initialization", "å›¾å­˜å‚¨åˆå§‹åŒ–å¤±è´¥", 0)
                return False
            
            # æ£€æŸ¥å›¾å­˜å‚¨ç±»å‹
            store_type = type(self.graph_store).__name__
            logger.info(f"å›¾å­˜å‚¨ç±»å‹: {store_type}")
            if "Neo4jPropertyGraphStore" not in store_type:
                logger.warning(f"âš ï¸ è­¦å‘Š: å½“å‰ä½¿ç”¨çš„æ˜¯ {store_type} è€Œé Neo4jPropertyGraphStoreã€‚æ•°æ®å°†ä¸ä¼šæŒä¹…åŒ–åˆ° Neo4jï¼")
                progress_callback("initialization", f"âš ï¸ è­¦å‘Š: æœªæ£€æµ‹åˆ°Neo4jé…ç½®ï¼Œæ•°æ®å°†ä¸ä¼šä¿å­˜ï¼", 60)
            
            # æµ‹è¯•è¿æ¥
            progress_callback("initialization", "æ­£åœ¨æµ‹è¯•æ•°æ®åº“è¿æ¥...", 80)
            try:
                self.graph_store.structured_query("MATCH (n) RETURN count(n) LIMIT 1")
                logger.info("Neo4jè¿æ¥æµ‹è¯•æˆåŠŸ")
            except Exception as e:
                logger.warning(f"Neo4jè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
                # ä¸ä¸­æ–­ï¼Œç»§ç»­æ‰§è¡Œï¼Œå› ä¸ºå¯èƒ½æ˜¯ç½‘ç»œæ³¢åŠ¨
            
            # 5. åˆå§‹åŒ–æ™ºèƒ½å›¾è°±æŸ¥è¯¢ä»£ç†
            progress_callback("initialization", "æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½å›¾è°±æŸ¥è¯¢ä»£ç†...", 90)
            try:
                # ä¼ å…¥ LLM å®ä¾‹ä»¥æ”¯æŒ LLM æ„å›¾åˆ†ç±»å™¨
                self.graph_agent = GraphAgent(self.graph_store, llm_instance=self.llm)
                logger.info("âœ… æ™ºèƒ½å›¾è°±æŸ¥è¯¢ä»£ç†åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"æ™ºèƒ½å›¾è°±æŸ¥è¯¢ä»£ç†åˆå§‹åŒ–å¤±è´¥: {e}")
                self.graph_agent = None
            
            progress_callback("initialization", "åˆå§‹åŒ–å®Œæˆ", 100)
            self._initialized = True
            logger.info("âœ… çŸ¥è¯†å›¾è°±ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            progress_callback("initialization", f"åˆå§‹åŒ–å¤±è´¥: {str(e)}", 0)
            return False
    
    def cleanup(self):
        """æ¸…ç†èµ„æºï¼Œé‡Šæ”¾å†…å­˜"""
        try:
            logger.info("å¼€å§‹æ¸…ç†èµ„æº...")
            
            # æ¸…ç†çº¿ç¨‹æ± 
            if hasattr(self, 'thread_pool') and self.thread_pool:
                try:
                    self.thread_pool.shutdown(wait=True)
                    logger.info("âœ… çº¿ç¨‹æ± å·²å…³é—­")
                except Exception as e:
                    logger.warning(f"å…³é—­çº¿ç¨‹æ± å¤±è´¥: {e}")
            
            # æ¸…ç† LLM
            if hasattr(self, 'llm') and self.llm:
                try:
                    del self.llm
                    self.llm = None
                    logger.info("âœ… LLM å·²æ¸…ç†")
                except Exception as e:
                    logger.warning(f"æ¸…ç† LLM å¤±è´¥: {e}")
            
            # æ¸…ç† Embedding æ¨¡å‹
            if hasattr(self, 'embed_model') and self.embed_model:
                try:
                    del self.embed_model
                    self.embed_model = None
                    logger.info("âœ… Embedding æ¨¡å‹å·²æ¸…ç†")
                except Exception as e:
                    logger.warning(f"æ¸…ç† Embedding æ¨¡å‹å¤±è´¥: {e}")
            
            # æ¸…ç†å›¾å­˜å‚¨
            if hasattr(self, 'graph_store') and self.graph_store:
                try:
                    if hasattr(self.graph_store, '_driver') and self.graph_store._driver:
                        self.graph_store._driver.close()
                    del self.graph_store
                    self.graph_store = None
                    logger.info("âœ… å›¾å­˜å‚¨å·²æ¸…ç†")
                except Exception as e:
                    logger.warning(f"æ¸…ç†å›¾å­˜å‚¨å¤±è´¥: {e}")
            
            # æ¸…ç†å›¾è°±ä»£ç†
            if hasattr(self, 'graph_agent') and self.graph_agent:
                try:
                    del self.graph_agent
                    self.graph_agent = None
                    logger.info("âœ… å›¾è°±ä»£ç†å·²æ¸…ç†")
                except Exception as e:
                    logger.warning(f"æ¸…ç†å›¾è°±ä»£ç†å¤±è´¥: {e}")
            
            # æ¸…ç†æ¨¡å—
            if hasattr(self, 'modules') and self.modules:
                try:
                    del self.modules
                    self.modules = None
                    logger.info("âœ… æ¨¡å—å·²æ¸…ç†")
                except Exception as e:
                    logger.warning(f"æ¸…ç†æ¨¡å—å¤±è´¥: {e}")
            
            self._initialized = False
            logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¸…ç†èµ„æºæ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def load_documents(self, progress_tracker: Optional[ProgressTracker] = None) -> list:
        """åŠ è½½æ–‡æ¡£å¹¶ä½¿ç”¨ä¼˜åŒ–çš„åˆ†å—ç­–ç•¥"""
        try:
            if not self.modules:
                self.initialize()
                
            if progress_tracker:
                progress_tracker.update_stage("document_loading", "æ­£åœ¨åŠ è½½æ–‡æ¡£...")
            else:
                progress_callback("document_loading", "æ­£åœ¨åŠ è½½æ–‡æ¡£...", 10)
            
            # ä½¿ç”¨SimpleDirectoryReaderåŠ è½½åŸå§‹æ–‡æ¡£
            import time
            t0 = time.time()
            fe = {}
            try:
                if ".pdf" in DOCUMENT_CONFIG.get('supported_extensions', ['.txt', '.docx', '.pdf']):
                    fe[".pdf"] = DeepSeekOCRParser()
            except Exception as e:
                logger.warning(f"OCRè§£æå™¨ä¸å¯ç”¨ï¼Œå·²è·³è¿‡PDFè§£æ: {e}")
                fe = {}
            reader = self.modules['SimpleDirectoryReader'](
                input_dir=DOCUMENT_CONFIG['path'],
                required_exts=DOCUMENT_CONFIG.get('supported_extensions', ['.txt', '.docx', '.pdf']),
                recursive=True,
                encoding='utf-8',
                file_extractor=fe
            )
            
            try:
                raw_documents = reader.load_data()
            except Exception as e:
                logger.error(f"OCRè§£ææˆ–PDFè§£æå¤±è´¥ï¼Œå°†è·³è¿‡PDFå¹¶é‡è¯•: {e}")
                try:
                    fallback_exts = [ext for ext in DOCUMENT_CONFIG.get('supported_extensions', ['.txt', '.docx', '.pdf']) if ext.lower() != '.pdf']
                    reader_no_pdf = self.modules['SimpleDirectoryReader'](
                        input_dir=DOCUMENT_CONFIG['path'],
                        required_exts=fallback_exts,
                        recursive=True,
                        encoding='utf-8',
                        file_extractor={}
                    )
                    raw_documents = reader_no_pdf.load_data()
                    logger.info("å·²è·³è¿‡PDFæ–‡ä»¶ï¼Œå…¶ä»–ç±»å‹æ–‡æ¡£åŠ è½½æˆåŠŸ")
                except Exception as e2:
                    logger.error(f"é™çº§é‡è¯•ä»å¤±è´¥: {e2}")
                    raw_documents = []
            load_time = time.time() - t0
            logger.info(f"æ–‡æ¡£åŠ è½½è€—æ—¶ {load_time:.2f}s, åŸå§‹æ–‡æ¡£æ•° {len(raw_documents)}")
            
            # å¢é‡å¤„ç†ï¼šè¿‡æ»¤å·²å¤„ç†çš„æ–‡æ¡£
            if DOCUMENT_CONFIG.get("incremental_processing", True):
                new_raw_docs = []
                for doc in raw_documents:
                    file_path = doc.metadata.get('file_path') or doc.metadata.get('file_name')
                    # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦‚æœæ˜¯æ–‡ä»¶åï¼Œå°è¯•æ‹¼æ¥ï¼ˆä¸å¤ªå‡†ç¡®ï¼Œæœ€å¥½æ˜¯full pathï¼‰
                    # LlamaIndex é€šå¸¸å°†ç»å¯¹è·¯å¾„æ”¾åœ¨ file_path ä¸­
                    if file_path and self.processed_file_manager.is_processed(str(file_path)):
                        logger.debug(f"è·³è¿‡å·²å¤„ç†æ–‡æ¡£: {file_path}")
                        continue
                    new_raw_docs.append(doc)
                
                skipped_count = len(raw_documents) - len(new_raw_docs)
                if skipped_count > 0:
                    logger.info(f"å¢é‡å¤„ç†: è·³è¿‡äº† {skipped_count} ä¸ªæœªä¿®æ”¹æ–‡æ¡£")
                raw_documents = new_raw_docs

            # ä½¿ç”¨è‡ªå®šä¹‰çš„åˆ†å—ç­–ç•¥å¤„ç†æ–‡æ¡£
            # æ”¯æŒå¤šçº¿ç¨‹å¤„ç†ä»¥åŠ é€Ÿ chunk åˆ†å‰²
            documents = []
            total_chunks = 0
            total_chars = 0
            chunk_time_sum = 0.0
            filtered_count = 0
            sample_bench_done = False
            
            # è·å–å¤šçº¿ç¨‹é…ç½®
            use_multithreading = DOCUMENT_CONFIG.get("use_multithreading_chunking", True)
            max_workers = DOCUMENT_CONFIG.get("max_chunking_workers", 4)
            
            if use_multithreading and len(raw_documents) > 1:
                # ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†æ–‡æ¡£
                logger.info(f"ä½¿ç”¨å¤šçº¿ç¨‹å¤„ç† {len(raw_documents)} ä¸ªæ–‡æ¡£ (workers={max_workers})")
                
                def process_document(raw_doc):
                    """å¤„ç†å•ä¸ªæ–‡æ¡£çš„å‡½æ•°ï¼Œç”¨äºå¤šçº¿ç¨‹"""
                    t1 = time.time()
                    
                    # åˆ†å—å¤„ç†
                    chunked_docs = self._chunk_document(raw_doc)
                    
                    # å…³é”®è¯é¢„ç­›é€‰
                    relevant_docs = []
                    doc_filtered_count = 0
                    for d in chunked_docs:
                            relevant_docs.append(d)
                    
                    chunk_time = time.time() - t1
                    doc_total_chars = sum(len(getattr(d, "text", "")) for d in relevant_docs)
                    
                    return {
                        'relevant_docs': relevant_docs,
                        'chunked_count': len(chunked_docs),
                        'filtered_count': doc_filtered_count,
                        'chunk_time': chunk_time,
                        'total_chars': doc_total_chars
                    }
                
                # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    # æäº¤æ‰€æœ‰ä»»åŠ¡
                    future_to_doc = {executor.submit(process_document, doc): doc for doc in raw_documents}
                    
                    # æ”¶é›†ç»“æœ
                    for future in as_completed(future_to_doc):
                        try:
                            result = future.result()
                            documents.extend(result['relevant_docs'])
                            total_chunks += result['chunked_count']
                            filtered_count += result['filtered_count']
                            chunk_time_sum += result['chunk_time']
                            total_chars += result['total_chars']
                        except Exception as e:
                            logger.error(f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}")
                
                logger.info(f"å¤šçº¿ç¨‹å¤„ç†å®Œæˆ: æ€»è€—æ—¶ {chunk_time_sum:.2f}s, å¹³å‡æ¯æ–‡æ¡£ {chunk_time_sum/len(raw_documents):.3f}s")
                
            else:
                # ä½¿ç”¨å•çº¿ç¨‹é¡ºåºå¤„ç†æ–‡æ¡£
                logger.info(f"ä½¿ç”¨å•çº¿ç¨‹å¤„ç† {len(raw_documents)} ä¸ªæ–‡æ¡£")
                
            for raw_doc in raw_documents:
                t1 = time.time()
                if DOCUMENT_CONFIG.get("benchmark_chunking", False) and not sample_bench_done:
                    self._benchmark_chunking(raw_doc)
                    sample_bench_done = True
                chunked_docs = self._chunk_document(raw_doc)
                
                # å…³é”®è¯é¢„ç­›é€‰
                relevant_docs = []
                for d in chunked_docs:
                        relevant_docs.append(d)
                
                chunk_time = time.time() - t1
                chunk_time_sum += chunk_time
                documents.extend(relevant_docs)
                total_chunks += len(chunked_docs) # è®°å½•æ€»å—æ•°ï¼ˆåŒ…æ‹¬è¢«è¿‡æ»¤çš„ï¼‰
                for d in relevant_docs:
                    total_chars += len(getattr(d, "text", ""))
            
            if filtered_count > 0:
                logger.info(f"å…³é”®è¯é¢„ç­›é€‰: è¿‡æ»¤äº† {filtered_count} ä¸ªæ— å…³åˆ†å—")
            
            msg = f"æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæœ‰æ•ˆæ–‡æ¡£å— (æ¥è‡ª {len(raw_documents)} ä¸ªåŸå§‹æ–‡æ¡£)"
            if progress_tracker:
                progress_tracker.update_stage("document_loading", msg)
            else:
                progress_callback("document_loading", msg, 15)
                
            logger.info(f"âœ… {msg}")
            if DOCUMENT_CONFIG.get("log_chunk_metrics", False):
                avg_chunk_chars = (total_chars / len(documents)) if documents else 0
                logger.info(f"åˆ†å—ç»Ÿè®¡: æ€»å—æ•° {total_chunks}, æœ‰æ•ˆå—æ•° {len(documents)}, å¹³å‡æœ‰æ•ˆå—é•¿åº¦ {avg_chunk_chars:.1f} å­—ç¬¦, åˆ†å—è€—æ—¶åˆè®¡ {chunk_time_sum:.2f}s")
            
            # å»ºç«‹å€’æ’ç´¢å¼•
            if documents:
                try:
                    indexer = DocumentIndex()
                    # ä½¿ç”¨é¢„å®šä¹‰çš„åŒ»å­¦å…³é”®è¯
                    keywords = [
                        "è¿‘è§†", "è¿œè§†", "æ•£å…‰", "å¼±è§†", "æ–œè§†", "å±ˆå…‰", "è€è§†", "ç™½å†…éšœ", 
                        "è§†åŠ›", "çœ¼è½´", "è§’è†œ", "æ™¶çŠ¶ä½“", "è§†ç½‘è†œ", "è„‰ç»œè†œ", "å·©è†œ", "çœ¼å‹",
                        "é˜¿æ‰˜å“", "OKé•œ", "å¡‘å½¢é•œ", "RGP", "çœ¼é•œ", "æ¥è§¦é•œ", "æ‰‹æœ¯", "æ¿€å…‰"
                    ]
                    self.document_index = indexer.build_index(documents, keywords)
                except Exception as e:
                    logger.warning(f"å»ºç«‹å€’æ’ç´¢å¼•å¤±è´¥: {e}")

            return documents
            
        except Exception as e:
            error_msg = f"åŠ è½½æ–‡æ¡£å¤±è´¥: {e}"
            logger.error(error_msg)
            if progress_tracker:
                progress_tracker.error("document_loading", error_msg)
            else:
                progress_callback("document_loading", error_msg, 0)
            return []
    
    def _chunk_document(self, document) -> List[Any]:
        """ä½¿ç”¨æ”¹è¿›çš„è¯­ä¹‰åˆ†å‰²ç­–ç•¥å¤„ç†å•ä¸ªæ–‡æ¡£
        
        é‡‡ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼š
        1. ç»“æ„åŒ–åˆ‡åˆ†ï¼šæŒ‰æ®µè½ï¼ˆåŒæ¢è¡Œ \n\nï¼‰åˆ‡åˆ†ï¼Œä¿ç•™åŸºæœ¬æ’ç‰ˆé€»è¾‘
        2. è¯­ä¹‰èšåˆï¼šè®¡ç®—ç›¸é‚»æ®µè½ç›¸ä¼¼åº¦ï¼Œé«˜ç›¸ä¼¼åº¦åˆ™åˆå¹¶ï¼Œç›´åˆ°è¾¾åˆ°å¤§å°é™åˆ¶
        3. é‡å ä¿ç•™ï¼šæ¯ä¸ª chunk ä¿ç•™ 10%-15% çš„é‡å¤å†…å®¹
        
        Args:
            document: åŸå§‹æ–‡æ¡£å¯¹è±¡
            
        Returns:
            åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
        """
        # è·å–é…ç½®å‚æ•°
        text_len = len(getattr(document, "text", ""))
        
        # æ–‡æ¡£åˆ†å—è¯Šæ–­æ—¥å¿—ï¼šè®°å½•åŸå§‹æ–‡æ¡£ä¿¡æ¯
        logger.info(f"ğŸ“„ æ–‡æ¡£åˆ†å—è¯Šæ–­ - åŸå§‹æ–‡æœ¬é•¿åº¦: {text_len:,} å­—ç¬¦")
        
        use_semantic = DOCUMENT_CONFIG.get('use_semantic_chunking', True)
        dyn = DOCUMENT_CONFIG.get('dynamic_chunking', False)
        base_chunk_size = DOCUMENT_CONFIG.get('chunk_size', 1024)
        max_chunk_length = DOCUMENT_CONFIG.get('max_chunk_length', 1400)
        min_chunk_length = DOCUMENT_CONFIG.get('min_chunk_length', 600)
        target_chars = DOCUMENT_CONFIG.get('dynamic_target_chars_per_chunk', base_chunk_size)
        
        # åŠ¨æ€è°ƒæ•´ chunk_size
        if dyn and text_len > 0:
            target_chars = DOCUMENT_CONFIG.get('dynamic_target_chars_per_chunk', base_chunk_size)
            chunk_size = max(min_chunk_length, min(max_chunk_length, target_chars))
            
            # å®ä½“å¯†åº¦æ£€æµ‹
            medical_keywords = ["è¿‘è§†", "è¿œè§†", "æ•£å…‰", "çœ¼è½´", "è§’è†œ", "è§†ç½‘è†œ", "è„‰ç»œè†œ", "çœ¼å‹", "è°ƒèŠ‚", "å±ˆå…‰"]
            doc_text = getattr(document, "text", "")
            if len(doc_text) > 0:
                keyword_count = sum(doc_text.count(k) for k in medical_keywords)
                density = keyword_count / len(doc_text)
                
                if density > 0.005:
                    logger.info(f"æ£€æµ‹åˆ°é«˜å¯†åº¦åŒ»å­¦æ–‡æœ¬ (å¯†åº¦: {density:.2%})ï¼Œè‡ªåŠ¨ç¼©å°åˆ†å—å¤§å°")
                    chunk_size = int(chunk_size * 0.8)
                    chunk_size = max(chunk_size, min_chunk_length)
        else:
            chunk_size = base_chunk_size
            
        # ä½¿ç”¨æ”¹è¿›çš„è¯­ä¹‰åˆ†å‰²å™¨
        import time
        t0 = time.time()
        
        if use_semantic:
            # ä½¿ç”¨æ”¹è¿›çš„è¯­ä¹‰åˆ†å‰²å™¨
            logger.debug("ä½¿ç”¨æ”¹è¿›çš„è¯­ä¹‰åˆ†å‰²å™¨è¿›è¡Œåˆ†å—ï¼ˆæ®µè½åˆ‡åˆ† + è¯­ä¹‰èšåˆ + é‡å ä¿ç•™ï¼‰")
            embedding_model = self.modules.get('embedding_model')
            
            # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„å‚æ•°
            improved_chunker = ImprovedSemanticChunker(
                embedding_model=embedding_model,
                chunk_size=chunk_size,
                overlap_ratio=0.12,  # 12% é‡å 
                similarity_threshold=0.70,  # ç›¸ä¼¼åº¦é˜ˆå€¼
                min_chunk_length=min_chunk_length,
                max_chunk_length=max_chunk_length
            )
            
            # ç›´æ¥åˆ†å‰²æ–‡æœ¬
            doc_text = getattr(document, "text", "")
            chunks = improved_chunker.split_text(doc_text)
            
            # å°† chunks è½¬æ¢ä¸ºèŠ‚ç‚¹
            nodes = []
            for i, chunk in enumerate(chunks):
                metadata = getattr(document, "metadata", {}).copy()
                metadata["chunk_index"] = i
                metadata["chunk_total"] = len(chunks)
                metadata["chunking_method"] = "improved_semantic"
                metadata["overlap_ratio"] = 0.12
                metadata["similarity_threshold"] = 0.70
                
                node = self.modules['Document'](text=chunk, metadata=metadata)
                nodes.append(node)
        else:
            # ä½¿ç”¨ä¼ ç»Ÿçš„å¥å­åˆ†å‰²å™¨
            logger.debug("ä½¿ç”¨ä¼ ç»Ÿå¥å­åˆ†å‰²å™¨è¿›è¡Œåˆ†å—")
            from llama_index.core.node_parser import SentenceSplitter
            chunk_overlap = max(0, min(int(chunk_size * 0.2), 200, DOCUMENT_CONFIG.get('CHUNK_OVERLAP', int(chunk_size * 0.2))))
            sentence_splitter = DOCUMENT_CONFIG.get('sentence_splitter', 'ã€‚ï¼ï¼Ÿ!?')
            semantic_separator = DOCUMENT_CONFIG.get('semantic_separator', '\n\n')
            
            node_parser = SentenceSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separator=semantic_separator,
                paragraph_separator=semantic_separator,
                include_prev_next_rel=True
            )
            
            nodes = node_parser.get_nodes_from_documents([document])
        
        gen_nodes_time = time.time() - t0
        
        # è¿‡æ»¤å’Œä¼˜åŒ–å—å¤§å°
        filtered_nodes = []
        for node in nodes:
            text_length = len(node.text)
            
            if text_length < min_chunk_length and filtered_nodes:
                pass
            
            if text_length > max_chunk_length:
                sub_chunks = self._split_large_chunk(node, max_chunk_length, int(chunk_size * 0.12))
                filtered_nodes.extend(sub_chunks)
            else:
                processed_node = self._ensure_medical_terminology_integrity(node)
                filtered_nodes.append(processed_node)
        
        # å°†èŠ‚ç‚¹è½¬æ¢å›æ–‡æ¡£å¯¹è±¡
        documents = []
        total_chars = 0
        for node in filtered_nodes:
            doc = self.modules['Document'](
                text=node.text,
                metadata=node.metadata
            )
            documents.append(doc)
            total_chars += len(node.text)
        
        # è¿‡æ»¤æ‰å­—æ•°å¤ªå°‘ï¼ˆ<50å­—ï¼‰æˆ–ä¸­æ–‡æå°‘ï¼ˆå¯èƒ½æ˜¯çº¯å›¾ä¹±ç ï¼‰çš„ Chunk
        import re
        filtered_documents = []
        noise_count = 0
        for doc in documents:
            text = getattr(doc, "text", "")
            text_len = len(text)
            
            # æ£€æŸ¥1: å­—æ•°æ˜¯å¦ >= 50
            if text_len < 50:
                noise_count += 1
                continue
            
            # æ£€æŸ¥2: ä¸­æ–‡å­—ç¬¦å æ¯”ï¼ˆä¸­æ–‡å­—ç¬¦åº”è¯¥å ä¸€å®šæ¯”ä¾‹ï¼Œé¿å…çº¯å›¾ä¹±ç ï¼‰
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
            chinese_ratio = chinese_chars / text_len if text_len > 0 else 0
            
            # å¦‚æœæ–‡æœ¬é•¿åº¦åœ¨ 50-200 ä¹‹é—´ï¼Œè¦æ±‚ä¸­æ–‡å æ¯” >= 30%
            # å¦‚æœæ–‡æœ¬é•¿åº¦ > 200ï¼Œè¦æ±‚ä¸­æ–‡å æ¯” >= 20%
            min_chinese_ratio = 0.30 if text_len <= 200 else 0.20
            if chinese_ratio < min_chinese_ratio:
                noise_count += 1
                continue
            
            filtered_documents.append(doc)
        
        # æ›´æ–° documents å’Œç»Ÿè®¡ä¿¡æ¯
        documents = filtered_documents
        total_chars = sum(len(getattr(d, "text", "")) for d in documents)
        
        if noise_count > 0:
            logger.info(f"ğŸ§¹ è¿‡æ»¤ OCR å™ªéŸ³: ç§»é™¤äº† {noise_count} ä¸ªæ— æ•ˆ chunkï¼ˆå­—æ•°<50 æˆ–ä¸­æ–‡å æ¯”è¿‡ä½ï¼‰")
        
        if DOCUMENT_CONFIG.get("log_chunk_metrics", True):
            avg_len = (total_chars / len(documents)) if documents else 0
            chunker_type = "æ”¹è¿›è¯­ä¹‰" if use_semantic else "ä¼ ç»Ÿ"
            
            # å¢å¼ºçš„è¯Šæ–­æ—¥å¿—ï¼šåŒ…å«åŸå§‹æ–‡æœ¬é•¿åº¦ã€åˆ†å—åæ•°é‡ã€å¹³å‡ chunk é•¿åº¦
            logger.info(
                f"ChunkStats[{chunker_type}]: size={chunk_size}, overlap=12%, "
                f"åŸå§‹é•¿åº¦={text_len:,} å­—ç¬¦, "
                f"åˆ†å—åæ•°é‡={len(documents)}, "
                f"å¹³å‡ chunk é•¿åº¦={avg_len:.1f} å­—ç¬¦, "
                f"ç”Ÿæˆæ—¶é—´={gen_nodes_time:.2f}s"
            )
            
            # è®¡ç®—é¢„æœŸçš„ chunk æ•°é‡ï¼ˆç”¨äºå¯¹æ¯”éªŒè¯ï¼‰
            if chunk_size > 0:
                expected_chunks = (text_len - int(chunk_size * 0.12)) / (chunk_size - int(chunk_size * 0.12))
                logger.info(
                    f"ğŸ“Š åˆ†å—å¯¹æ¯”: å®é™…={len(documents)} ä¸ª chunks, "
                    f"ç†è®ºé¢„æœŸâ‰ˆ{expected_chunks:.0f} ä¸ª chunks "
                    f"(åŸºäº chunk_size={chunk_size}, overlap={int(chunk_size * 0.12)})"
                )
        
        return documents
    
    def _split_large_chunk(self, node, max_length: int, overlap: int) -> List[Any]:
        """é€’å½’åˆ†å‰²è¿‡å¤§çš„æ–‡æœ¬å—
        
        Args:
            node: èŠ‚ç‚¹å¯¹è±¡
            max_length: æœ€å¤§é•¿åº¦
            overlap: é‡å å­—ç¬¦æ•°
            
        Returns:
            åˆ†å‰²åçš„èŠ‚ç‚¹åˆ—è¡¨
        """
        text = node.text
        if len(text) <= max_length:
            return [node]
        
        # æ‰¾åˆ°åˆé€‚çš„åˆ†å‰²ç‚¹ï¼ˆä¼˜å…ˆåœ¨å¥å­è¾¹ç•Œåˆ†å‰²ï¼‰
        split_points = []
        current_pos = 0
        
        # æŸ¥æ‰¾å¥å­åˆ†éš”ç¬¦
        sentence_separators = list(DOCUMENT_CONFIG.get('sentence_splitter', 'ã€‚ï¼ï¼Ÿ!?'))
        
        while current_pos < len(text) - max_length:
            # åœ¨æœ€å¤§é•¿åº¦é™„è¿‘æŸ¥æ‰¾å¥å­åˆ†éš”ç¬¦
            search_start = current_pos + max_length - 100  # ç•™å‡º100å­—ç¬¦çš„æœç´¢ç©ºé—´
            search_end = min(current_pos + max_length, len(text))
            
            split_pos = -1
            for sep in sentence_separators:
                # ä»åå¾€å‰æœç´¢ï¼Œæ‰¾åˆ°æœ€æ¥è¿‘max_lengthçš„åˆ†éš”ç¬¦
                pos = text.rfind(sep, current_pos, search_end)
                if pos != -1 and pos > current_pos:
                    split_pos = pos + 1  # åŒ…å«åˆ†éš”ç¬¦
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„å¥å­åˆ†éš”ç¬¦ï¼Œå°±åœ¨æœ€å¤§é•¿åº¦å¤„åˆ†å‰²
            if split_pos == -1:
                split_pos = min(current_pos + max_length, len(text))
            
            split_points.append(split_pos)
            current_pos = split_pos
        
        # åˆ›å»ºåˆ†å‰²åçš„èŠ‚ç‚¹
        nodes = []
        start_pos = 0
        for end_pos in split_points:
            chunk_text = text[start_pos:end_pos]
            
            # åˆ›å»ºæ–°èŠ‚ç‚¹
            new_node = self.modules['Document'](
                text=chunk_text,
                metadata=node.metadata.copy()
            )
            nodes.append(new_node)
            
            # æ›´æ–°èµ·å§‹ä½ç½®ï¼Œè€ƒè™‘é‡å 
            start_pos = max(end_pos - overlap, 0)
        
        # å¤„ç†æœ€åä¸€ä¸ªå—
        if start_pos < len(text):
            last_chunk = text[start_pos:]
            if len(last_chunk) > 0:  # ç¡®ä¿ä¸æ˜¯ç©ºå—
                nodes.append(self.modules['Document'](text=last_chunk, metadata=node.metadata.copy()))
        
        return nodes
    
    def _ensure_medical_terminology_integrity(self, node) -> Any:
        """ç¡®ä¿åŒ»å­¦æœ¯è¯­å®Œæ•´æ€§
        å¢åŠ è¾¹ç•Œæ£€æµ‹ï¼šç¡®ä¿æ¯ä¸ªå®ä½“çš„é¦–å°¾éƒ½å‡ºç°åœ¨åŒä¸€chunkä¸­
        """
        text = node.text
        
        # å…³é”®åŒ»å­¦æœ¯è¯­åˆ—è¡¨ï¼Œç”¨äºæ£€æŸ¥è¾¹ç•Œæˆªæ–­
        critical_terms = [
            "è§’è†œå¡‘å½¢é•œ", "ä½æµ“åº¦é˜¿æ‰˜å“", "çœ¼è½´é•¿åº¦", "ç—…ç†æ€§è¿‘è§†", "è§†ç½‘è†œè„±è½",
            "è°ƒèŠ‚å¹…åº¦", "LogMARè§†åŠ›è¡¨", "å…¨é£ç§’æ¿€å…‰æ‰‹æœ¯", "å‡†åˆ†å­æ¿€å…‰æ‰‹æœ¯"
        ]
        
        # å¸¸è§çš„æœ‰æ•ˆå­æœ¯è¯­ï¼ˆå¦‚æœæˆªæ–­åœ¨è¿™ä¸ªä½ç½®ï¼Œæ˜¯å¯ä»¥æ¥å—çš„ï¼Œæˆ–è€…æ˜¯ç‹¬ç«‹çš„å®ä½“ï¼‰
        valid_subterms = ["è§’è†œ", "è§†ç½‘è†œ", "è¿‘è§†", "è°ƒèŠ‚", "çœ¼è½´", "æ‰‹æœ¯", "æ¿€å…‰"]
        
        # æ£€æŸ¥æœ«å°¾æˆªæ–­
        # å¦‚æœæ–‡æœ¬ä»¥æŸä¸ªæœ¯è¯­çš„å‰ç¼€ç»“å°¾ï¼ˆä½†ä¸æ˜¯å®Œæ•´æœ¯è¯­ï¼‰ï¼Œä¸”è¯¥å‰ç¼€æœ¬èº«ä¸æ˜¯æœ‰æ•ˆæœ¯è¯­ï¼Œåˆ™æˆªæ–­å®ƒ
        # ä¾é  overlap åœ¨ä¸‹ä¸€ä¸ª chunk ä¸­å®Œæ•´è¯»å–
        for term in critical_terms:
            # æ£€æŸ¥é•¿åº¦è‡³å°‘ä¸º2çš„å‰ç¼€
            for i in range(2, len(term)):
                prefix = term[:i]
                if text.endswith(prefix):
                    # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯å®Œæ•´æœ¯è¯­ï¼ˆé€šè¿‡æ˜¯å¦èƒ½åŒ¹é…æ›´é•¿çš„å‰ç¼€æ¥åˆ¤æ–­ - å¾ªç¯ä¼šç»§ç»­ï¼‰
                    # ä½†åœ¨è¿™é‡Œæˆ‘ä»¬åªçœ‹å½“å‰ prefixã€‚å¦‚æœ text ä»¥ prefix ç»“å°¾ï¼Œ
                    # æˆ‘ä»¬éœ€è¦ç¡®è®¤å®ƒä¸æ˜¯å®Œæ•´ term çš„ä¸€éƒ¨åˆ†ï¼ˆå³ text ç»“å°¾å°±æ˜¯ prefixï¼Œè€Œä¸æ˜¯ prefix + ...ï¼‰
                    # text.endswith(prefix) å·²ç»æ˜¯ç¡®è®¤äº†ã€‚
                    
                    # åªè¦é•¿åº¦ä¸ç­‰äº term çš„é•¿åº¦ï¼Œå°±æ˜¯éƒ¨åˆ†åŒ¹é…
                    if len(prefix) < len(term):
                        # æ£€æŸ¥è¿™ä¸ªå‰ç¼€æ˜¯å¦æœ¬èº«å°±æ˜¯æœ‰æ•ˆè¯
                        if prefix in valid_subterms:
                            continue
                            
                        # è¿™æ˜¯ä¸€ä¸ªä¸å®Œæ•´çš„æˆªæ–­ï¼Œä¾‹å¦‚ "è§’è†œå¡‘"
                        # æˆ‘ä»¬å°†å…¶ç§»é™¤ï¼Œè®©ä¸‹ä¸€ä¸ª chunk (æœ‰ overlap) æ¥å¤„ç†å®Œæ•´çš„ "è§’è†œå¡‘å½¢é•œ"
                        logger.debug(f"è¾¹ç•Œæ£€æµ‹: å‘ç°æœ«å°¾æˆªæ–­çš„æœ¯è¯­ç‰‡æ®µ '{prefix}' (åŸè¯: {term})ï¼Œå·²è‡ªåŠ¨ä¿®å‰ª")
                        # åˆ›å»ºæ–°çš„ Document å¯¹è±¡æ¥æ›¿æ¢åŸæ¥çš„å¯¹è±¡
                        new_text = text[:-len(prefix)]
                        return self.modules['Document'](text=new_text, metadata=node.metadata.copy())
        
        return node
    
    def _chunk_with_params(self, document, chunk_size: int, chunk_overlap: int, max_chunk_length: int, min_chunk_length: int) -> List[Any]:
        use_semantic = DOCUMENT_CONFIG.get('use_semantic_chunking', True)
        
        if use_semantic:
            embedding_model = self.modules.get('embedding_model')
            
            semantic_splitter = ImprovedSemanticSplitter(
                embedding_model=embedding_model,
                chunk_size=chunk_size,
                overlap_ratio=chunk_overlap / chunk_size if chunk_size > 0 else 0.12,
                min_chunk_length=min_chunk_length,
                max_chunk_length=max_chunk_length,
                similarity_threshold=DOCUMENT_CONFIG.get('similarity_threshold', 0.75),
                paragraph_separator=DOCUMENT_CONFIG.get('semantic_separator', '\n\n')
            )
            
            nodes = semantic_splitter.get_nodes_from_documents([document])
        else:
            from llama_index.core.node_parser import SentenceSplitter
            sentence_splitter = DOCUMENT_CONFIG.get('sentence_splitter', 'ã€‚ï¼ï¼Ÿ!?')
            semantic_separator = DOCUMENT_CONFIG.get('semantic_separator', '\n\n')
            node_parser = SentenceSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separator=semantic_separator,
                paragraph_separator=semantic_separator,
                include_prev_next_rel=True
            )
            nodes = node_parser.get_nodes_from_documents([document])
        
        filtered_nodes = []
        for node in nodes:
            text_length = len(node.text)
            if text_length < min_chunk_length and filtered_nodes:
                prev_node = filtered_nodes[-1]
                combined_text = prev_node.text + " " + node.text
                if len(combined_text) <= max_chunk_length:
                    merged_node = self.modules['Document'](text=combined_text, metadata=prev_node.metadata.copy())
                    merged_node.id_ = f"{prev_node.id_}_merged"
                    filtered_nodes[-1] = merged_node
                    continue
            if text_length > max_chunk_length:
                sub_chunks = self._split_large_chunk(node, max_chunk_length, chunk_overlap)
                filtered_nodes.extend(sub_chunks)
            else:
                filtered_nodes.append(node)
        docs = []
        for n in filtered_nodes:
            docs.append(self.modules['Document'](text=n.text, metadata=n.metadata))
        return docs
    
    def _benchmark_chunking(self, document):
        import time
        old_size = 600
        old_overlap = 80
        old_max = 800
        old_min = 500
        t0 = time.time()
        old_docs = self._chunk_with_params(document, old_size, old_overlap, old_max, old_min)
        old_t = time.time() - t0
        new_size = DOCUMENT_CONFIG.get('chunk_size', 1024)
        new_overlap = DOCUMENT_CONFIG.get('chunk_overlap', 120)
        new_max = DOCUMENT_CONFIG.get('max_chunk_length', 1400)
        new_min = DOCUMENT_CONFIG.get('min_chunk_length', 600)
        t1 = time.time()
        new_docs = self._chunk_with_params(document, new_size, new_overlap, new_max, new_min)
        new_t = time.time() - t1
        old_chars = sum(len(d.text) for d in old_docs)
        new_chars = sum(len(d.text) for d in new_docs)
        logger.info(f"BenchmarkChunking: old(size={old_size},overlap={old_overlap}) chunks={len(old_docs)} time={old_t:.2f}s avg_len={(old_chars/len(old_docs)) if old_docs else 0:.1f}")
        logger.info(f"BenchmarkChunking: new(size={new_size},overlap={new_overlap}) chunks={len(new_docs)} time={new_t:.2f}s avg_len={(new_chars/len(new_docs)) if new_docs else 0:.1f}")
    
    def build_knowledge_graph(self, documents: list, progress_tracker: Optional[ProgressTracker] = None) -> Any:
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        if not documents:
            error_msg = "æ²¡æœ‰æ–‡æ¡£å¯ç”¨äºæ„å»ºçŸ¥è¯†å›¾è°±"
            if progress_tracker:
                progress_tracker.error("knowledge_graph", error_msg)
            return None
        
        try:
            if progress_tracker:
                progress_tracker.update_stage("knowledge_graph", "å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
            else:
                progress_callback("knowledge_graph", "å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...", 20)
            
            # # 0. é¢„å¤„ç†ï¼šåŸºäºåˆ«åæ˜ å°„æ›¿æ¢æ–‡æœ¬ä¸­çš„éæ ‡å®ä½“
            # if EXTRACTOR_CONFIG.get("alias_mapping"):
            #     logger.info("æ­£åœ¨æ‰§è¡Œæ–‡æœ¬é¢„å¤„ç†ï¼šåˆ«åæ›¿æ¢...")
            #     mapping = EXTRACTOR_CONFIG["alias_mapping"]
                
            #     # é¢„ç¼–è¯‘æ­£åˆ™ï¼šæŒ‰é•¿åº¦é™åºæ’åºï¼Œç¡®ä¿ä¼˜å…ˆåŒ¹é…é•¿è¯
            #     sorted_aliases = sorted(mapping.keys(), key=len, reverse=True)
            #     pattern_str = '|'.join(map(re.escape, sorted_aliases))
            #     pattern = re.compile(pattern_str)
                
            #     processed_count = 0
            #     for doc in documents:
            #         if not hasattr(doc, "text") or not doc.text:
            #             continue
                    
            #         original_text = doc.text
            #         # ä½¿ç”¨æ­£åˆ™ä¸€æ¬¡æ€§æ›¿æ¢ï¼Œé¿å…é€’å½’æ›¿æ¢é—®é¢˜ (å¦‚ AL->çœ¼è½´é•¿åº¦, ç„¶å çœ¼è½´->çœ¼è½´é•¿åº¦ => çœ¼è½´é•¿åº¦é•¿åº¦)
            #         modified_text = pattern.sub(lambda m: mapping[m.group(0)], original_text)
                    
            #         if modified_text != original_text:
            #             if hasattr(doc, "set_content"):
            #                 doc.set_content(modified_text)
            #             else:
            #                 doc.text = modified_text
            #             processed_count += 1
                
            #     logger.info(f"åˆ«åæ›¿æ¢å®Œæˆï¼Œå…±ä¿®æ”¹äº† {processed_count} ä¸ªæ–‡æ¡£")
            
            # åˆ›å»ºæå–å™¨
            if progress_tracker:
                progress_tracker.update_stage("knowledge_graph", "æ­£åœ¨åˆ›å»ºå®ä½“æå–å™¨...", 25)
                
            extractor = ExtractorFactory.create_extractor(self.llm)
            if not extractor:
                error_msg = "å®ä½“æå–å™¨åˆ›å»ºå¤±è´¥"
                if progress_tracker:
                    progress_tracker.error("knowledge_graph", error_msg)
                return None
            
            total_docs = len(documents)
            self.metrics["total_docs"] = total_docs
            
            if progress_tracker:
                progress_tracker.update_stage("knowledge_graph", "æ­£åœ¨åˆå§‹åŒ–å›¾è°±ç´¢å¼•...", 30)
            
            # ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºéª¨æ¶ï¼ˆDocument + Chunkï¼‰
            if progress_tracker:
                progress_tracker.update_stage("knowledge_graph", "æ­£åœ¨åˆ›å»ºæ–‡æ¡£å’Œå—éª¨æ¶...", 32)
            self._create_document_chunk_skeleton(documents)
            
            # åˆå§‹åŒ–ç©ºç´¢å¼• (ä»…å»ºç«‹ç®¡é“)
            # ä½¿ç”¨ç©ºåˆ—è¡¨åˆå§‹åŒ–ï¼ŒPropertyGraphIndex ä¼šè®¾ç½®å¥½ store å’Œ extractors
            index = self.modules['PropertyGraphIndex'].from_documents(
                [],
                llm=self.llm,
                embed_model=self.embed_model,
                property_graph_store=self.graph_store,
                kg_extractors=[extractor],
                show_progress=True
            )
            
            # æ”¶é›†å¾…æ ‡è®°çš„æ–‡ä»¶è·¯å¾„
            file_paths_to_mark = set()
            for doc in documents:
                # å°è¯•è·å–æ–‡ä»¶è·¯å¾„
                fp = doc.metadata.get('file_path') or doc.metadata.get('file_name')
                if fp:
                    file_paths_to_mark.add(str(fp))
            
            # æ‰¹é‡å¹¶è¡Œå¤„ç†æ–‡æ¡£
            import time
            start_time = time.time()
            
            logger.info(f"å¼€å§‹å¹¶è¡Œå¤„ç† {len(documents)} ä¸ªæ–‡æ¡£å—...")
            
            # ä½¿ç”¨æ‰¹å¤„ç†ä»¥æ”¯æŒç»†ç²’åº¦è¿›åº¦æ›´æ–°
            total_docs = len(documents)
            batch_size = DOCUMENT_CONFIG.get("batch_size", 5)
            
            # è¿›åº¦èŒƒå›´: 35% -> 90%
            start_pct = 35
            end_pct = 90
            
            for i in range(0, total_docs, batch_size):
                batch = documents[i:i + batch_size]
                current_batch_end = min(i + batch_size, total_docs)
                
                progress = start_pct + ((current_batch_end / total_docs) * (end_pct - start_pct))
                msg = f"æ­£åœ¨å¤„ç†æ–‡æ¡£å— {i + 1}-{current_batch_end}/{total_docs}"
                update_every = max(1, int(DOCUMENT_CONFIG.get("progress_update_every_batches", 1)))
                batch_index = i // batch_size
                should_update = (batch_index % update_every == 0) or (current_batch_end == total_docs)
                if should_update:
                    if progress_tracker:
                        progress_tracker.update_stage("knowledge_graph", msg, progress)
                    else:
                        if i % (batch_size * 2) == 0:
                            logger.info(f"{msg} ({progress:.1f}%)")
                
                # æ’å…¥èŠ‚ç‚¹ï¼ˆå®ä½“æå–ï¼‰
                index.insert_nodes(batch)
                
                # æ¯å¤„ç†å®Œä¸€æ‰¹åï¼Œæ¸…ç†å†…å­˜
                if i % (batch_size * 5) == 0:
                    import gc
                    gc.collect()
                    logger.debug(f"å·²å¤„ç† {current_batch_end}/{total_docs} ä¸ªæ–‡æ¡£å—ï¼Œæ¸…ç†å†…å­˜")
            
            # # 3. åå¤„ç†ï¼šåˆ›å»ºè¯­ä¹‰å¼±å…³è”
            # if progress_tracker:
            #     progress_tracker.update_stage("knowledge_graph", "æ­£åœ¨åˆ†æè¯­ä¹‰å¼±å…³è”...", 95)
            # self._create_semantic_relationships(documents, index)
            
            self.metrics["processed_docs"] = total_docs
            e_count, r_count = self._get_graph_counts(self.graph_store)
            self.metrics["entities_count"] = e_count
            self.metrics["relationships_count"] = r_count
            
            # æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†
            processed_count = 0
            for fp in file_paths_to_mark:
                self.processed_file_manager.mark_processed(fp)
                processed_count += 1
            
            if processed_count > 0:
                logger.info(f"å·²æ ‡è®° {processed_count} ä¸ªæ–‡ä»¶ä¸ºå·²å¤„ç†")
            if getattr(self.processed_file_manager, "_dirty", False):
                self.processed_file_manager.save_records()

            # å®ä½“å¯¹é½ - å·²æ³¨é‡Šï¼šä½¿ç”¨ç‹¬ç«‹çš„ offline_entity_alignment.py è„šæœ¬
            # self._perform_entity_resolution(index, progress_tracker)
            
            # ä¸ºæ‰€æœ‰å®ä½“ç”Ÿæˆæè¿°
            # if ENTITY_DESCRIPTION_CONFIG.get("enable", False):
            #     self._generate_entity_descriptions(index, progress_tracker)
            # else:
            #     logger.info("å®ä½“æè¿°ç”Ÿæˆå·²ç¦ç”¨")
            
            # åˆ›å»ºæº¯æºç»“æ„: (Entity)-[:MENTIONS]->(Chunk)-[:FROM]->(Document)
            self._create_provenance_structure(documents, index, progress_tracker)

            if progress_tracker:
                progress_tracker.update_stage("knowledge_graph", "çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ", 100)
            else:
                progress_callback("knowledge_graph", "çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ", 100)
                
            total_time = time.time() - start_time
            logger.info(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼Œè€—æ—¶ {total_time:.2f} ç§’")
            return index
            
        except Exception as e:
            error_msg = f"æ„å»ºçŸ¥è¯†å›¾è°±å¤±è´¥: {e}"
            logger.error(error_msg)
            if progress_tracker:
                progress_tracker.error("knowledge_graph", error_msg)
            else:
                progress_callback("knowledge_graph", error_msg, 0)
            return None
    
    def _create_semantic_relationships(self, documents: List[Any], index: Any):
        """
        åˆ›å»ºè¯­ä¹‰å¼±å…³è”
        è‹¥åŒä¸€æ–‡æœ¬å—ä¸­å‡ºç°ä¸¤ä¸ªæ ‡å‡†å®ä½“ä¸”æœªå»ºç«‹å…³ç³»ï¼Œåˆ™åˆ›å»º 'RELATED_TO' å¼±å…³è”
        å·²æ³¨é‡Šï¼šç§»é™¤ StandardTermMapper (æ ‡å‡†è¯æ˜ å°„) ç›¸å…³ä»£ç 
        """
        logger.info("æ­£åœ¨åˆ†ææ½œåœ¨çš„è¯­ä¹‰å¼±å…³è”...")
        # æ³¨é‡Š StandardTermMapper (æ ‡å‡†è¯æ˜ å°„) ç›¸å…³ä»£ç 
        # from enhanced_entity_extractor import StandardTermMapper
        # from llama_index.core.graph_stores.types import Relation
        # import itertools
        
        # new_relations = []
        # count = 0
        
        # # å»ºç«‹å®ä½“åˆ°æ ‡å‡†åçš„æ˜ å°„ä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾
        # # StandardTermMapper.STANDARD_ENTITIES æ˜¯ä¸ª set
        
        # for doc in documents:
        #     text = getattr(doc, "text", "")
        #     if not text:
        #         continue
        #         
        #     found_entities = []
        #     # ç®€å•çš„å­—ç¬¦ä¸²åŒ¹é… 
        #     # ä¼˜åŒ–ï¼šåªæ£€æŸ¥é•¿åº¦ > 1 çš„å®ä½“
        #     for entity in StandardTermMapper.STANDARD_ENTITIES:
        #         if entity in text:
        #             found_entities.append(entity)
        #     
        #     # å¦‚æœæ‰¾åˆ°2ä¸ªä»¥ä¸Šå®ä½“
        #     if len(found_entities) >= 2:
        #         # ç”Ÿæˆä¸¤ä¸¤ç»„åˆ
        #         for e1, e2 in itertools.combinations(found_entities, 2):
        #             rel = Relation(
        #                 source_id=e1,
        #                 target_id=e2,
        #                 label="RELATED_TO",
        #                 properties={"confidence": "low", "type": "co_occurrence", "source_chunk": doc.id_}
        #             )
        #             new_relations.append(rel)
        #             count += 1
        
        # if new_relations:
        #     logger.info(f"å‘ç° {count} ä¸ªæ½œåœ¨å¼±å…³è”ï¼Œæ­£åœ¨æ³¨å…¥å›¾è°±...")
        #     try:
        #         # å°è¯•ä½¿ç”¨ upsert æˆ– add
        #         # LlamaIndex çš„ PropertyGraphStore æ¥å£é€šå¸¸æœ‰ upsert_relations
        #         if hasattr(index.property_graph_store, "upsert_relations"):
        #             index.property_graph_store.upsert_relations(new_relations)
        #         elif hasattr(index.property_graph_store, "add"):
        #              index.property_graph_store.add(relations=new_relations)
        #         else:
        #             logger.warning("Graph store does not support batch relation insertion")
        #     except Exception as e:
        #         logger.warning(f"æ³¨å…¥å¼±å…³è”å¤±è´¥: {e}")
    
    def _get_graph_counts(self, graph_store) -> tuple:
        try:
            is_neo4j = "Neo4jPropertyGraphStore" in str(type(graph_store))
            if is_neo4j:
                with graph_store._driver.session() as session:
                    # PropertyGraphIndex ä½¿ç”¨ __Entity__ æ ‡ç­¾
                    e = session.run("MATCH (n:__Entity__) RETURN count(n) as c").single()["c"]
                    r = session.run("MATCH ()-[r]->() RETURN count(r) as c").single()["c"]
                    return int(e or 0), int(r or 0)
            triplets = graph_store.get_triplets()
            entities = set()
            for t in triplets:
                entities.add(t[0].name)
                entities.add(t[2].name)
            return len(entities), len(triplets)
        except Exception:
            return 0, 0
    
    def stream_query_knowledge_graph(self, query: str, index: Any = None, hard_match_nodes: List = None, query_intent: str = None) -> Any:
        """
        æµå¼æŸ¥è¯¢çŸ¥è¯†å›¾è°±ï¼Œè¿”å›ç”Ÿæˆå™¨
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            index: å›¾è°±ç´¢å¼•ï¼ˆå¯é€‰ï¼‰
            hard_match_nodes: ç¡¬åŒ¹é…çš„èŠ‚ç‚¹åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰ï¼Œæ¥è‡ªæŸ¥è¯¢å‰ç½®å¤„ç†
            
        Returns:
            ç”Ÿæˆå™¨ï¼Œä¾æ¬¡ç”Ÿæˆï¼š
            1. LLMå›ç­”çš„æ–‡æœ¬ç‰‡æ®µ (str)
            2. æœ€ç»ˆçš„å›¾è°±è·¯å¾„æ•°æ® (dict)
        """
        try:
            logger.info(f"å¼€å§‹æµå¼æŸ¥è¯¢: {query}")
            
            if index is None:
                if not self.graph_store:
                    yield "é”™è¯¯: å›¾å­˜å‚¨æœªåˆå§‹åŒ–"
                    return
                
                # ç¡®ä¿LLMå’ŒEmbed Modelå·²å°±ç»ª
                if not self.llm or not self.embed_model:
                     if not self.initialize():
                         yield "é”™è¯¯: ç»„ä»¶åˆå§‹åŒ–å¤±è´¥"
                         return
                
                # ç¡®ä¿ modules å·²åˆå§‹åŒ–
                if not self.modules:
                    logger.error("modules æœªåˆå§‹åŒ–")
                    yield "é”™è¯¯: æ¨¡å—æœªåˆå§‹åŒ–"
                    return
                
                # æ£€æŸ¥ modules æ˜¯å¦æ˜¯å­—å…¸ç±»å‹ï¼ˆé˜²æ­¢è¢«é”™è¯¯åœ°èµ‹å€¼ä¸ºå‡½æ•°ï¼‰
                if not isinstance(self.modules, dict):
                    logger.error(f"modules ç±»å‹é”™è¯¯: {type(self.modules)}, æœŸæœ› dict")
                    # å°è¯•é‡æ–°åˆå§‹åŒ–
                    self.modules = LlamaModuleFactory.get_modules()
                    if not isinstance(self.modules, dict):
                        yield f"é”™è¯¯: æ¨¡å—åˆå§‹åŒ–å¤±è´¥ï¼Œç±»å‹: {type(self.modules)}"
                        return
                
                try:
                    index = self.modules['PropertyGraphIndex'].from_existing(
                        property_graph_store=self.graph_store,
                        llm=self.llm,
                        embed_model=self.embed_model
                    )
                except Exception as e:
                    logger.error(f"åŠ è½½ç°æœ‰ç´¢å¼•å¤±è´¥: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
                    yield f"åŠ è½½ç´¢å¼•å¤±è´¥: {str(e)}"
                    return
            
            # åˆ›å»ºæ£€ç´¢å™¨ï¼šä½¿ç”¨çº¯å‘é‡æ£€ç´¢
            # ç¡®ä¿ HYBRID_SEARCH_CONFIG æ˜¯å­—å…¸ç±»å‹
            if not isinstance(HYBRID_SEARCH_CONFIG, dict):
                logger.error(f"HYBRID_SEARCH_CONFIG ç±»å‹é”™è¯¯: {type(HYBRID_SEARCH_CONFIG)}")
                initial_retrieval_k = 50
            else:
                initial_retrieval_k = HYBRID_SEARCH_CONFIG.get("initial_top_k", 50)
            logger.info(f"ä½¿ç”¨çº¯å‘é‡æ£€ç´¢ï¼ŒTop K: {initial_retrieval_k}")
            
            # æ·»åŠ åå¤„ç†å™¨ï¼ˆæŒ‰æ¼æ–—å¼è¿‡æ»¤é¡ºåºï¼‰
            postprocessors = []
            
            # 0. ç¡¬åŒ¹é…èŠ‚ç‚¹åå¤„ç†å™¨ï¼ˆæœ€ä¼˜å…ˆï¼Œæ”¾åœ¨æœ€å‰é¢ï¼‰
            if hard_match_nodes:
                try:
                    from llama.hard_match_postprocessor import HardMatchPostprocessor
                    hard_match_processor = HardMatchPostprocessor(hard_match_nodes)
                    postprocessors.append(hard_match_processor)
                    logger.info(f"æ·»åŠ ç¡¬åŒ¹é…åå¤„ç†å™¨: {len(hard_match_nodes)} ä¸ªèŠ‚ç‚¹")
                except Exception as e:
                    logger.warning(f"ç¡¬åŒ¹é…åå¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            
            # 1. åˆæ­¥é‡æ’åºï¼ˆRerankï¼‰ï¼šä» Top 50 ç­›é€‰åˆ° Top 10ï¼Œå‰”é™¤æ˜æ˜¾ä¸ç›¸å…³çš„èŠ‚ç‚¹
            reranker = RerankerFactory.create_reranker()
            if reranker:
                # åˆ›å»ºé™æµé‡æ’åºå™¨ï¼ˆåªä¿ç•™ Top 10ï¼‰
                try:
                    from llama.limited_rerank_postprocessor import LimitedRerankPostprocessor
                    limited_reranker = LimitedRerankPostprocessor(
                        reranker=reranker,
                        top_n=10  # é‡æ’åºååªä¿ç•™ Top 10
                    )
                    postprocessors.append(limited_reranker)
                    logger.info("æ·»åŠ åˆæ­¥é‡æ’åºåå¤„ç†å™¨ï¼šä» Top 50 ç­›é€‰åˆ° Top 10")
                except ImportError:
                    # é™çº§ï¼šç›´æ¥ä½¿ç”¨é‡æ’åºå™¨
                    logger.warning("LimitedRerankPostprocessor å¯¼å…¥å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨é‡æ’åºå™¨")
                    postprocessors.append(reranker)
            
            # 2-3. å¹¶è¡Œå›¾è°±åå¤„ç†ï¼šè¯­ä¹‰è¡¥å¿ + å›¾è°±ä¸Šä¸‹æ–‡ï¼ˆå¹¶è¡Œæ‰§è¡Œä»¥å‡å°‘å»¶è¿Ÿï¼‰
            try:
                from semantic_enrichment_postprocessor import SemanticEnrichmentPostprocessor
                from graph_context_postprocessor import GraphContextPostprocessor
                from llama.parallel_graph_postprocessor import ParallelGraphPostprocessor
                
                # åˆ›å»ºè¯­ä¹‰è¡¥å¿å’Œå›¾è°±ä¸Šä¸‹æ–‡åå¤„ç†å™¨å®ä¾‹
                semantic_enricher = SemanticEnrichmentPostprocessor(
                    graph_store=self.graph_store,
                    max_neighbors_per_entity=10,
                    query_intent=query_intent  # ä¼ é€’æŸ¥è¯¢æ„å›¾ï¼Œç”¨äºè¿‡æ»¤é‚»å±…å…³ç³»
                )
                
                graph_context = GraphContextPostprocessor(
                    graph_store=self.graph_store,
                    max_path_depth=2,
                    max_paths=10,
                    query_intent=query_intent,  # ä¼ é€’æŸ¥è¯¢æ„å›¾ï¼Œç”¨äºå…ƒè·¯å¾„æœç´¢
                    enable_community_detection=True,  # å¯ç”¨ç¤¾åŒºå‘ç°
                    community_threshold=0.3  # ç¤¾åŒºå¯†åº¦é˜ˆå€¼
                )
                
                # åˆ›å»ºå¹¶è¡Œåå¤„ç†å™¨ï¼ˆä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œï¼‰
                parallel_processor = ParallelGraphPostprocessor(
                    semantic_enricher=semantic_enricher,
                    graph_context=graph_context,
                    max_workers=2  # ä¸¤ä¸ªå¹¶è¡Œä»»åŠ¡
                )
                postprocessors.append(parallel_processor)
                logger.info(f"âœ… æ·»åŠ å¹¶è¡Œå›¾è°±åå¤„ç†å™¨ï¼ˆè¯­ä¹‰è¡¥å¿ + å›¾è°±ä¸Šä¸‹æ–‡å¹¶è¡Œæ‰§è¡Œï¼Œæ„å›¾: {query_intent or 'GENERAL'}ï¼‰")
            except ImportError as e:
                logger.warning(f"å¹¶è¡Œåå¤„ç†å™¨å¯¼å…¥å¤±è´¥ï¼Œé™çº§åˆ°ä¸²è¡Œæ‰§è¡Œ: {e}")
                # é™çº§ï¼šä¸²è¡Œæ‰§è¡Œ
                try:
                    from semantic_enrichment_postprocessor import SemanticEnrichmentPostprocessor
                    semantic_enricher = SemanticEnrichmentPostprocessor(
                        graph_store=self.graph_store,
                        max_neighbors_per_entity=10,
                        query_intent=query_intent
                    )
                    postprocessors.append(semantic_enricher)
                    logger.info(f"æ·»åŠ è¯­ä¹‰è¡¥å¿åå¤„ç†å™¨ï¼ˆä¸²è¡Œæ¨¡å¼ï¼‰")
                except Exception as e2:
                    logger.warning(f"è¯­ä¹‰è¡¥å¿åå¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e2}")
                
                try:
                    from graph_context_postprocessor import GraphContextPostprocessor
                    graph_context = GraphContextPostprocessor(
                        graph_store=self.graph_store,
                        max_path_depth=2,
                        max_paths=10,
                        query_intent=query_intent,
                        enable_community_detection=True,
                        community_threshold=0.3
                    )
                    postprocessors.append(graph_context)
                    logger.info(f"æ·»åŠ å›¾è°±ä¸Šä¸‹æ–‡åå¤„ç†å™¨ï¼ˆä¸²è¡Œæ¨¡å¼ï¼‰")
                except Exception as e3:
                    logger.warning(f"å›¾è°±ä¸Šä¸‹æ–‡åå¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e3}")
            
            # åˆ›å»ºæŸ¥è¯¢å¼•æ“ï¼šä½¿ç”¨çº¯å‘é‡æ£€ç´¢
            engine_kwargs = {
                "include_text": True,
                "similarity_top_k": initial_retrieval_k,
                "streaming": True
            }
            
            if postprocessors:
                engine_kwargs["node_postprocessors"] = postprocessors
            
            query_engine = index.as_query_engine(**engine_kwargs)
            logger.info("ä½¿ç”¨é»˜è®¤æŸ¥è¯¢å¼•æ“ï¼ˆçº¯å‘é‡æ£€ç´¢ï¼‰")
            
            # æ‰§è¡ŒæŸ¥è¯¢ï¼Œè·å–æµå¼å“åº”å¯¹è±¡
            # é˜¶æ®µè¿›åº¦ï¼šæ£€ç´¢å¼€å§‹
            yield {
                "type": "progress",
                "stage": "retrieval",
                "message": "å¼€å§‹æ£€ç´¢ä¸å›¾ä¸Šä¸‹æ–‡å¤„ç†"
            }
            streaming_response = query_engine.query(query)
            # é˜¶æ®µè¿›åº¦ï¼šæ£€ç´¢å®Œæˆ
            yield {
                "type": "progress",
                "stage": "retrieval",
                "message": "æ£€ç´¢å®Œæˆï¼Œå¼€å§‹ç”Ÿæˆå›ç­”"
            }
            
            # ä¼˜å…ˆå°è¯•ä» source_nodes ä¸­æå–è·¯å¾„å¹¶å°½æ—©å‘é€
            try:
                import json as _json
                paths_early = []
                if hasattr(streaming_response, "source_nodes") and streaming_response.source_nodes:
                    for node_with_score in streaming_response.source_nodes:
                        node = getattr(node_with_score, "node", node_with_score)
                        metadata = getattr(node, "metadata", {}) or {}
                        if metadata.get("node_type") == "graph_context":
                            paths_data = metadata.get("paths_data")
                            if paths_data:
                                try:
                                    parsed = _json.loads(paths_data)
                                    if isinstance(parsed, list):
                                        # æå–æ ¼å¼åŒ–åçš„è·¯å¾„å­—ç¬¦ä¸²
                                        paths_early = [p.get("path_str", p) for p in parsed]
                                        break
                                except Exception:
                                    pass
                if paths_early:
                    yield {
                        "type": "graph_paths",
                        "data": paths_early
                    }
            except Exception:
                pass
            
            # 1. å®æ—¶æ¨é€LLMç”Ÿæˆçš„æ–‡æœ¬
            full_answer = ""
            for token in streaming_response.response_gen:
                full_answer += token
                yield token
            
            # 2. ä» GraphContext æ³¨å…¥çš„ source_nodes ä¸­æå–è·¯å¾„å’ŒåŸå§‹æ–‡æœ¬ä¸Šä¸‹æ–‡
            paths = []
            contexts = []
            try:
                import json as _json
                if hasattr(streaming_response, "source_nodes") and streaming_response.source_nodes:
                    for node_with_score in streaming_response.source_nodes:
                        node = getattr(node_with_score, "node", node_with_score)
                        metadata = getattr(node, "metadata", {}) or {}
                        
                        # åˆ†ç¦»å›¾è·¯å¾„æ•°æ®å’Œæ™®é€šæ–‡æœ¬å—å†…å®¹
                        if metadata.get("node_type") == "graph_context":
                            paths_data = metadata.get("paths_data")
                            if paths_data:
                                try:
                                    parsed = _json.loads(paths_data)
                                    if isinstance(parsed, list):
                                        # æå–æ ¼å¼åŒ–åçš„è·¯å¾„å­—ç¬¦ä¸²
                                        paths.extend([p.get("path_str", p) for p in parsed])
                                except Exception:
                                    pass
                        else:
                            # æå–åŸå§‹æ–‡æœ¬å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡
                            content = node.get_content()
                            if content and content not in contexts:
                                contexts.append(content)
            except Exception as e:
                logger.warning(f"æå–ä¸Šä¸‹æ–‡æˆ–è·¯å¾„å¤±è´¥: {e}")
            
            # 3. æ¨é€è·¯å¾„æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
            if paths:
                yield {
                    "type": "graph_paths",
                    "data": paths,
                    "full_answer": full_answer
                }
            
            # 4. æ¨é€æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼ˆç”¨äºè¯„ä¼°ç­‰åœºæ™¯ï¼‰
            if contexts:
                yield {
                    "type": "retrieved_contexts",
                    "data": contexts
                }
            
            # 5. å®Œæˆäº‹ä»¶
            yield {
                "type": "done",
                "full_answer": full_answer,
                "contexts": contexts
            }
            
        except Exception as e:
            logger.error(f"æµå¼æŸ¥è¯¢å¤±è´¥: {e}")
            yield f"æŸ¥è¯¢å‡ºé”™: {str(e)}"

    def query_knowledge_graph(self, query: str, index: Any = None, return_paths: bool = True) -> Dict[str, Any]:
        """
        æŸ¥è¯¢çŸ¥è¯†å›¾è°±ï¼Œè¿”å›ç­”æ¡ˆå’Œå›¾è°±æ¨ç†è·¯å¾„
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            index: å›¾è°±ç´¢å¼•ï¼ˆå¯é€‰ï¼‰
            return_paths: æ˜¯å¦è¿”å›å›¾è°±è·¯å¾„
            
        Returns:
            åŒ…å«ç­”æ¡ˆå’Œå›¾è°±è·¯å¾„çš„å­—å…¸
        """
        try:
            logger.info(f"æŸ¥è¯¢çŸ¥è¯†å›¾è°±: {query}")
            
            if index is None:
                if not self.graph_store:
                    return {
                        "answer": "é”™è¯¯: å›¾å­˜å‚¨æœªåˆå§‹åŒ–",
                        "paths": []
                    }
                
                # ç¡®ä¿LLMå’ŒEmbed Modelå·²å°±ç»ª
                if not self.llm or not self.embed_model:
                     if not self.initialize():
                         return {
                             "answer": "é”™è¯¯: ç»„ä»¶åˆå§‹åŒ–å¤±è´¥",
                             "paths": []
                         }
                
                # ç¡®ä¿ modules å·²åˆå§‹åŒ–ä¸”ç±»å‹æ­£ç¡®
                if not self.modules or not isinstance(self.modules, dict):
                    logger.warning(f"modules ç±»å‹å¼‚å¸¸: {type(self.modules)}ï¼Œå°è¯•é‡æ–°è·å–")
                    self.modules = LlamaModuleFactory.get_modules()
                    if not isinstance(self.modules, dict):
                         return {
                             "answer": "é”™è¯¯: æ¨¡å—åˆå§‹åŒ–å¤±è´¥",
                             "paths": [],
                             "contexts": []
                         }
                
                try:
                    index = self.modules['PropertyGraphIndex'].from_existing(
                        property_graph_store=self.graph_store,
                        llm=self.llm,
                        embed_model=self.embed_model
                    )
                except Exception as e:
                    logger.error(f"åŠ è½½ç°æœ‰ç´¢å¼•å¤±è´¥: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
                    return {
                        "answer": f"åŠ è½½ç´¢å¼•å¤±è´¥: {str(e)}",
                        "paths": [],
                        "contexts": []
                    }
            
            query_engine = index.as_query_engine(
                include_text=True,
                similarity_top_k=5
            )
            
            # æ·»åŠ åå¤„ç†å™¨åˆ—è¡¨
            postprocessors = []
            initial_k = 5  # é»˜è®¤å€¼
            
            # æ·»åŠ è¯­ä¹‰è¡¥å¿åå¤„ç†å™¨ï¼ˆä¸€åº¦å…³è”èŠ‚ç‚¹æ‹‰å–ï¼‰
            try:
                from semantic_enrichment_postprocessor import SemanticEnrichmentPostprocessor
                semantic_enricher = SemanticEnrichmentPostprocessor(
                    graph_store=self.graph_store,
                    max_neighbors_per_entity=10
                )
                postprocessors.append(semantic_enricher)
                logger.info("âœ… å¯ç”¨è¯­ä¹‰è¡¥å¿åå¤„ç†å™¨ï¼ˆä¸€åº¦å…³è”èŠ‚ç‚¹æ‹‰å–ï¼‰")
            except Exception as e:
                logger.warning(f"è¯­ä¹‰è¡¥å¿åå¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            
            # æ·»åŠ é‡æ’åºé€»è¾‘
            reranker = RerankerFactory.create_reranker()
            if reranker:
                initial_k = RERANK_CONFIG.get('initial_top_k', 10)
                logger.info(f"å¯ç”¨é‡æ’åº: initial_k={initial_k}, model={RERANK_CONFIG.get('model')}")
                postprocessors.append(reranker)
            
            # æ·»åŠ å›¾è°±ä¸Šä¸‹æ–‡åå¤„ç†å™¨ï¼ˆåœ¨Top-Kå®ä½“é—´å»ºç«‹æœ€çŸ­è·¯å¾„è¿æ¥ï¼Œå¹¶è½¬ä¸ºè‡ªç„¶è¯­è¨€æ³¨å…¥Promptï¼‰
            try:
                from graph_context_postprocessor import GraphContextPostprocessor
                graph_context = GraphContextPostprocessor(
                    graph_store=self.graph_store,
                    max_path_depth=2,
                    max_paths=10
                )
                postprocessors.append(graph_context)
                logger.info("âœ… å¯ç”¨å›¾è°±ä¸Šä¸‹æ–‡åå¤„ç†å™¨ï¼ˆæœ€çŸ­è·¯å¾„è¿æ¥Top-Kå®ä½“ï¼‰")
            except Exception as e:
                logger.warning(f"å›¾è°±ä¸Šä¸‹æ–‡åå¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                
            # å¦‚æœæœ‰åå¤„ç†å™¨ï¼Œåº”ç”¨åˆ°æŸ¥è¯¢å¼•æ“
            if postprocessors:
                query_engine = index.as_query_engine(
                    include_text=True,
                    similarity_top_k=initial_k,
                    node_postprocessors=postprocessors
                )
            
            # æ‰§è¡ŒæŸ¥è¯¢
            response = query_engine.query(query)
            answer = str(response)
            
            # æå–è·¯å¾„å’Œä¸Šä¸‹æ–‡
            paths = []
            contexts = []
            try:
                import json as _json
                if hasattr(response, "source_nodes") and response.source_nodes:
                    for node_with_score in response.source_nodes:
                        node = getattr(node_with_score, "node", node_with_score)
                        metadata = getattr(node, "metadata", {}) or {}
                        
                        if metadata.get("node_type") == "graph_context":
                            paths_data = metadata.get("paths_data")
                            if paths_data:
                                try:
                                    parsed = _json.loads(paths_data)
                                    if isinstance(parsed, list):
                                        paths.extend([p.get("path_str", p) for p in parsed])
                                except Exception:
                                    pass
                        else:
                            content = node.get_content()
                            if content and content not in contexts:
                                contexts.append(content)
            except Exception as e:
                logger.warning(f"æå–ä¸Šä¸‹æ–‡æˆ–è·¯å¾„å¤±è´¥: {e}")
            
            return {
                "answer": answer,
                "paths": paths,
                "contexts": contexts
            }
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "answer": f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
                "paths": [],
                "contexts": []
            }
    
    def generate_embeddings_for_nodes(self, node_ids: List[str] = None, node_names: List[str] = None) -> Dict[str, Any]:
        """
        ä¸ºæŒ‡å®šèŠ‚ç‚¹ç”Ÿæˆ embedding å‘é‡
        
        Args:
            node_ids: èŠ‚ç‚¹IDåˆ—è¡¨ï¼ˆNeo4j elementIdï¼‰
            node_names: èŠ‚ç‚¹åç§°åˆ—è¡¨
            
        Returns:
            åŒ…å«æˆåŠŸå’Œå¤±è´¥ä¿¡æ¯çš„å­—å…¸
        """
        if not self.graph_store or not self.embed_model:
            return {
                "success": False,
                "message": "å›¾å­˜å‚¨æˆ–åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–",
                "processed": 0,
                "failed": 0
            }
        
        is_neo4j = "Neo4jPropertyGraphStore" in str(type(self.graph_store))
        if not is_neo4j:
            return {
                "success": False,
                "message": "å½“å‰å›¾å­˜å‚¨ä¸æ˜¯ Neo4jï¼Œæ— æ³•ç”Ÿæˆ embedding",
                "processed": 0,
                "failed": 0
            }
        
        try:
            processed_count = 0
            failed_count = 0
            failed_nodes = []
            
            with self.graph_store._driver.session() as session:
                # æŸ¥è¯¢éœ€è¦ç”Ÿæˆ embedding çš„èŠ‚ç‚¹
                # æ¡ä»¶ï¼šæ²¡æœ‰ embedding æˆ– source ä¸º manual/æ‰‹å·¥å½•å…¥ï¼ˆæ‰‹åŠ¨æ–°å¢çš„èŠ‚ç‚¹ï¼‰
                # å¦‚æœæ˜ç¡®æŒ‡å®šäº† node_idsï¼Œåˆ™åªæ£€æŸ¥æ˜¯å¦æœ‰ embeddingï¼Œä¸é™åˆ¶ source
                if node_ids:
                    # æ ¹æ®èŠ‚ç‚¹IDæŸ¥è¯¢ï¼ˆæ˜ç¡®æŒ‡å®šIDæ—¶ï¼Œåªæ£€æŸ¥æ˜¯å¦ç¼ºå°‘embeddingï¼‰
                    query = """
                    MATCH (n:__Entity__)
                    WHERE elementId(n) IN $node_ids
                    AND n.embedding IS NULL
                    AND (n.source IS NULL OR n.source IN ['manual', 'æ‰‹å·¥å½•å…¥'])
                    RETURN elementId(n) as id, n.name as name, COALESCE(n.label, n.type, '__Entity__') as label
                    """
                    result = session.run(query, node_ids=node_ids)
                elif node_names:
                    # æ ¹æ®èŠ‚ç‚¹åç§°æŸ¥è¯¢ï¼ˆåªæ£€æŸ¥æ˜¯å¦ç¼ºå°‘embeddingï¼‰
                    query = """
                    MATCH (n:__Entity__)
                    WHERE n.name IN $node_names
                    AND n.embedding IS NULL
                    AND (n.source IS NULL OR n.source IN ['manual', 'æ‰‹å·¥å½•å…¥'])
                    RETURN elementId(n) as id, n.name as name, COALESCE(n.label, n.type, '__Entity__') as label
                    """
                    result = session.run(query, node_names=node_names)
                else:
                    # æŸ¥è¯¢æ‰€æœ‰æ²¡æœ‰ embedding çš„ manual/æ‰‹å·¥å½•å…¥èŠ‚ç‚¹
                    query = """
                    MATCH (n:__Entity__)
                    WHERE n.embedding IS NULL 
                    AND (n.source IS NULL OR n.source IN ['manual', 'æ‰‹å·¥å½•å…¥'])
                    RETURN elementId(n) as id, n.name as name, COALESCE(n.label, n.type, '__Entity__') as label
                    LIMIT 100
                    """
                    result = session.run(query)
                
                nodes_to_process = []
                for record in result:
                    nodes_to_process.append({
                        "id": record["id"],
                        "name": record["name"],
                        "label": record["label"]  # COALESCE åçš„ labelï¼Œç”¨äºç”Ÿæˆ embedding æ–‡æœ¬
                    })
                
                if not nodes_to_process:
                    return {
                        "success": True,
                        "message": "æ²¡æœ‰éœ€è¦ç”Ÿæˆ embedding çš„èŠ‚ç‚¹",
                        "processed": 0,
                        "failed": 0
                    }
                
                logger.info(f"å‡†å¤‡ä¸º {len(nodes_to_process)} ä¸ªèŠ‚ç‚¹ç”Ÿæˆ embedding")
                
                # æ‰¹é‡ç”Ÿæˆ embedding
                for node_info in nodes_to_process:
                    try:
                        # æ„å»ºç”¨äºç”Ÿæˆ embedding çš„æ–‡æœ¬
                        # æ ¼å¼ï¼šèŠ‚ç‚¹åç§° + èŠ‚ç‚¹ç±»å‹ï¼ˆå¦‚æœæœ‰ä¸”ä¸æ˜¯é»˜è®¤çš„Entityï¼‰
                        embed_text = node_info["name"]
                        if node_info["label"] and node_info["label"] != "Entity":
                            embed_text = f"{node_info['name']} {node_info['label']}"
                        
                        # ç”Ÿæˆ embedding
                        logger.info(f"æ­£åœ¨ä¸ºèŠ‚ç‚¹ '{node_info['name']}' (ID: {node_info['id']}) ç”Ÿæˆ embeddingï¼Œæ–‡æœ¬: {embed_text}")
                        embedding = self.embed_model.get_text_embedding(embed_text)
                        
                        if not embedding or len(embedding) == 0:
                            raise ValueError(f"ç”Ÿæˆçš„ embedding ä¸ºç©º")
                        
                        logger.info(f"ç”Ÿæˆçš„ embedding ç»´åº¦: {len(embedding)}")
                        
                        # æ›´æ–°èŠ‚ç‚¹å±æ€§å’Œæ ‡ç­¾ï¼ˆlabelsï¼‰
                        # ä¸ºæ‰‹åŠ¨æ–°å¢çš„èŠ‚ç‚¹æ·»åŠ  'manual' æ ‡ç­¾ï¼ˆNeo4j çš„ labelsï¼Œä¸æ˜¯å±æ€§ï¼‰
                        # åŒæ—¶ç¡®ä¿èŠ‚ç‚¹çš„ label å±æ€§å­˜åœ¨ï¼ˆå¦‚æœæ²¡æœ‰åˆ™è®¾ç½®ä¸º '__Entity__'ï¼‰
                        update_query = """
                        MATCH (n:__Entity__)
                        WHERE elementId(n) = $node_id
                        SET n.embedding = $embedding,
                            n.updated_at = timestamp(),
                            n:manual,
                            n.label = CASE 
                                WHEN n.label IS NULL THEN '__Entity__'
                                ELSE n.label
                            END
                        RETURN n.name as name, 
                               n.embedding IS NOT NULL as has_embedding,
                               labels(n) as labels,
                               n.label as label
                        """
                        result = session.run(update_query, node_id=node_info["id"], embedding=embedding)
                        
                        # éªŒè¯æ˜¯å¦æˆåŠŸæ›´æ–°
                        record = result.single()
                        if not record:
                            raise ValueError(f"èŠ‚ç‚¹ {node_info['id']} ä¸å­˜åœ¨æˆ–æ›´æ–°å¤±è´¥")
                        
                        # éªŒè¯ embeddingã€labels å’Œ label æ˜¯å¦çœŸçš„å†™å…¥
                        verify_query = """
                        MATCH (n)
                        WHERE elementId(n) = $node_id
                        RETURN n.embedding IS NOT NULL as has_embedding, 
                               size(n.embedding) as embedding_size,
                               labels(n) as labels,
                               n.label as label
                        """
                        verify_result = session.run(verify_query, node_id=node_info["id"])
                        verify_record = verify_result.single()
                        
                        if verify_record and verify_record["has_embedding"]:
                            labels_info = f"ï¼Œlabels: {verify_record.get('labels', [])}"
                            label_info = f"ï¼Œlabelå±æ€§: {verify_record.get('label', 'N/A')}"
                            logger.info(
                                f"âœ… å·²ä¸ºèŠ‚ç‚¹ '{node_info['name']}' ç”Ÿæˆå¹¶å†™å…¥ embedding "
                                f"(ç»´åº¦: {verify_record.get('embedding_size', 'N/A')}{labels_info}{label_info})"
                            )
                            processed_count += 1
                        else:
                            raise ValueError(f"èŠ‚ç‚¹æ›´æ–°æˆåŠŸä½†éªŒè¯æ—¶æœªæ‰¾åˆ° embedding å±æ€§")
                        
                        
                    except Exception as e:
                        failed_count += 1
                        failed_nodes.append({
                            "name": node_info.get("name", "Unknown"),
                            "error": str(e)
                        })
                        logger.warning(f"âŒ ä¸ºèŠ‚ç‚¹ '{node_info.get('name')}' ç”Ÿæˆ embedding å¤±è´¥: {e}")
                
                message = f"æˆåŠŸä¸º {processed_count} ä¸ªèŠ‚ç‚¹ç”Ÿæˆ embedding"
                if failed_count > 0:
                    message += f"ï¼Œ{failed_count} ä¸ªèŠ‚ç‚¹å¤±è´¥"
                
                return {
                    "success": True,
                    "message": message,
                    "processed": processed_count,
                    "failed": failed_count,
                    "failed_nodes": failed_nodes if failed_nodes else None
                }
                
        except Exception as e:
            logger.error(f"ç”ŸæˆèŠ‚ç‚¹ embedding æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {
                "success": False,
                "message": f"ç”Ÿæˆ embedding å¤±è´¥: {str(e)}",
                "processed": processed_count,
                "failed": failed_count
            }
    
    def _generate_entity_descriptions(self, index: Any, progress_tracker: Optional[ProgressTracker] = None):
        """ä¸ºæ‰€æœ‰å®ä½“èŠ‚ç‚¹ç”Ÿæˆå¹¶æ›´æ–° description å­—æ®µï¼ˆå¤šçº¿ç¨‹ç‰ˆæœ¬ï¼‰"""
        try:
            if not self.graph_store or not self.llm:
                logger.warning("å›¾å­˜å‚¨æˆ–LLMæœªåˆå§‹åŒ–ï¼Œè·³è¿‡å®ä½“æè¿°ç”Ÿæˆ")
                return
            
            is_neo4j = "Neo4jPropertyGraphStore" in str(type(self.graph_store))
            if not is_neo4j:
                logger.warning("å½“å‰å›¾å­˜å‚¨ä¸æ˜¯ Neo4jï¼Œè·³è¿‡å®ä½“æè¿°ç”Ÿæˆ")
                return
            
            description_prompt_template = ENTITY_DESCRIPTION_CONFIG.get("description_prompt", "")
            num_workers = ENTITY_DESCRIPTION_CONFIG.get("num_workers", 2)
            request_delay = ENTITY_DESCRIPTION_CONFIG.get("request_delay", 0.3)
            max_retries = ENTITY_DESCRIPTION_CONFIG.get("max_retries", 3)
            retry_delay = ENTITY_DESCRIPTION_CONFIG.get("retry_delay", 5.0)
            
            if progress_tracker:
                progress_tracker.update_stage("knowledge_graph", "æ­£åœ¨ä¸ºæ‰€æœ‰å®ä½“ç”Ÿæˆæè¿°...", 98)
            else:
                progress_callback("knowledge_graph", "æ­£åœ¨ä¸ºæ‰€æœ‰å®ä½“ç”Ÿæˆæè¿°...", 98)
            
            logger.info("å¼€å§‹ä¸ºæ‰€æœ‰å®ä½“èŠ‚ç‚¹ç”Ÿæˆæè¿°...")
            
            # æŸ¥è¯¢æ‰€æœ‰éœ€è¦å¤„ç†çš„å®ä½“
            entities_to_process = []
            with self.graph_store._driver.session() as session:
                query = """
                MATCH (n:__Entity__)
                WHERE n.description IS NULL OR n.description = ''
                RETURN DISTINCT n.name as name, COALESCE(n.type, n.label, 'Entity') as entity_type
                LIMIT 100
                """
                result = session.run(query)
                
                for record in result:
                    entities_to_process.append({
                        "name": record["name"],
                        "type": record["entity_type"]
                    })
            
            if not entities_to_process:
                logger.info("æ²¡æœ‰éœ€è¦ç”Ÿæˆæè¿°çš„å®ä½“èŠ‚ç‚¹")
                return
            
            logger.info(f"å‡†å¤‡ä¸º {len(entities_to_process)} ä¸ªå®ä½“èŠ‚ç‚¹ç”Ÿæˆæè¿°ï¼ˆä½¿ç”¨ {num_workers} ä¸ªworkerï¼‰")
            
            # çº¿ç¨‹å®‰å…¨çš„è®¡æ•°å™¨
            processed_count = 0
            failed_count = 0
            count_lock = threading.Lock()
            
            def generate_single_description(entity_info: Dict[str, str]) -> Tuple[bool, str]:
                """ä¸ºå•ä¸ªå®ä½“ç”Ÿæˆæè¿°ï¼ˆçº¿ç¨‹å‡½æ•°ï¼Œå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
                nonlocal processed_count, failed_count
                
                entity_name = entity_info["name"]
                entity_type = entity_info["type"]
                
                # æ„å»º prompt
                prompt = description_prompt_template.format(
                    entity_name=entity_name,
                    entity_type=entity_type
                )
                
                # å¸¦é‡è¯•æœºåˆ¶çš„APIè°ƒç”¨
                last_exception = None
                for attempt in range(max_retries + 1):
                    try:
                        # è¯·æ±‚å‰å»¶è¿Ÿï¼ˆé™æµæ§åˆ¶ï¼‰
                        if attempt > 0:
                            # é‡è¯•æ—¶ä½¿ç”¨æŒ‡æ•°é€€é¿
                            wait_time = retry_delay * (2 ** (attempt - 1))
                            error_str = str(last_exception) if last_exception else ""
                            # 429é”™è¯¯éœ€è¦æ›´é•¿çš„ç­‰å¾…æ—¶é—´
                            if "429" in error_str or "Too Many Requests" in error_str or "RateLimitError" in error_str:
                                wait_time = wait_time * 2  # 429é”™è¯¯åŠ å€ç­‰å¾…
                            logger.warning(f"å®ä½“æè¿°ç”Ÿæˆé‡è¯• ({entity_name}): ç­‰å¾… {wait_time:.2f} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries + 1} æ¬¡)")
                            time.sleep(wait_time)
                        else:
                            # é¦–æ¬¡è¯·æ±‚å‰çŸ­æš‚å»¶è¿Ÿ
                            time.sleep(request_delay)
                        
                        # è°ƒç”¨ LLM ç”Ÿæˆæè¿°
                        logger.debug(f"æ­£åœ¨ä¸ºå®ä½“ '{entity_name}' ({entity_type}) ç”Ÿæˆæè¿°...")
                        response = self.llm.complete(prompt)
                        description = response.text.strip()
                        
                        # æ¸…ç†æè¿°ï¼ˆç§»é™¤å¯èƒ½çš„å¼•å·ã€å¤šä½™ç©ºç™½ç­‰ï¼‰
                        description = description.strip('"\'').strip()
                        if len(description) > 200:
                            description = description[:200] + "..."
                        
                        if not description:
                            logger.warning(f"å®ä½“ '{entity_name}' çš„æè¿°ç”Ÿæˆä¸ºç©º")
                            with count_lock:
                                failed_count += 1
                            return False, entity_name
                        
                        # æ›´æ–°èŠ‚ç‚¹ description å±æ€§ï¼ˆæ¯ä¸ªçº¿ç¨‹ä½¿ç”¨è‡ªå·±çš„ sessionï¼‰
                        with self.graph_store._driver.session() as session:
                            update_query = """
                            MATCH (n:__Entity__ {name: $entity_name})
                            SET n.description = $description,
                                n.updated_at = timestamp()
                            RETURN n.name as name
                            """
                            session.run(update_query, 
                                      entity_name=entity_name,
                                      description=description)
                        
                        logger.info(f"âœ… å·²ä¸ºå®ä½“ '{entity_name}' ç”Ÿæˆæè¿°: {description[:50]}...")
                        with count_lock:
                            processed_count += 1
                        return True, entity_name
                        
                    except Exception as e:
                        last_exception = e
                        error_str = str(e)
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯429é™æµé”™è¯¯
                        if "429" in error_str or "Too Many Requests" in error_str or "RateLimitError" in error_str:
                            if attempt < max_retries:
                                # 429é”™è¯¯éœ€è¦æ›´é•¿çš„ç­‰å¾…æ—¶é—´ï¼Œç»§ç»­é‡è¯•
                                continue
                            else:
                                logger.error(f"âŒ ä¸ºå®ä½“ '{entity_name}' ç”Ÿæˆæè¿°å¤±è´¥ï¼ˆ429é™æµï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰: {e}")
                                with count_lock:
                                    failed_count += 1
                                return False, entity_name
                        else:
                            # å…¶ä»–é”™è¯¯ï¼Œå¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œè®°å½•å¹¶è¿”å›å¤±è´¥
                            if attempt >= max_retries:
                                logger.warning(f"âŒ ä¸ºå®ä½“ '{entity_name}' ç”Ÿæˆæè¿°å¤±è´¥ï¼ˆå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰: {e}")
                                with count_lock:
                                    failed_count += 1
                                return False, entity_name
                            # å…¶ä»–é”™è¯¯ä¹Ÿç»§ç»­é‡è¯•
                            continue
                
                # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
                logger.error(f"âŒ ä¸ºå®ä½“ '{entity_name}' ç”Ÿæˆæè¿°å¤±è´¥ï¼ˆæ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼‰: {last_exception}")
                with count_lock:
                    failed_count += 1
                return False, entity_name
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
            with ThreadPoolExecutor(max_workers=num_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                futures = {
                    executor.submit(generate_single_description, entity_info): entity_info
                    for entity_info in entities_to_process
                }
                
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                for future in as_completed(futures):
                    entity_info = futures[future]
                    try:
                        success, entity_name = future.result()
                    except Exception as exc:
                        logger.error(f"å®ä½“ '{entity_info.get('name')}' å¤„ç†æ—¶å‘ç”Ÿå¼‚å¸¸: {exc}")
                        with count_lock:
                            failed_count += 1
            
            logger.info(f"âœ… å®ä½“æè¿°ç”Ÿæˆå®Œæˆ: æˆåŠŸ {processed_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª")
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆå®ä½“æè¿°æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    def _create_document_chunk_skeleton(self, documents: List[Any]):
        """
        ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºéª¨æ¶ (Document + Chunk)
        ä¸‰å±‚æ‹“æ‰‘æ¶æ„çš„ç‰©ç†å±‚å’Œä¸Šä¸‹æ–‡å±‚
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼ˆå·²ç»æ˜¯åˆ†å—åçš„æ–‡æ¡£ï¼‰
        """
        try:
            is_neo4j = "Neo4jPropertyGraphStore" in str(type(self.graph_store))
            if not is_neo4j:
                logger.warning("å½“å‰å›¾å­˜å‚¨ä¸æ˜¯ Neo4jï¼Œè·³è¿‡éª¨æ¶åˆ›å»º")
                return
            
            logger.info("å¼€å§‹åˆ›å»ºæ–‡æ¡£å’Œå—éª¨æ¶ (Document + Chunk)...")
            
            with self.graph_store._driver.session() as session:
                # 1. æŒ‰æ–‡æ¡£åˆ†ç»„ï¼ˆæ ¹æ® file_path æˆ– source_file_nameï¼‰
                # ä¼˜åŒ–ï¼šä½¿ç”¨ç”Ÿæˆå™¨å‡å°‘å†…å­˜å ç”¨
                doc_groups = {}
                doc_chunks = {}  # doc_id -> list of (chunk_id, chunk_index, chunk_data)
                chunk_to_doc = {}  # chunk_id -> document_id
                
                for idx, doc in enumerate(documents):
                    # è·å–æ–‡æ¡£çš„å”¯ä¸€æ ‡è¯†ï¼ˆä¼˜å…ˆä½¿ç”¨ file_pathï¼‰
                    doc_path = doc.metadata.get('file_path') or doc.metadata.get('file_name')
                    if not doc_path:
                        doc_path = getattr(doc, 'id_', str(id(doc)))
                    
                    # è®¡ç®—æ–‡ä»¶å“ˆå¸Œä½œä¸ºå”¯ä¸€æ ‡è¯†
                    try:
                        if os.path.exists(doc_path):
                            file_hash = get_file_hash(doc_path)
                        else:
                            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨è·¯å¾„çš„å“ˆå¸Œ
                            file_hash = str(hash(doc_path) % 1000000000)
                    except Exception:
                        file_hash = str(hash(doc_path) % 1000000000)
                    
                    if file_hash not in doc_groups:
                        doc_groups[file_hash] = {
                            'file_hash': file_hash,
                            'file_path': doc_path,
                            'metadata': doc.metadata
                        }
                        doc_chunks[file_hash] = []
                    
                    # è®°å½• chunk ä¿¡æ¯
                    chunk_id = getattr(doc, 'id_', str(id(doc)))
                    chunk_text = getattr(doc, 'text', '')
                    # chunk_index å°†åœ¨åé¢æŒ‰æ–‡æ¡£åˆ†ç»„åé‡æ–°è®¡ç®—
                    page_number = doc.metadata.get('page_label') or doc.metadata.get('page_number') or 0
                    
                    doc_chunks[file_hash].append({
                        'chunk_id': chunk_id,
                        'text': chunk_text,
                        'page_number': page_number
                    })
                    chunk_to_doc[chunk_id] = file_hash
                    
                    # å®šæœŸæ¸…ç†å†…å­˜
                    if idx % 1000 == 0:
                        import gc
                        gc.collect()
                
                # ä¸ºæ¯ä¸ªæ–‡æ¡£çš„ chunks åˆ†é…æ­£ç¡®çš„ chunk_index
                for file_hash, chunks in doc_chunks.items():
                    for chunk_idx, chunk_info in enumerate(chunks):
                        chunk_info['chunk_index'] = chunk_idx
                
                # 2. åˆ›å»º Document èŠ‚ç‚¹ï¼ˆç‰©ç†å±‚ï¼‰
                created_docs = 0
                for file_hash, doc_info in doc_groups.items():
                    doc_metadata = doc_info['metadata']
                    file_path = doc_info['file_path']
                    
                    # ä» metadata è·å–æ–‡æ¡£ä¿¡æ¯
                    file_name = doc_metadata.get('file_name') or os.path.basename(file_path)
                    upload_date = doc_metadata.get('created_at', int(time.time()))
                    url = doc_metadata.get('file_url', '')
                    
                    query = """
                    MERGE (d:Document {file_hash: $file_hash})
                    ON CREATE SET d.file_name = $file_name,
                                  d.upload_date = $upload_date,
                                  d.url = $url,
                                  d.created_at = timestamp()
                    ON MATCH SET d.updated_at = timestamp()
                    RETURN d.file_hash as file_hash
                    """
                    
                    try:
                        result = session.run(query, 
                                            file_hash=file_hash,
                                            file_name=file_name,
                                            upload_date=upload_date,
                                            url=url)
                        result.single()
                        created_docs += 1
                    except Exception as e:
                        logger.warning(f"åˆ›å»º Document èŠ‚ç‚¹å¤±è´¥ ({file_hash}): {e}")
                
                logger.info(f"âœ… åˆ›å»ºäº† {created_docs} ä¸ª Document èŠ‚ç‚¹")
                
                # æ¸…ç†ä¸å†éœ€è¦çš„ doc_groups
                del doc_groups
                import gc
                gc.collect()
                
                # 3. åˆ›å»º Chunk èŠ‚ç‚¹å¹¶å»ºç«‹ Chunk-[:PART_OF]->Document å’Œ Chunk-[:NEXT]->Chunk å…³ç³»ï¼ˆä¸Šä¸‹æ–‡å±‚ï¼‰
                created_chunks = 0
                created_part_of_rels = 0
                created_next_rels = 0
                
                for file_hash, chunks in doc_chunks.items():
                    # æŒ‰ chunk_index æ’åº
                    chunks.sort(key=lambda x: x['chunk_index'])
                    
                    prev_chunk_id = None
                    for chunk_info in chunks:
                        chunk_id = chunk_info['chunk_id']
                        chunk_text = chunk_info['text']
                        chunk_index = chunk_info['chunk_index']
                        page_number = chunk_info['page_number']
                        
                        # åˆ›å»º Chunk èŠ‚ç‚¹å¹¶å»ºç«‹ PART_OF å…³ç³»
                        query = """
                        MATCH (doc:Document {file_hash: $file_hash})
                        MERGE (c:Chunk {id: $chunk_id})
                        ON CREATE SET c.text = $text,
                                      c.chunk_index = $chunk_index,
                                      c.page_number = $page_number,
                                      c.created_at = timestamp()
                        MERGE (c)-[:PART_OF]->(doc)
                        RETURN c.id as id
                        """
                        
                        try:
                            result = session.run(query,
                                                file_hash=file_hash,
                                                chunk_id=chunk_id,
                                                text=chunk_text,
                                                chunk_index=chunk_index,
                                                page_number=page_number)
                            result.single()
                            created_chunks += 1
                            created_part_of_rels += 1
                            
                            # å»ºç«‹ NEXT å…³ç³»ï¼ˆé“¾å¼ç»“æ„ï¼‰
                            if prev_chunk_id:
                                next_query = """
                                MATCH (c_prev:Chunk {id: $prev_chunk_id})
                                MATCH (c:Chunk {id: $chunk_id})
                                MERGE (c_prev)-[:NEXT]->(c)
                                RETURN count(*) as count
                                """
                                try:
                                    session.run(next_query,
                                               prev_chunk_id=prev_chunk_id,
                                               chunk_id=chunk_id)
                                    created_next_rels += 1
                                except Exception as e:
                                    logger.debug(f"åˆ›å»º NEXT å…³ç³»å¤±è´¥ ({prev_chunk_id} -> {chunk_id}): {e}")
                            
                            prev_chunk_id = chunk_id
                        except Exception as e:
                            logger.warning(f"åˆ›å»º Chunk èŠ‚ç‚¹å¤±è´¥ ({chunk_id}): {e}")
                
                logger.info(f"âœ… åˆ›å»ºäº† {created_chunks} ä¸ª Chunk èŠ‚ç‚¹ï¼Œ{created_part_of_rels} ä¸ª PART_OF å…³ç³»ï¼Œ{created_next_rels} ä¸ª NEXT å…³ç³»")
                logger.info(f"âœ… éª¨æ¶åˆ›å»ºå®Œæˆ: {created_docs} ä¸ª Document, {created_chunks} ä¸ª Chunk")
                
                # æ¸…ç†ä¸å†éœ€è¦çš„å­—å…¸
                del doc_chunks
                del chunk_to_doc
                gc.collect()
                
        except Exception as e:
            logger.error(f"åˆ›å»ºæ–‡æ¡£å’Œå—éª¨æ¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    def _create_provenance_structure(self, documents: List[Any], index: Any, progress_tracker: Optional[ProgressTracker] = None):
        """
        ç¬¬äºŒæ­¥ï¼šåˆ›å»ºæº¯æºå…³ç³» (Chunk)-[:MENTIONS]->(Entity)
        ä¸‰å±‚æ‹“æ‰‘æ¶æ„çš„è¯­ä¹‰å±‚
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼ˆå·²ç»æ˜¯åˆ†å—åçš„æ–‡æ¡£ï¼‰
            index: PropertyGraphIndex å®ä¾‹
            progress_tracker: è¿›åº¦è·Ÿè¸ªå™¨
        """
        try:
            is_neo4j = "Neo4jPropertyGraphStore" in str(type(self.graph_store))
            if not is_neo4j:
                logger.warning("å½“å‰å›¾å­˜å‚¨ä¸æ˜¯ Neo4jï¼Œè·³è¿‡æº¯æºå…³ç³»åˆ›å»º")
                return
            
            if progress_tracker:
                progress_tracker.update_stage("knowledge_graph", "æ­£åœ¨åˆ›å»ºæº¯æºå…³ç³»...", 99)
            else:
                progress_callback("knowledge_graph", "æ­£åœ¨åˆ›å»ºæº¯æºå…³ç³»...", 99)
            
            logger.info("å¼€å§‹åˆ›å»ºæº¯æºå…³ç³» (Chunk)-[:MENTIONS]->(Entity)...")
            
            # å»ºç«‹ chunk_id åˆ° chunk_text çš„æ˜ å°„
            chunk_text_map = {}
            for doc in documents:
                chunk_id = getattr(doc, 'id_', str(id(doc)))
                chunk_text = getattr(doc, 'text', '')
                chunk_text_map[chunk_id] = chunk_text
            
            with self.graph_store._driver.session() as session:
                # æŸ¥è¯¢æ‰€æœ‰å®ä½“
                entity_query = """
                MATCH (e:__Entity__)
                RETURN DISTINCT e.name as entity_name
                LIMIT 1000
                """
                entities = []
                for record in session.run(entity_query):
                    entities.append(record["entity_name"])
                
                logger.info(f"æ‰¾åˆ° {len(entities)} ä¸ªå®ä½“ï¼Œå¼€å§‹å»ºç«‹ MENTIONS å…³ç³»...")
                
                # å¯¹äºæ¯ä¸ªå®ä½“ï¼ŒæŸ¥æ‰¾åŒ…å«è¯¥å®ä½“çš„ chunkï¼Œå¹¶å»ºç«‹ MENTIONS å…³ç³»ï¼ˆæ–¹å‘ï¼šChunk -> Entityï¼‰
                created_mentions = 0
                batch_size = 100
                
                for i in range(0, len(entities), batch_size):
                    entity_batch = entities[i:i + batch_size]
                    
                    for entity_name in entity_batch:
                        # æŸ¥æ‰¾åŒ…å«è¯¥å®ä½“åç§°çš„ chunkï¼Œå»ºç«‹ (Chunk)-[:MENTIONS]->(Entity) å…³ç³»
                        mentions_query = """
                        MATCH (e:__Entity__ {name: $entity_name})
                        MATCH (c:Chunk)
                        WHERE c.text CONTAINS $entity_name
                        MERGE (c)-[:MENTIONS]->(e)
                        RETURN count(*) as count
                        """
                        
                        try:
                            result = session.run(mentions_query, entity_name=entity_name)
                            count = result.single()["count"]
                            created_mentions += count
                        except Exception as e:
                            logger.debug(f"å»ºç«‹ MENTIONS å…³ç³»å¤±è´¥ ({entity_name}): {e}")
                
                logger.info(f"âœ… åˆ›å»ºäº† {created_mentions} ä¸ª MENTIONS å…³ç³»")
                logger.info(f"âœ… æº¯æºå…³ç³»åˆ›å»ºå®Œæˆ: {created_mentions} ä¸ª (Chunk)-[:MENTIONS]->(Entity) å…³ç³»")
                
        except Exception as e:
            logger.error(f"åˆ›å»ºæº¯æºå…³ç³»æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")

# å…¨å±€æ„å»ºå™¨å®ä¾‹ - ä¸ºäº†ä¿æŒå…¼å®¹æ€§ï¼Œå˜é‡åä»ä¸º builder
builder = KnowledgeGraphManager()
